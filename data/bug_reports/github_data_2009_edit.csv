repo,issue_link,issue_id,post,question,answer
gradle/gradle,https://github.com/gradle/gradle/issues/9319,gradle_gradle_issues_9319,"gradle metata doesn't work smoothly with 403's (s3+cloudfront)

 Arguably the problem here is that when you put cloudfront in front of an s3 bucket, instead of returning the appropriate 404 on not found, it returns a secure 403... I can see an argument that it is aws that needs to change but... good luck with that? It might be nice to have a flag, or simply treat 403's the same way instead of bailing out.  failure is permanent. I think it would be nice if gradle treated 403's and 404's the same as in both cases you're unable to retrieve the metadata, so simply treat it like it doesn't exist.",Can you please add the Gradle version and whether this is transient or permanent? Also what you would expect Gradle to do?,"affects 5.3.0-5.4.0, failure is permanent. I think it would be nice if gradle treated 403's and 404's the same as in both cases you're unable to retrieve the metadata, so simply treat it like it doesn't exist."
gradle/gradle,https://github.com/gradle/gradle/issues/5835,gradle_gradle_issues_5835,"Remove javax.annotation package & sub-packages from gradle-api-<version>.jar

Remove javax.annotation package & sub-packages from gradle-api-<version>.jar. They cause duplicate package problems with the module path. Would be preferable to have transitive dependencies on an artifact like com.google.code.findbugs:jsr305. Maybe some additional packages should also be removed from that jar, like javax.inject & javax.xml*.",What's your use case for running with gradle-api-<version>.jar on the module path?,Would be preferable to have transitive dependencies on an artifact like com.google.code.findbugs:jsr305.
gradle/gradle,https://github.com/gradle/gradle/issues/7603,gradle_gradle_issues_7603,"Gradle doesn't stop forked .bat files

<!--- Please follow the instructions below. We receive dozens of issues every week, so to stay productive, we will close issues that don't provide enough information. Please open Android-related issues on the Android Issue Tracker at https://source.android.com/source/report-bugs Please open Kotlin DSL-related issues at https://github.com/gradle/kotlin-dsl/issues Please open Gradle Native-related issues at https://github.com/gradle/gradle-native/issues --> <!--- Provide a brief summary of the issue in the title above --> When a program is started by Gradle and is long-running, and the user presses ctrl-c during that time, Gradle is killed but the subprocess started by Gradle keeps running. NOTE 1: This seems to affect Windows only! NOTE 2: This also only seems to affect bat scripts started by Gradle, not executables. So Gradle -> bat -> exe leaves the exe running on ctrl-c. ### Expected Behavior <!--- If you're describing a bug, tell us what should happen --> <!--- If you're suggesting a change/improvement, tell us how it should work --> Both Gradle and Python exits. ### Current Behavior <!--- If describing a bug, tell us what happens instead of the expected behavior --> <!--- If suggesting a change/improvement, explain the difference from current behavior --> Gradle exits, but Python keeps running: ![image](https://user-images.githubusercontent.com/1163299/47909468-493a1400-de90-11e8-8d31-ed887e98ee81.png) ### Context <!--- How has this issue affected you? What are you trying to accomplish? --> <!--- Providing context helps us come up with a solution that is most useful in the real world --> It causes the following: Access denied problems (because files are kept locked), memory leak issues, build errors ### Steps to Reproduce (for bugs) <!--- Provide a self-contained example project (as an attached archive or a Github project). --> <!--- In the rare cases where this is infeasible, we will also accept a detailed set of instructions. --> File: freeze.py  File: build.gradle  File: python.bat  1. Put the files build.gradle, python.bat and freeze.py into the same directory, with contents copied from above. 2. Start cmd.exe 3. Chdir into the directory 4. Enter gradle dummy 5. Press ctrl+c ### Your Environment <!--- Include as many relevant details about the environment you experienced the bug in --> <!--- A build scan https://scans.gradle.com/get-started is ideal --> * Build scan URL:  NOTE: Gradle 5/nightly is also affected",Can you please clarify?,"1 NOTE 2: This also only seems to affect bat scripts started by Gradle, not executables. So Gradle -> bat -> exe leaves the exe running on ctrl-c..batFile: python.bat  , python.bat"
gradle/gradle,https://github.com/gradle/gradle/issues/7941,gradle_gradle_issues_7941,"Gradle 5 does not pick annotation processor from project dependency

<!--- Please follow the instructions below. We receive dozens of issues every week, so to stay productive, we will close issues that don't provide enough information. Please open Android-related issues on the Android Issue Tracker at https://source.android.com/source/report-bugs Please open Kotlin DSL-related issues at https://github.com/gradle/kotlin-dsl/issues Please open Gradle Native-related issues at https://github.com/gradle/gradle-native/issues --> <!--- Provide a brief summary of the issue in the title above --> My project has subprojects that are generating different artifacts. Among those, the subproject of interest are * model - contains annotations that will be used in other sub-projects * model-processor - contains annotation processor that generates sources for any class found with the annotations from model subproject * consumer - contains classes annotated with annotations from model subproject that should be accompanied by sources generated by annotation processor from model-processor subproject. The build.gradle for consumer sub-project looks like  ### Expected Behavior <!--- If you're describing a bug, tell us what should happen --> <!--- If you're suggesting a change/improvement, tell us how it should work --> I expect to find the generated sources under ﻿consumer/build/generated/source/apt/main ### Current Behavior <!--- If describing a bug, tell us what happens instead of the expected behavior --> <!--- If suggesting a change/improvement, explain the difference from current behavior --> I am not getting the files generated under consumer/build/generated/source/apt/main with the Gradle 5.0. I do get the annotation processor to work with the same build script and Gradle 4.10.2. ### Context <!--- How has this issue affected you? What are you trying to accomplish? --> <!--- Providing context helps us come up with a solution that is most useful in the real world --> I am trying to switch to Gradle 5 from Gradle 4.10.2 where the generation works. If I cannot get the annotation processor to work, I cannot migrate. ### Steps to Reproduce (for bugs) <!--- Provide a self-contained example project (as an attached archive or a Github project). --> <!--- In the rare cases where this is infeasible, we will also accept a detailed set of instructions. --> Clone https://github.com/ishaigor/annotation-processor-sample Follow the steps in README ### Your Environment <!--- Include as many relevant details about the environment you experienced the bug in --> <!--- A build scan https://scans.gradle.com/get-started is ideal --> * Build scan URL: * https://gradle.com/s/sremu47v5j5kk - non working 5.0 * https://gradle.com/s/5pa5eu7fsrnic - working 4.10.2",Could you share the build you're seeing this in? @marcphilipp @oehme anything obvious to check?,Clone https://github.com/ishaigor/annotation-processor-sample Follow the steps in README
gradle/gradle,https://github.com/gradle/gradle/issues/12016,gradle_gradle_issues_12016,"strictly does not override constraint version from a project dependency

### Expected Behavior When a project depends on another project with a constraint version, it should be able to downgrade the version using strictly. ### Current Behavior Another project's constraint version cannot be downgraded and the build fails to resolve versions. There doesn't seem to be a way to exclude the constraints either. ### Context I use constraints to apply dependency management to projects in a multi-project build. There is one set of versioned dependencies and all projects have the versions applied using something like  This means that many constraints are added to projects even though there is no dependency declaration. I found that when adding a project dependency, all of it's constraints are applied even if the project does not have a dependency matching them. This means that for dependency A, there is no way to exclude the transitive dependency A from this project dependency - it is not a listed dependency, but it's constraint is applied either way. This would be fine, but it seems that strictly does not override the version in this case. force works fine, but is deprecated https://docs.gradle.org/current/userguide/dependency_downgrade_and_exclude.html#forced_dependencies_vs_strict_dependencies The language ""If, for some reason, you can’t use strict versions, you can force a dependency doing this:"" makes me think that this is not an intended case. ### Steps to Reproduce ~~- git clone https://github.com/anuraaga/gradle-constraint-overrides~~ ~~- cd gradle-constraint-overrides~~ ~~- ./gradlew :consumer:dependencies~~ Apologies for the complex example, my attempt above at simplifying it didn't reproduce yet. - git clone https://github.com/anuraaga/armeria/ - git fetch origin modern-gradle - git checkout modern-gradle - Comment out https://github.com/anuraaga/armeria/blob/modern-gradle/thrift0.9/build.gradle#L14 - ./gradlew :thrift0.9:dependencies --configuration compileClasspath - Note the thrift dependencies have FAILED to resolve ### Your Environment <!--- Include as many relevant details about the environment you experienced the bug in --> <!--- A build scan https://scans.gradle.com/get-started is ideal --> Build scan URL: https://gradle.com/s/s3hab734ooyzi",Do I miss something?,"~~~~ Apologies for the complex example, my attempt above at simplifying it didn't reproduce yet. - git clone https://github.com/anuraaga/armeria/ - git fetch origin modern-gradle - git checkout modern-gradle - Comment out https://github.com/anuraaga/armeria/blob/modern-gradle/thrift0.9/build.gradle#L14 - ./gradlew :thrift0.9:dependencies --configuration compileClasspath"
gradle/gradle,https://github.com/gradle/gradle/issues/9664,gradle_gradle_issues_9664,"Are up-to-date checks and incremental java compilation not meant to be used on CI servers?

**UPDATE:** Today I finally was able to get incremental Java compilation and UP-TO-DATE checks to work across machines by caching directories I listed below (initially, I had an error in dir moving logic). Although, my question remains: are these features not meant to be used on CI? If so, why? What made me ask this question: I figured that these features [require your BUILD_DIR to be an absolute path which does not change between builds](https://github.com/gradle/gradle/issues/9337), and this may be hard to achieve in CI environments. Also, it's not immediately clear which folders need to be cached in order for these features to work, so a user has to do some guesswork here. **ORIGINAL TEXT** I am trying to make incremental build (up-to-date checks) and incremental java compilation to work on CI servers. After spending significant time on these attempts, I feel like I can't make any progress on my own and need advice from the community. Maybe these features were never meant to work across machines? I know that the build cache feature was made to address this, but it works as ""all-or-nothing"" cache and does not provide incremental compilation. I am using Gradle Wrapper 5.4.1 and Gradle 5.4.1. I am using standard compileJava task. Here's a list of directories I cache across my build runners: 1. GRADLE_USER_HOME/caches (absolute path, same on all runners) 2. PROJECT_DIR/.gradle (different path on each runner because PROJECT_DIR contains runner id) 3. BUILD_DIR (absolute path, same on all runners) When compileJava starts on a new runner, it simply deletes cached classes as stale, saying:  Both features work locally. I also tried moving the project folder to emulate PROJECT_DIR change across runners, features still work. Moreover, I tried sending archived PROJECT_DIR folder to my colleague, and to my surprise, incremental build and compilation worked across our machines. But since Gradle does not give any insight into why files are considered stale, I can't figure out why it does not work on CI. I am using Gitlab CI, Gradle builds run inside a Docker container, but there's no magic going on there AFAIK, so everything should work the same way it does between my and colleague's machines.","Why not use the build cache, which is designed exactly for this use case?","**UPDATE:** Today I finally was able to get incremental Java compilation and UP-TO-DATE checks to work across machines by caching directories I listed above. Although, my question remains: are these features not meant to be used on CI? And if so, why? **ORIGINAL TEXT**"
gradle/gradle,https://github.com/gradle/gradle/issues/11412,gradle_gradle_issues_11412,"Gradle 6.0 generates incorrect versions for Kotlin multiplatform projects when publishing to Bintray

<!--- Please use our bug report template to report problems with something that has never worked. Regressions reports are greatly appreciated during our RC phase and before a final release. --> <!--- Provide a brief summary of the issue in the title above --> ### Expected Behavior When publishing a Kotlin multiplatform project to Bintray using the maven-publish plugin, only one version should be created, that being the project's version. All artifacts should then be published to that version in Bintray. This was the case in Gradle versions before 6.0. In practice, for a package logkat with two platform targets (JVM and linux-x64) and a version of 0.5.0, the correct Bintray output would be: ![image](https://user-images.githubusercontent.com/12981608/69076352-45c4ec80-0a01-11ea-8d52-4c3db2c3761c.png) ### Current Behavior In Gradle 6.0, a version is created for the project's version, but then versions are created for each artifact. In practice, for a package logkat with two platform targets (JVM and linux-x64) and a version of 0.5.0, the putput for Gradle 6.0 is: ![image](https://user-images.githubusercontent.com/12981608/69076595-d3a0d780-0a01-11ea-9b75-c424adce5a28.png) ### Context This issue happens with Gradle 6.0 only. All previous versions do not have this issue. The ""correct"" output is from Gradle 5.6.3. ### Steps to Reproduce My project at https://gitlab.com/serebit/logkat currently has a Gradle 6.0 wrapper, and generates the above when publishing to its ""snapshot"" repository. ### Your Environment This happens both on my Linux desktop and in GitLab CI, using OpenJDK 8 or OpenJDK 11.",Could you try with Gradle 5.6.4 since 5.6 is when we introduced that parallelism?,"The ""correct"" output is from Gradle 5.6.3."
openzfs/zfs,https://github.com/openzfs/zfs/issues/8317,openzfs_zfs_issues_8317,"Error with disabled DMU prefetcher under high workload

<!-- Please fill out the following template, which will help other contributors address your issue. --> <!-- Thank you for reporting an issue. *IMPORTANT* - Please search our issue tracker *before* making a new issue. If you cannot find a similar issue, then create a new issue. https://github.com/zfsonlinux/zfs/issues *IMPORTANT* - This issue tracker is for *bugs* and *issues* only. Please search the wiki and the mailing list archives before asking questions on the mailing list. https://github.com/zfsonlinux/zfs/wiki/Mailing-Lists Please fill in as much of the template as possible. --> ### System information <!-- add version after ""|"" character --> Type | Version/Name --- | --- Distribution Name | Proxmox PVE (debian based) Distribution Version | 5.3 Linux Kernel | 4.15.18 Architecture | x64 ZFS Version | version: 0.7.12-1 SPL Version | version: 0.7.12-1 <!-- Commands to find ZFS/SPL versions: modinfo zfs | grep -iw version modinfo spl | grep -iw version --> ### Describe the problem you're observing Server is ProxmoxVE node which is used as shared storage (NFS mainly) and additional node for cluster quorum. With default and little bit tuned parameters it was used for a long period of time without any issue.  root@storageB:/tmp# zpool status poolB pool: poolB state: ONLINE scan: scrub canceled on Tue Aug 14 10:38:24 2018 config: NAME STATE READ WRITE CKSUM poolB ONLINE 0 0 0 mirror-0 ONLINE 0 0 0 dm-uuid-mpath-35000cca24d4e9860 ONLINE 0 0 0 dm-uuid-mpath-35000cca24d089668 ONLINE 0 0 0 mirror-1 ONLINE 0 0 0 dm-uuid-mpath-35000cca24d4ff57c ONLINE 0 0 0 dm-uuid-mpath-35000cca24d4f0990 ONLINE 0 0 0 mirror-2 ONLINE 0 0 0 dm-uuid-mpath-35000cca24d4fa5ec ONLINE 0 0 0 dm-uuid-mpath-35000cca24d46b730 ONLINE 0 0 0 mirror-4 ONLINE 0 0 0 dm-uuid-mpath-35000cca24d08b658 ONLINE 0 0 0 dm-uuid-mpath-35000cca2553f3234 ONLINE 0 0 0 mirror-5 ONLINE 0 0 0 dm-uuid-mpath-35000cca25541ff0c ONLINE 0 0 0 dm-uuid-mpath-35000cca24d08ee64 ONLINE 0 0 0 logs dm-uuid-mpath-35000a72030075898 ONLINE 0 0 0 spares dm-uuid-mpath-35000cca24d4f6e24 AVAIL errors: No known data errors root@storageB:/tmp# zfs list NAME USED AVAIL REFER MOUNTPOINT poolB 13.6T 12.7T 104K /poolB poolB/pve 13.6T 12.7T 96K /poolB/pve poolB/pve/datastore 13.6T 12.7T 132K /poolB/pve/datastore poolB/pve/datastore/nfs 13.4T 12.7T 13.2T /poolB/pve/datastore/nfs poolB/pve/datastore/zvol 129G 12.7T 96K /poolB/pve/datastore/zvol poolB/pve/datastore/zvol/vm-114-disk-0 32.3G 12.8T 8.07G - poolB/pve/datastore/zvol/vm-114-disk-1 32.3G 12.8T 8.07G - poolB/pve/datastore/zvol/vm-114-disk-2 32.3G 12.8T 16.1G - poolB/pve/datastore/zvol/vm-114-disk-3 32.3G 12.8T 8.07G - rpool 10.7G 18.2G 96K /rpool rpool/ROOT 6.75G 18.2G 96K /rpool/ROOT rpool/ROOT/pve-1 6.75G 18.2G 6.75G / rpool/data 96K 18.2G 96K /rpool/data rpool/swap 3.85G 18.8G 3.19G - SOURCE poolB/pve/datastore/nfs sharenfs rw=@172.16.253.0/24,no_subtree_check,async,no_wdelay,rw=@172.16.252.0/24,no_subtree_check,async,no_wdelay local  ZFS Subsystem Report Sun Jan 20 16:36:21 2019 ARC Summary: (HEALTHY) Memory Throttle Count: 0 ARC Misc: Deleted: 465 Mutex Misses: 0 Evict Skips: 120 ARC Size: 72.02% 155.56 GiB Target Size: (Adaptive) 100.00% 216.00 GiB Min Size (Hard Limit): 88.89% 192.00 GiB Max Size (High Water): 1:1 216.00 GiB ARC Size Breakdown: Recently Used Cache Size: 32.07% 46.41 GiB Frequently Used Cache Size: 67.93% 98.30 GiB ARC Hash Breakdown: Elements Max: 34.75M Elements Current: 100.00% 34.75M Collisions: 34.38M Chain Max: 10 Chains: 9.31M ARC Total accesses: 412.02M Cache Hit Ratio: 94.52% 389.43M Cache Miss Ratio: 5.48% 22.58M Actual Hit Ratio: 89.88% 370.33M Data Demand Efficiency: 80.39% 89.80M Data Prefetch Efficiency: 85.73% 21.69M CACHE HITS BY CACHE LIST: Anonymously Used: 4.91% 19.10M Most Recently Used: 15.61% 60.77M Most Frequently Used: 79.49% 309.56M Most Recently Used Ghost: 0.00% 0 Most Frequently Used Ghost: 0.00% 0 CACHE HITS BY DATA TYPE: Demand Data: 18.54% 72.20M Prefetch Data: 4.78% 18.60M Demand Metadata: 76.45% 297.71M Prefetch Metadata: 0.24% 926.91k CACHE MISSES BY DATA TYPE: Demand Data: 77.96% 17.61M Prefetch Data: 13.71% 3.10M Demand Metadata: 7.65% 1.73M Prefetch Metadata: 0.68% 153.10k ZFS Tunables: dbuf_cache_hiwater_pct 10 dbuf_cache_lowater_pct 10 dbuf_cache_max_bytes 104857600 dbuf_cache_max_shift 5 dmu_object_alloc_chunk_shift 7 ignore_hole_birth 1 l2arc_feed_again 1 l2arc_feed_min_ms 100 l2arc_feed_secs 1 l2arc_headroom 4 l2arc_headroom_boost 200 l2arc_noprefetch 0 l2arc_norw 0 l2arc_write_boost 134217728 l2arc_write_max 67108864 metaslab_aliquot 524288 metaslab_bias_enabled 1 metaslab_debug_load 0 metaslab_debug_unload 0 metaslab_fragmentation_factor_enabled 1 metaslab_lba_weighting_enabled 1 metaslab_preload_enabled 1 metaslabs_per_vdev 200 send_holes_without_birth_time 1 spa_asize_inflation 24 spa_config_path /etc/zfs/zpool.cache spa_load_verify_data 1 spa_load_verify_maxinflight 10000 spa_load_verify_metadata 1 spa_slop_shift 5 zfetch_array_rd_sz 1048576 zfetch_max_distance 8388608 zfetch_max_streams 8 zfetch_min_sec_reap 2 zfs_abd_scatter_enabled 1 zfs_abd_scatter_max_order 10 zfs_admin_snapshot 1 zfs_arc_average_blocksize 8192 zfs_arc_dnode_limit 0 zfs_arc_dnode_limit_percent 10 zfs_arc_dnode_reduce_percent 10 zfs_arc_grow_retry 0 zfs_arc_lotsfree_percent 10 zfs_arc_max 231928233984 zfs_arc_meta_adjust_restarts 4096 zfs_arc_meta_limit 0 zfs_arc_meta_limit_percent 75 zfs_arc_meta_min 0 zfs_arc_meta_prune 10000 zfs_arc_meta_strategy 1 zfs_arc_min 206158430208 zfs_arc_min_prefetch_lifespan 0 zfs_arc_p_dampener_disable 1 zfs_arc_p_min_shift 0 zfs_arc_pc_percent 0 zfs_arc_shrink_shift 9 zfs_arc_sys_free 0 zfs_autoimport_disable 1 zfs_checksums_per_second 20 zfs_compressed_arc_enabled 1 zfs_dbgmsg_enable 0 zfs_dbgmsg_maxsize 4194304 zfs_dbuf_state_index 0 zfs_deadman_checktime_ms 5000 zfs_deadman_enabled 1 zfs_deadman_synctime_ms 1000000 zfs_dedup_prefetch 0 zfs_delay_min_dirty_percent 60 zfs_delay_scale 500000 zfs_delays_per_second 20 zfs_delete_blocks 20480 zfs_dirty_data_max 4294967296 zfs_dirty_data_max_max 4294967296 zfs_dirty_data_max_max_percent 25 zfs_dirty_data_max_percent 10 zfs_dirty_data_sync 67108864 zfs_dmu_offset_next_sync 0 zfs_expire_snapshot 300 zfs_flags 0 zfs_free_bpobj_enabled 1 zfs_free_leak_on_eio 0 zfs_free_max_blocks 100000 zfs_free_min_time_ms 1000 zfs_immediate_write_sz 32768 zfs_max_recordsize 1048576 zfs_mdcomp_disable 0 zfs_metaslab_fragmentation_threshold 70 zfs_metaslab_segment_weight_enabled 1 zfs_metaslab_switch_threshold 2 zfs_mg_fragmentation_threshold 85 zfs_mg_noalloc_threshold 0 zfs_multihost_fail_intervals 5 zfs_multihost_history 0 zfs_multihost_import_intervals 10 zfs_multihost_interval 1000 zfs_multilist_num_sublists 0 zfs_no_scrub_io 0 zfs_no_scrub_prefetch 0 zfs_nocacheflush 0 zfs_nopwrite_enabled 1 zfs_object_mutex_size 64 zfs_pd_bytes_max 52428800 zfs_per_txg_dirty_frees_percent 30 zfs_prefetch_disable 1 zfs_read_chunk_size 1048576 zfs_read_history 0 zfs_read_history_hits 0 zfs_recover 0 zfs_recv_queue_length 16777216 zfs_resilver_delay 2 zfs_resilver_min_time_ms 3000 zfs_scan_idle 50 zfs_scan_ignore_errors 0 zfs_scan_min_time_ms 1000 zfs_scrub_delay 4 zfs_send_corrupt_data 0 zfs_send_queue_length 16777216 zfs_sync_pass_deferred_free 2 zfs_sync_pass_dont_compress 5 zfs_sync_pass_rewrite 2 zfs_sync_taskq_batch_pct 75 zfs_top_maxinflight 32 zfs_txg_history 0 zfs_txg_timeout 5 zfs_vdev_aggregation_limit 131072 zfs_vdev_async_read_max_active 24 zfs_vdev_async_read_min_active 12 zfs_vdev_async_write_active_max_dirty_percent 60 zfs_vdev_async_write_active_min_dirty_percent 10 zfs_vdev_async_write_max_active 24 zfs_vdev_async_write_min_active 8 zfs_vdev_cache_bshift 16 zfs_vdev_cache_max 16384 zfs_vdev_cache_size 0 zfs_vdev_max_active 1000 zfs_vdev_mirror_non_rotating_inc 0 zfs_vdev_mirror_non_rotating_seek_inc 1 zfs_vdev_mirror_rotating_inc 0 zfs_vdev_mirror_rotating_seek_inc 5 zfs_vdev_mirror_rotating_seek_offset 1048576 zfs_vdev_queue_depth_pct 1000 zfs_vdev_raidz_impl [fastest] original scalar sse2 ssse3 zfs_vdev_read_gap_limit 32768 zfs_vdev_scheduler noop zfs_vdev_scrub_max_active 2 zfs_vdev_scrub_min_active 1 zfs_vdev_sync_read_max_active 24 zfs_vdev_sync_read_min_active 12 zfs_vdev_sync_write_max_active 12 zfs_vdev_sync_write_min_active 24 zfs_vdev_write_gap_limit 4096 zfs_zevent_cols 80 zfs_zevent_console 0 zfs_zevent_len_max 512 zil_replay_disable 0 zil_slog_bulk 786432 zio_delay_max 30000 zio_dva_throttle_enabled 1 zio_requeue_io_start_cut_in_line 1 zio_taskq_batch_pct 75 zvol_inhibit_dev 0 zvol_major 230 zvol_max_discard_blocks 16384 zvol_prefetch_bytes 131072 zvol_request_sync 0 zvol_threads 32 zvol_volmode 1  poolB/pve/datastore/zvol 129G 12.7T 96K /poolB/pve/datastore/zvol poolB/pve/datastore/zvol/vm-114-disk-0 32.3G 12.8T 8.07G - poolB/pve/datastore/zvol/vm-114-disk-1 32.3G 12.8T 8.07G - poolB/pve/datastore/zvol/vm-114-disk-2 32.3G 12.8T 16.1G - poolB/pve/datastore/zvol/vm-114-disk-3 32.3G 12.8T 8.07G - root@storageB:/tmp# zfs get all poolB/pve/datastore/zvol NAME PROPERTY VALUE SOURCE poolB/pve/datastore/zvol type filesystem - poolB/pve/datastore/zvol creation Tue May 1 12:02 2018 - poolB/pve/datastore/zvol used 129G - poolB/pve/datastore/zvol available 12.7T - poolB/pve/datastore/zvol referenced 96K - poolB/pve/datastore/zvol compressratio 1.00x - poolB/pve/datastore/zvol mounted yes - poolB/pve/datastore/zvol quota none default poolB/pve/datastore/zvol reservation none default poolB/pve/datastore/zvol recordsize 128K local poolB/pve/datastore/zvol mountpoint /poolB/pve/datastore/zvol default poolB/pve/datastore/zvol sharenfs off local poolB/pve/datastore/zvol checksum edonr inherited from poolB poolB/pve/datastore/zvol compression off default poolB/pve/datastore/zvol atime off local poolB/pve/datastore/zvol devices on default poolB/pve/datastore/zvol exec on default poolB/pve/datastore/zvol setuid on default poolB/pve/datastore/zvol readonly off default poolB/pve/datastore/zvol zoned off default poolB/pve/datastore/zvol snapdir hidden default poolB/pve/datastore/zvol aclinherit restricted default poolB/pve/datastore/zvol createtxg 2043435 - poolB/pve/datastore/zvol canmount on local poolB/pve/datastore/zvol xattr sa local poolB/pve/datastore/zvol copies 1 default poolB/pve/datastore/zvol version 5 - poolB/pve/datastore/zvol utf8only off - poolB/pve/datastore/zvol normalization none - poolB/pve/datastore/zvol casesensitivity sensitive - poolB/pve/datastore/zvol vscan off default poolB/pve/datastore/zvol nbmand off default poolB/pve/datastore/zvol sharesmb off default poolB/pve/datastore/zvol refquota none default poolB/pve/datastore/zvol refreservation none default poolB/pve/datastore/zvol guid 5321946544684090761 - poolB/pve/datastore/zvol primarycache none local poolB/pve/datastore/zvol secondarycache none inherited from poolB poolB/pve/datastore/zvol usedbysnapshots 0B - poolB/pve/datastore/zvol usedbydataset 96K - poolB/pve/datastore/zvol usedbychildren 129G - poolB/pve/datastore/zvol usedbyrefreservation 0B - poolB/pve/datastore/zvol logbias latency default poolB/pve/datastore/zvol dedup off default poolB/pve/datastore/zvol mlslabel none default poolB/pve/datastore/zvol sync standard default poolB/pve/datastore/zvol dnodesize legacy default poolB/pve/datastore/zvol refcompressratio 1.00x - poolB/pve/datastore/zvol written 96K - poolB/pve/datastore/zvol logicalused 40.3G - poolB/pve/datastore/zvol logicalreferenced 40K - poolB/pve/datastore/zvol volmode default default poolB/pve/datastore/zvol filesystem_limit none default poolB/pve/datastore/zvol snapshot_limit none default poolB/pve/datastore/zvol filesystem_count none default poolB/pve/datastore/zvol snapshot_count none default poolB/pve/datastore/zvol snapdev hidden default poolB/pve/datastore/zvol acltype posixacl inherited from poolB poolB/pve/datastore/zvol context none default poolB/pve/datastore/zvol fscontext none default poolB/pve/datastore/zvol defcontext none default poolB/pve/datastore/zvol rootcontext none default poolB/pve/datastore/zvol relatime off default poolB/pve/datastore/zvol redundant_metadata most inherited from poolB poolB/pve/datastore/zvol overlay off default root@storageB:/tmp# zfs get all poolB/pve/datastore/zvol/vm-114-disk-0 NAME PROPERTY VALUE SOURCE poolB/pve/datastore/zvol/vm-114-disk-0 type volume - poolB/pve/datastore/zvol/vm-114-disk-0 creation Sat Jan 19 22:51 2019 - poolB/pve/datastore/zvol/vm-114-disk-0 used 32.3G - poolB/pve/datastore/zvol/vm-114-disk-0 available 12.8T - poolB/pve/datastore/zvol/vm-114-disk-0 referenced 8.07G - poolB/pve/datastore/zvol/vm-114-disk-0 compressratio 1.00x - poolB/pve/datastore/zvol/vm-114-disk-0 reservation none default poolB/pve/datastore/zvol/vm-114-disk-0 volsize 32G local poolB/pve/datastore/zvol/vm-114-disk-0 volblocksize 32K - poolB/pve/datastore/zvol/vm-114-disk-0 checksum edonr inherited from poolB poolB/pve/datastore/zvol/vm-114-disk-0 compression off default poolB/pve/datastore/zvol/vm-114-disk-0 readonly off default poolB/pve/datastore/zvol/vm-114-disk-0 createtxg 28837837 - poolB/pve/datastore/zvol/vm-114-disk-0 copies 1 default poolB/pve/datastore/zvol/vm-114-disk-0 refreservation 32.3G local poolB/pve/datastore/zvol/vm-114-disk-0 guid 8360771561613258997 - poolB/pve/datastore/zvol/vm-114-disk-0 primarycache none inherited from poolB/pve/datastore/zvol poolB/pve/datastore/zvol/vm-114-disk-0 secondarycache none inherited from poolB poolB/pve/datastore/zvol/vm-114-disk-0 usedbysnapshots 0B - poolB/pve/datastore/zvol/vm-114-disk-0 usedbydataset 8.07G - poolB/pve/datastore/zvol/vm-114-disk-0 usedbychildren 0B - poolB/pve/datastore/zvol/vm-114-disk-0 usedbyrefreservation 24.2G - poolB/pve/datastore/zvol/vm-114-disk-0 logbias latency default poolB/pve/datastore/zvol/vm-114-disk-0 dedup off default poolB/pve/datastore/zvol/vm-114-disk-0 mlslabel none default poolB/pve/datastore/zvol/vm-114-disk-0 sync standard default poolB/pve/datastore/zvol/vm-114-disk-0 refcompressratio 1.00x - poolB/pve/datastore/zvol/vm-114-disk-0 written 8.07G - poolB/pve/datastore/zvol/vm-114-disk-0 logicalused 8.07G - poolB/pve/datastore/zvol/vm-114-disk-0 logicalreferenced 8.07G - poolB/pve/datastore/zvol/vm-114-disk-0 volmode default default poolB/pve/datastore/zvol/vm-114-disk-0 snapshot_limit none default poolB/pve/datastore/zvol/vm-114-disk-0 snapshot_count none default poolB/pve/datastore/zvol/vm-114-disk-0 snapdev hidden default poolB/pve/datastore/zvol/vm-114-disk-0 context none default poolB/pve/datastore/zvol/vm-114-disk-0 fscontext none default poolB/pve/datastore/zvol/vm-114-disk-0 defcontext none default poolB/pve/datastore/zvol/vm-114-disk-0 rootcontext none default poolB/pve/datastore/zvol/vm-114-disk-0 redundant_metadata most inherited from poolB  this is an example how log text should be marked (wrap it with  -->  root@storageB:/tmp# zpool status poolB pool: poolB state: ONLINE scan: scrub canceled on Tue Aug 14 10:38:24 2018 config: NAME STATE READ WRITE CKSUM poolB ONLINE 0 0 0 mirror-0 ONLINE 0 0 0 dm-uuid-mpath-35000cca24d4e9860 ONLINE 0 0 0 dm-uuid-mpath-35000cca24d089668 ONLINE 0 0 0 mirror-1 ONLINE 0 0 0 dm-uuid-mpath-35000cca24d4ff57c ONLINE 0 0 0 dm-uuid-mpath-35000cca24d4f0990 ONLINE 0 0 0 mirror-2 ONLINE 0 0 0 dm-uuid-mpath-35000cca24d4fa5ec ONLINE 0 0 0 dm-uuid-mpath-35000cca24d46b730 ONLINE 0 0 0 mirror-4 ONLINE 0 0 0 dm-uuid-mpath-35000cca24d08b658 ONLINE 0 0 0 dm-uuid-mpath-35000cca2553f3234 ONLINE 0 0 0 mirror-5 ONLINE 0 0 0 dm-uuid-mpath-35000cca25541ff0c ONLINE 0 0 0 dm-uuid-mpath-35000cca24d08ee64 ONLINE 0 0 0 logs dm-uuid-mpath-35000a72030075898 ONLINE 0 0 0 spares dm-uuid-mpath-35000cca24d4f6e24 AVAIL errors: No known data errors root@storageB:/tmp# zfs list NAME USED AVAIL REFER MOUNTPOINT poolB 13.6T 12.7T 104K /poolB poolB/pve 13.6T 12.7T 96K /poolB/pve poolB/pve/datastore 13.6T 12.7T 132K /poolB/pve/datastore poolB/pve/datastore/nfs 13.4T 12.7T 13.2T /poolB/pve/datastore/nfs poolB/pve/datastore/zvol 129G 12.7T 96K /poolB/pve/datastore/zvol poolB/pve/datastore/zvol/vm-114-disk-0 32.3G 12.8T 8.07G - poolB/pve/datastore/zvol/vm-114-disk-1 32.3G 12.8T 8.07G - poolB/pve/datastore/zvol/vm-114-disk-2 32.3G 12.8T 16.1G - poolB/pve/datastore/zvol/vm-114-disk-3 32.3G 12.8T 8.07G - rpool 10.7G 18.2G 96K /rpool rpool/ROOT 6.75G 18.2G 96K /rpool/ROOT rpool/ROOT/pve-1 6.75G 18.2G 6.75G / rpool/data 96K 18.2G 96K /rpool/data rpool/swap 3.85G 18.8G 3.19G - SOURCE poolB/pve/datastore/nfs sharenfs rw=@172.16.253.0/24,no_subtree_check,async,no_wdelay,rw=@172.16.252.0/24,no_subtree_check,async,no_wdelay local  ZFS Subsystem Report Sun Jan 20 16:36:21 2019 ARC Summary: (HEALTHY) Memory Throttle Count: 0 ARC Misc: Deleted: 465 Mutex Misses: 0 Evict Skips: 120 ARC Size: 72.02% 155.56 GiB Target Size: (Adaptive) 100.00% 216.00 GiB Min Size (Hard Limit): 88.89% 192.00 GiB Max Size (High Water): 1:1 216.00 GiB ARC Size Breakdown: Recently Used Cache Size: 32.07% 46.41 GiB Frequently Used Cache Size: 67.93% 98.30 GiB ARC Hash Breakdown: Elements Max: 34.75M Elements Current: 100.00% 34.75M Collisions: 34.38M Chain Max: 10 Chains: 9.31M ARC Total accesses: 412.02M Cache Hit Ratio: 94.52% 389.43M Cache Miss Ratio: 5.48% 22.58M Actual Hit Ratio: 89.88% 370.33M Data Demand Efficiency: 80.39% 89.80M Data Prefetch Efficiency: 85.73% 21.69M CACHE HITS BY CACHE LIST: Anonymously Used: 4.91% 19.10M Most Recently Used: 15.61% 60.77M Most Frequently Used: 79.49% 309.56M Most Recently Used Ghost: 0.00% 0 Most Frequently Used Ghost: 0.00% 0 CACHE HITS BY DATA TYPE: Demand Data: 18.54% 72.20M Prefetch Data: 4.78% 18.60M Demand Metadata: 76.45% 297.71M Prefetch Metadata: 0.24% 926.91k CACHE MISSES BY DATA TYPE: Demand Data: 77.96% 17.61M Prefetch Data: 13.71% 3.10M Demand Metadata: 7.65% 1.73M Prefetch Metadata: 0.68% 153.10k ZFS Tunables: dbuf_cache_hiwater_pct 10 dbuf_cache_lowater_pct 10 dbuf_cache_max_bytes 104857600 dbuf_cache_max_shift 5 dmu_object_alloc_chunk_shift 7 ignore_hole_birth 1 l2arc_feed_again 1 l2arc_feed_min_ms 100 l2arc_feed_secs 1 l2arc_headroom 4 l2arc_headroom_boost 200 l2arc_noprefetch 0 l2arc_norw 0 l2arc_write_boost 134217728 l2arc_write_max 67108864 metaslab_aliquot 524288 metaslab_bias_enabled 1 metaslab_debug_load 0 metaslab_debug_unload 0 metaslab_fragmentation_factor_enabled 1 metaslab_lba_weighting_enabled 1 metaslab_preload_enabled 1 metaslabs_per_vdev 200 send_holes_without_birth_time 1 spa_asize_inflation 24 spa_config_path /etc/zfs/zpool.cache spa_load_verify_data 1 spa_load_verify_maxinflight 10000 spa_load_verify_metadata 1 spa_slop_shift 5 zfetch_array_rd_sz 1048576 zfetch_max_distance 8388608 zfetch_max_streams 8 zfetch_min_sec_reap 2 zfs_abd_scatter_enabled 1 zfs_abd_scatter_max_order 10 zfs_admin_snapshot 1 zfs_arc_average_blocksize 8192 zfs_arc_dnode_limit 0 zfs_arc_dnode_limit_percent 10 zfs_arc_dnode_reduce_percent 10 zfs_arc_grow_retry 0 zfs_arc_lotsfree_percent 10 zfs_arc_max 231928233984 zfs_arc_meta_adjust_restarts 4096 zfs_arc_meta_limit 0 zfs_arc_meta_limit_percent 75 zfs_arc_meta_min 0 zfs_arc_meta_prune 10000 zfs_arc_meta_strategy 1 zfs_arc_min 206158430208 zfs_arc_min_prefetch_lifespan 0 zfs_arc_p_dampener_disable 1 zfs_arc_p_min_shift 0 zfs_arc_pc_percent 0 zfs_arc_shrink_shift 9 zfs_arc_sys_free 0 zfs_autoimport_disable 1 zfs_checksums_per_second 20 zfs_compressed_arc_enabled 1 zfs_dbgmsg_enable 0 zfs_dbgmsg_maxsize 4194304 zfs_dbuf_state_index 0 zfs_deadman_checktime_ms 5000 zfs_deadman_enabled 1 zfs_deadman_synctime_ms 1000000 zfs_dedup_prefetch 0 zfs_delay_min_dirty_percent 60 zfs_delay_scale 500000 zfs_delays_per_second 20 zfs_delete_blocks 20480 zfs_dirty_data_max 4294967296 zfs_dirty_data_max_max 4294967296 zfs_dirty_data_max_max_percent 25 zfs_dirty_data_max_percent 10 zfs_dirty_data_sync 67108864 zfs_dmu_offset_next_sync 0 zfs_expire_snapshot 300 zfs_flags 0 zfs_free_bpobj_enabled 1 zfs_free_leak_on_eio 0 zfs_free_max_blocks 100000 zfs_free_min_time_ms 1000 zfs_immediate_write_sz 32768 zfs_max_recordsize 1048576 zfs_mdcomp_disable 0 zfs_metaslab_fragmentation_threshold 70 zfs_metaslab_segment_weight_enabled 1 zfs_metaslab_switch_threshold 2 zfs_mg_fragmentation_threshold 85 zfs_mg_noalloc_threshold 0 zfs_multihost_fail_intervals 5 zfs_multihost_history 0 zfs_multihost_import_intervals 10 zfs_multihost_interval 1000 zfs_multilist_num_sublists 0 zfs_no_scrub_io 0 zfs_no_scrub_prefetch 0 zfs_nocacheflush 0 zfs_nopwrite_enabled 1 zfs_object_mutex_size 64 zfs_pd_bytes_max 52428800 zfs_per_txg_dirty_frees_percent 30 zfs_prefetch_disable 1 zfs_read_chunk_size 1048576 zfs_read_history 0 zfs_read_history_hits 0 zfs_recover 0 zfs_recv_queue_length 16777216 zfs_resilver_delay 2 zfs_resilver_min_time_ms 3000 zfs_scan_idle 50 zfs_scan_ignore_errors 0 zfs_scan_min_time_ms 1000 zfs_scrub_delay 4 zfs_send_corrupt_data 0 zfs_send_queue_length 16777216 zfs_sync_pass_deferred_free 2 zfs_sync_pass_dont_compress 5 zfs_sync_pass_rewrite 2 zfs_sync_taskq_batch_pct 75 zfs_top_maxinflight 32 zfs_txg_history 0 zfs_txg_timeout 5 zfs_vdev_aggregation_limit 131072 zfs_vdev_async_read_max_active 24 zfs_vdev_async_read_min_active 12 zfs_vdev_async_write_active_max_dirty_percent 60 zfs_vdev_async_write_active_min_dirty_percent 10 zfs_vdev_async_write_max_active 24 zfs_vdev_async_write_min_active 8 zfs_vdev_cache_bshift 16 zfs_vdev_cache_max 16384 zfs_vdev_cache_size 0 zfs_vdev_max_active 1000 zfs_vdev_mirror_non_rotating_inc 0 zfs_vdev_mirror_non_rotating_seek_inc 1 zfs_vdev_mirror_rotating_inc 0 zfs_vdev_mirror_rotating_seek_inc 5 zfs_vdev_mirror_rotating_seek_offset 1048576 zfs_vdev_queue_depth_pct 1000 zfs_vdev_raidz_impl [fastest] original scalar sse2 ssse3 zfs_vdev_read_gap_limit 32768 zfs_vdev_scheduler noop zfs_vdev_scrub_max_active 2 zfs_vdev_scrub_min_active 1 zfs_vdev_sync_read_max_active 24 zfs_vdev_sync_read_min_active 12 zfs_vdev_sync_write_max_active 12 zfs_vdev_sync_write_min_active 24 zfs_vdev_write_gap_limit 4096 zfs_zevent_cols 80 zfs_zevent_console 0 zfs_zevent_len_max 512 zil_replay_disable 0 zil_slog_bulk 786432 zio_delay_max 30000 zio_dva_throttle_enabled 1 zio_requeue_io_start_cut_in_line 1 zio_taskq_batch_pct 75 zvol_inhibit_dev 0 zvol_major 230 zvol_max_discard_blocks 16384 zvol_prefetch_bytes 131072 zvol_request_sync 0 zvol_threads 32 zvol_volmode 1  poolB/pve/datastore/zvol 129G 12.7T 96K /poolB/pve/datastore/zvol poolB/pve/datastore/zvol/vm-114-disk-0 32.3G 12.8T 8.07G - poolB/pve/datastore/zvol/vm-114-disk-1 32.3G 12.8T 8.07G - poolB/pve/datastore/zvol/vm-114-disk-2 32.3G 12.8T 16.1G - poolB/pve/datastore/zvol/vm-114-disk-3 32.3G 12.8T 8.07G - root@storageB:/tmp# zfs get all poolB/pve/datastore/zvol NAME PROPERTY VALUE SOURCE poolB/pve/datastore/zvol type filesystem - poolB/pve/datastore/zvol creation Tue May 1 12:02 2018 - poolB/pve/datastore/zvol used 129G - poolB/pve/datastore/zvol available 12.7T - poolB/pve/datastore/zvol referenced 96K - poolB/pve/datastore/zvol compressratio 1.00x - poolB/pve/datastore/zvol mounted yes - poolB/pve/datastore/zvol quota none default poolB/pve/datastore/zvol reservation none default poolB/pve/datastore/zvol recordsize 128K local poolB/pve/datastore/zvol mountpoint /poolB/pve/datastore/zvol default poolB/pve/datastore/zvol sharenfs off local poolB/pve/datastore/zvol checksum edonr inherited from poolB poolB/pve/datastore/zvol compression off default poolB/pve/datastore/zvol atime off local poolB/pve/datastore/zvol devices on default poolB/pve/datastore/zvol exec on default poolB/pve/datastore/zvol setuid on default poolB/pve/datastore/zvol readonly off default poolB/pve/datastore/zvol zoned off default poolB/pve/datastore/zvol snapdir hidden default poolB/pve/datastore/zvol aclinherit restricted default poolB/pve/datastore/zvol createtxg 2043435 - poolB/pve/datastore/zvol canmount on local poolB/pve/datastore/zvol xattr sa local poolB/pve/datastore/zvol copies 1 default poolB/pve/datastore/zvol version 5 - poolB/pve/datastore/zvol utf8only off - poolB/pve/datastore/zvol normalization none - poolB/pve/datastore/zvol casesensitivity sensitive - poolB/pve/datastore/zvol vscan off default poolB/pve/datastore/zvol nbmand off default poolB/pve/datastore/zvol sharesmb off default poolB/pve/datastore/zvol refquota none default poolB/pve/datastore/zvol refreservation none default poolB/pve/datastore/zvol guid 5321946544684090761 - poolB/pve/datastore/zvol primarycache none local poolB/pve/datastore/zvol secondarycache none inherited from poolB poolB/pve/datastore/zvol usedbysnapshots 0B - poolB/pve/datastore/zvol usedbydataset 96K - poolB/pve/datastore/zvol usedbychildren 129G - poolB/pve/datastore/zvol usedbyrefreservation 0B - poolB/pve/datastore/zvol logbias latency default poolB/pve/datastore/zvol dedup off default poolB/pve/datastore/zvol mlslabel none default poolB/pve/datastore/zvol sync standard default poolB/pve/datastore/zvol dnodesize legacy default poolB/pve/datastore/zvol refcompressratio 1.00x - poolB/pve/datastore/zvol written 96K - poolB/pve/datastore/zvol logicalused 40.3G - poolB/pve/datastore/zvol logicalreferenced 40K - poolB/pve/datastore/zvol volmode default default poolB/pve/datastore/zvol filesystem_limit none default poolB/pve/datastore/zvol snapshot_limit none default poolB/pve/datastore/zvol filesystem_count none default poolB/pve/datastore/zvol snapshot_count none default poolB/pve/datastore/zvol snapdev hidden default poolB/pve/datastore/zvol acltype posixacl inherited from poolB poolB/pve/datastore/zvol context none default poolB/pve/datastore/zvol fscontext none default poolB/pve/datastore/zvol defcontext none default poolB/pve/datastore/zvol rootcontext none default poolB/pve/datastore/zvol relatime off default poolB/pve/datastore/zvol redundant_metadata most inherited from poolB poolB/pve/datastore/zvol overlay off default root@storageB:/tmp# zfs get all poolB/pve/datastore/zvol/vm-114-disk-0 NAME PROPERTY VALUE SOURCE poolB/pve/datastore/zvol/vm-114-disk-0 type volume - poolB/pve/datastore/zvol/vm-114-disk-0 creation Sat Jan 19 22:51 2019 - poolB/pve/datastore/zvol/vm-114-disk-0 used 32.3G - poolB/pve/datastore/zvol/vm-114-disk-0 available 12.8T - poolB/pve/datastore/zvol/vm-114-disk-0 referenced 8.07G - poolB/pve/datastore/zvol/vm-114-disk-0 compressratio 1.00x - poolB/pve/datastore/zvol/vm-114-disk-0 reservation none default poolB/pve/datastore/zvol/vm-114-disk-0 volsize 32G local poolB/pve/datastore/zvol/vm-114-disk-0 volblocksize 32K - poolB/pve/datastore/zvol/vm-114-disk-0 checksum edonr inherited from poolB poolB/pve/datastore/zvol/vm-114-disk-0 compression off default poolB/pve/datastore/zvol/vm-114-disk-0 readonly off default poolB/pve/datastore/zvol/vm-114-disk-0 createtxg 28837837 - poolB/pve/datastore/zvol/vm-114-disk-0 copies 1 default poolB/pve/datastore/zvol/vm-114-disk-0 refreservation 32.3G local poolB/pve/datastore/zvol/vm-114-disk-0 guid 8360771561613258997 - poolB/pve/datastore/zvol/vm-114-disk-0 primarycache none inherited from poolB/pve/datastore/zvol poolB/pve/datastore/zvol/vm-114-disk-0 secondarycache none inherited from poolB poolB/pve/datastore/zvol/vm-114-disk-0 usedbysnapshots 0B - poolB/pve/datastore/zvol/vm-114-disk-0 usedbydataset 8.07G - poolB/pve/datastore/zvol/vm-114-disk-0 usedbychildren 0B - poolB/pve/datastore/zvol/vm-114-disk-0 usedbyrefreservation 24.2G - poolB/pve/datastore/zvol/vm-114-disk-0 logbias latency default poolB/pve/datastore/zvol/vm-114-disk-0 dedup off default poolB/pve/datastore/zvol/vm-114-disk-0 mlslabel none default poolB/pve/datastore/zvol/vm-114-disk-0 sync standard default poolB/pve/datastore/zvol/vm-114-disk-0 refcompressratio 1.00x - poolB/pve/datastore/zvol/vm-114-disk-0 written 8.07G - poolB/pve/datastore/zvol/vm-114-disk-0 logicalused 8.07G - poolB/pve/datastore/zvol/vm-114-disk-0 logicalreferenced 8.07G - poolB/pve/datastore/zvol/vm-114-disk-0 volmode default default poolB/pve/datastore/zvol/vm-114-disk-0 snapshot_limit none default poolB/pve/datastore/zvol/vm-114-disk-0 snapshot_count none default poolB/pve/datastore/zvol/vm-114-disk-0 snapdev hidden default poolB/pve/datastore/zvol/vm-114-disk-0 context none default poolB/pve/datastore/zvol/vm-114-disk-0 fscontext none default poolB/pve/datastore/zvol/vm-114-disk-0 defcontext none default poolB/pve/datastore/zvol/vm-114-disk-0 rootcontext none default poolB/pve/datastore/zvol/vm-114-disk-0 redundant_metadata most inherited from poolB  this is an example how log text should be marked (wrap it with  -->",Where is the error? The log is not complete.,0] send_traverse D 0 300 2 0x80000000 Jan 20 12:24:23 storageB kernel: [43741.57573
openzfs/zfs,https://github.com/openzfs/zfs/issues/8411,openzfs_zfs_issues_8411,"zpool corrupt metadata improve error message

### System information Type | Version/Name --- | --- Distribution Name | ArchLinux Distribution Version | latest, rolling Linux Kernel | 4.20.6-arch1-1-ARCH Architecture | AMD64 ZFS Version | 0.7.12_4.20.6.arch1.1-1] SPL Version | 0.7.12_4.20.6.arch1.1-1 ### Describe the problem you're observing This is pretty simple, if any one of my mirrors disappears, for example I have 3 mirrors and only 2 are available, ZFS says the metadata is corrupted and needs to be destroyed and recreated, it slipped my mind that I had 3 mirrors and thought the 2 were normal and I was at a loss with my data. Everything points to lost data as shown below, trying to clear it using 'zpool clear zpool' returns no such pool. Finally, I ran a 'zdb' and found that I have 3 mirrors instead of 2, so I went and checked my drives and something was a little loose and after a reboot I was back in action. It could of been bad if I just recreated it, maybe a good addition would be to show missing disks when a mirror goes missing not just remove it from the zpool status/import commands. zpool import pool: zpool id: state: FAULTED status: The pool metadata is corrupted. action: The pool cannot be imported due to damaged devices or data. The pool may be active on another system, but can be imported using the '-f' flag. see: http://zfsonlinux.org/msg/ZFS-8000-72 config: zpool FAULTED corrupted data ### Describe how to reproduce the problem Pull a mirror out of the pool and boot it up",Can you give us more information about your setup? You seem to have removed the issue template. Thanks edit: possibly related to #8175,"### System information Type | Version/Name --- | --- Distribution Name | ArchLinux Distribution Version | latest, rolling Linux Kernel | 4.20.6-arch1-1-ARCH Architecture | AMD64 ZFS Version | 0.7.12_4.20.6.arch1.1-1] SPL Version | 0.7.12_4.20.6.arch1.1-1 ### Describe the problem you're observing"
openzfs/zfs,https://github.com/openzfs/zfs/issues/9555,openzfs_zfs_issues_9555,"Fedora30,zfs-0.8.2-1 ""cannot receive incremental stream: checksum mismatch or incomplete stream"" on old snapshot

### System information Type | Version/Name --- | --- Distribution Name | Fedora Distribution Version | 30 Linux Kernel | 5.3.7-200 Architecture | x86_64 ZFS Version | 0.8.2-1 SPL Version | 0.8.2-1 ### Describe the problem you're observing Hello. I am reluctant to open this issue since it may not be relevant to many users. This is also the first time I use GitHub so I will try to format my issue properly. I encounter the following error when applying an incremental snapshot:  The error does not occur immediately, but after some time, somewhere within the stream. As the text suggests, the snapshot was created previously and saved to a file; unfortunately it does not exist in a pool anymore. I was able to process the snapshot with , which I understand to indicate that the snapshot is ""valid"" and that the error I received was incorrect.  I have changed the name of the dataset in question to 'x'; I hope this is OK. A number of such incremental snapshots had been applied already without issue. The datetime they were created is indicated as the snapshot name; in general there were several snapshots generated per day, and the last one is dated . So in particular the snapshot at  did not have any issues being applied. I note this just in case it might be an indication of a behavior which was introduced by a version change on the sender that may have occurred between those two times. If necessary, I may be able to establish whether such a version change took place, and specifically which version was in use, if the appropriate records still exist on my system(s). I also tried applying the snapshot using zfs-fuse. In that case, the error message was different:  Other notes: - The system performing the receive is running inside a VM. - The system which performed the send was Fedora, running zfs 0.7, likely 0.7.9 or there-abouts (_Edit: I am doubting whether this is correct, it may have actually been 0.7.12. I recall using 0.7.12 for some time._). I am not sure exactly but I may be able to determine this if I can locate the appropriate records.  on the receiving side:   on the sending side, **as it exists today** (not at 2018-08-30, when the snapshot was created)  Since the application of the snapshot fails mid-file, I was thinking I could maybe isolate where the failure takes place:  and then compare the output of  and , to indicate the block/location which would appear to trigger the error. I'm not sure if this would be useful. I am open to suggestions for whatever steps I should take. ### Describe how to reproduce the problem Install Fedora 30, install zfsonlinux, apply the sequence of snapshots. Unfortunately I do not think I can share the snapshots as they are large and contain sensitive information. Aside from the installation of a few user-applications, the only actions I have taken on this newly installed system have been to install zfsonlinux. ### Include any warning/errors/backtraces from the system logs I was not aware of any. I am available to provide any relevant information. Thank you.","Do you have both the compressed and an uncompressed version on the source? If so, try with the file zstreamdump sees as valid, instead of the compressed one?","(_Edit: I am doubting whether this is correct, it may have actually been 0.7.12. I recall using 0.7.12 for some time._)"
fullcalendar/fullcalendar,https://github.com/fullcalendar/fullcalendar/issues/4756,fullcalendar_fullcalendar_issues_4756,"Issues occur when wrapped in Polymer LitElement

I am trying to wrap Fullcalendar as a Polymer LitElement component. I have encountered two styling issues which probably are caused by missing HTML class. * prev and next on header are not styled. They are displayed as e900 and e901, respectively. * timeGrid view: All cells in focused column instead of the focused single cell are selected. Class fc-highlight is not added after cells are selected. Which key parts have I missed during wrapping? file index.html:  file fc.js:  file ./core-style.js:  file ./daygrid-style.js:  file ./timegrid-style.js:  These source files are bundled with tools _Rollup_ and _BABEL_ into two files index.html and fc.js, which are uploaded to [jsfiddle](https://jsfiddle.net/cnliou/y39pxoq7/) and runnable by Opera v58 (Firefox v60.3 appears to eat up my CPU but not respond). However, command _npm run buid_ prints these warnings: [log.txt](https://github.com/fullcalendar/fullcalendar/files/3365635/log.txt)","Would you be able to post a [runnable, stripped-down demonstration of the bug](http://fullcalendar.io/wiki/Reduced-Test-Cases/)? Would really appreciate it because the time saved reproducing will be time spent fixing.","import {timeGridStyle} from './timegrid-style.js'; import '@fullcalendar/interaction/main.js'; dayGrid','Month,timeGrid,timeGridDay file ./core-style.js:  file ./daygrid-style.js:  file ./timegrid-style.js:  These files are bundled with tools _Rollup_ and _BABEL_ into two files index.html and fc.js, which is runnable at [jsfiddle](https://jsfiddle.net/cnliou/y39pxoq7/2/) by Opera (Firefox appears to eat up CPU but not respond). However, command _npm run buid_ prints these warnings: [log.txt](https://github.com/fullcalendar/fullcalendar/files/3365635/log.txt)"
fullcalendar/fullcalendar,https://github.com/fullcalendar/fullcalendar/issues/4999,fullcalendar_fullcalendar_issues_4999,"The last timeslot is wider than the slotWidth if the slotWidth is small enough that the slots do not fill the width of the calendar

I am using the ""resource timeline"" and a very low slotWidth setting of 6 or 8. (with tiny font-size) but the right column behaves wrong on most responsive points. View it here: ![Screenshot 2019-08-23 at 08 45 41](https://user-images.githubusercontent.com/14107298/63572767-f7c85600-c583-11e9-98c1-c256ba6fb31d.png) On responsive points it jumps to the correct size but on places more of the responsive points the column gets bigger. I guess it is a bug? I'm going to define my own exact widths for fullCalendar as a solution on the outer wrapper to solve this. So the wrapper always jumps to the points that it looks good. Edit: Here is the codepen https://codepen.io/Julesezaar/pen/oNvZrjy?&editable=true&editors=001","Would you be able to post a [runnable, stripped-down demonstration of the bug](http://fullcalendar.io/wiki/Reduced-Test-Cases/)? Would really appreciate it because the time saved reproducing will be time spent fixing.",Edit: Here is the codepen https://codepen.io/Julesezaar/pen/oNvZrjy?&editable=true&editors=001
fullcalendar/fullcalendar,https://github.com/fullcalendar/fullcalendar/issues/5037,fullcalendar_fullcalendar_issues_5037,"Internet Explorer 11 Timeline Year View on remote desktop session displays dates with longdatestring

On Internet Explorer 11 on the remote desktop session at the company I work for the dates display in long format - stretching the days and the events. ![image](https://user-images.githubusercontent.com/55244693/64792637-9f2c1d80-d571-11e9-9896-f5c0f8743e02.png) This issue only occurs on Internet Explorer 11 and not on any other browser on the remote desktop session, such as Chrome as can be seen below. ![image](https://user-images.githubusercontent.com/55244693/64792912-1497ee00-d572-11e9-9846-d965d63e9655.png) Screenshot of calendar setup - ![image](https://user-images.githubusercontent.com/55244693/64844393-93873800-d5ff-11e9-953b-37999927bed9.png) If I use Internet Explorer 11 off the remote desktop session this issue does not occur. I was just wondering if this issue has happened before and if anyone has any solutions or ideas about how to resolve it. Thanks.","Would you be able to post a [runnable, stripped-down demonstration of the bug](http://fullcalendar.io/wiki/Reduced-Test-Cases/)? Would really appreciate it because the time saved reproducing will be time spent fixing.",Screenshot of calendar setup - ![image](https://user-images.githubusercontent.com/55244693/64844393-93873800-d5ff-11e9-953b-37999927bed9.png)
vanilla/vanilla,https://github.com/vanilla/vanilla/issues/8858,vanilla_vanilla_issues_8858,"Attachment Upload Problem

Attachment failed to add to comment with error message 'You are not allowed to upload files in this category'. 1. Enable permissions on attachments. 1. Activate Advanced Editor. 1. Click the add file - uploads OK. 1. Click to 'Post Comment' 1. Error message displayed Seems to be class.editor.plugin.php line 998 **original:** && $category['AllowFileUploads'] !==1 **suggested:** && $category['AllowFileUploads'] !=1 on local IIS the value of $category['AllowFileUploads'] is 1, on Linux shared hosting $category['AllowFileUploads'] is ""1""",Could you describe the hosting environment where you're having issues in more detail? - PHP version - DB version,1. 1. 1. 1. 1. **
celery/celery,https://github.com/celery/celery/issues/5836,celery_celery_issues_5836,"celery worker has --quiet to suppress banner output but celery beat does not

Sorry for leaving out the issue template but I believe this is fairly trivial and straight-forward. In master, there is code to enable the suppressing of printing the banner when running celery worker with --quiet: https://github.com/celery/celery/blob/9773eba837982c84380c93bd3788470273e7674d/celery/apps/worker.py#L138-L139 This conditional is not present in the code that runs celery beat: https://github.com/celery/celery/blob/9773eba837982c84380c93bd3788470273e7674d/celery/apps/beat.py#L77-L78 https://github.com/celery/celery/blob/9773eba837982c84380c93bd3788470273e7674d/celery/apps/beat.py#L100 This causes a few issues for us because we expect all our services to only emit JSON.",do you have the time to send improvements?,This causes a few issues for us because we expect all our services to only emit JSON.
celery/celery,https://github.com/celery/celery/issues/4885,celery_celery_issues_4885,"High memory usage in producer code while using apply_async in celery 4.1 with Redis as a broker

**Update: O/p of /usr/bin/celery -A proj report** software -> celery:4.1.0 (latentcall) kombu:4.1.0 py:3.6.2 billiard:3.5.0.3 redis:2.10.6 platform -> system:Linux arch:64bit, ELF imp:CPython loader -> celery.loaders.app.AppLoader settings -> transport:redis results:redis://:**@127.0.0.1:6379/1 BROKER_URL: 'redis://:********@127.0.0.1:6379/1' CELERY_RESULT_BACKEND: 'redis://:********@127.0.0.1:6379/1' CELERY_INCLUDE: ['services.tasks'] CELERY_TASK_RESULT_EXPIRES: '5' CELERY_ACCEPT_CONTENT: ['json'] CELERY_RESULT_SERIALIZER: 'json' CELERY_ACKS_LATE: 'True' CELERYD_PREFETCH_MULTIPLIER: '1' BROKER_TRANSPORT_OPTIONS: { 'visibility_timeout': 43200} BROKER_TRANSPORT_OPTIONS: { 'visibility_timeout': 43200} CELERY_RESULT_SERIALIZER: 'json' CELERY_ACCEPT_CONTENT: ['json'] CELERY_ACKS_LATE: 'True' CELERY_TASK_RESULT_EXPIRES: '5' CELERYD_PREFETCH_MULTIPLIER: '1' Hi, we have producer code which keeps on running and inserts tasks in a Redis , where consumer code picks tasks from the Redis and perform certain operations. Now after certain days we observe strange behaviour with prouder process where memory usage of producer shoots up drastically. For example: after running 10-15 days memory usage remains ~500 MB but then it shoots up to ~20 GB and then keep on increasing. We observe following logs in producer code: **Could not insert task with error[maximum recursion depth exceeded]** Traceback (most recent call last): File ""/usr/lib/python3.6/site-packages/redis/connection.py"", line 590, in send_packed_command self._sock.sendall(item) ConnectionResetError: [Errno 104] Connection reset by peer During handling of the above exception, another exception occurred: Traceback (most recent call last): File ""/usr/lib/python3.6/site-packages/redis/client.py"", line 2408, in _execute return command(*args) File ""/usr/lib/python3.6/site-packages/redis/connection.py"", line 610, in send_command self.send_packed_command(self.pack_command(*args)) File ""/usr/lib/python3.6/site-packages/redis/connection.py"", line 603, in send_packed_command (errno, errmsg)) redis.exceptions.ConnectionError: Error 104 while writing to socket. Connection reset by peer. During handling of the above exception, another exception occurred: Traceback (most recent call last): File ""file.py"", line 975, in <module> serializer=common.serializer_value) File ""/usr/lib/python3.6/site-packages/celery/app/task.py"", line 536, in apply_async **options File ""/usr/lib/python3.6/site-packages/celery/app/base.py"", line 736, in send_task self.backend.on_task_call(P, task_id) File ""/usr/lib/python3.6/site-packages/celery/backends/redis.py"", line 189, in on_task_call self.result_consumer.consume_from(task_id) File ""/usr/lib/python3.6/site-packages/celery/backends/redis.py"", line 76, in consume_from self._consume_from(task_id) File ""/usr/lib/python3.6/site-packages/celery/backends/redis.py"", line 82, in _consume_from self._pubsub.subscribe(key) File ""/usr/lib/python3.6/site-packages/redis/client.py"", line 2482, in subscribe ret_val = self.execute_command('SUBSCRIBE', *iterkeys(new_channels)) File ""/usr/lib/python3.6/site-packages/redis/client.py"", line 2404, in execute_command self._execute(connection, connection.send_command, *args) File ""/usr/lib/python3.6/site-packages/redis/client.py"", line 2415, in _execute connection.connect() File ""/usr/lib/python3.6/site-packages/redis/connection.py"", line 502, in connect callback(self) File ""/usr/lib/python3.6/site-packages/redis/client.py"", line 2376, in on_connect self.subscribe(**channels) **Also we get following error message in Redis logs:** Client id=1665824 addr=127.0.0.1:56318 fd=125 name= age=8 idle=0 flags=N db=1 sub=835676 psub=0 multi=-1 qbuf=0 qbuf-free=32768 obl=0 oll=4259 omem=104778152 events=rw cmd=subscribe scheduled to be closed ASAP for overcoming of output buffer limits. ******Please Note:** On different servers we get recursion depth exceeded error and error mentioned in Redis.log but stack trace is not printed on every server. So it seems to suggest that either there is some issue when we exceed recursion depth while performing ""apply_async"" or issue is with client output buffer of Redis. Also we don't see significant increase in memory usage of Redis as that in case of producer code.** I am not able to figure out exact root cause, any help will be much appreciated. Thanks in advance.",Could you try upgrading and report back? Thanks.,"**Update: O/p of /usr/bin/celery -A proj report** software -> celery:3.1.16 (Cipater) kombu:3.0.23 py:2.6.6 billiard:3.3.0.18 redis:2.10.3 platform -> system:Linux arch:64bit, ELF imp:CPython loader -> celery.loaders.app.AppLoader settings -> transport:redis results:redis://:WAjvGxey5vj52TFGQdVHxpgXE9Ez6Z7KTaYBJeYan9PXSGw8zMGTaszeya7D9pJh@127.0.0.1:6379/1 BRT_OPTION{ 'visibility_timeout': 43200} NTENT: ['json'] CELERY_ACK"
celery/celery,https://github.com/celery/celery/issues/5229,celery_celery_issues_5229,"Issue with chord group task exceptions and Redis backend

## Checklist - [x] I have included the output of celery -A proj report in the issue. (if you are not able to do this, then at least specify the Celery version affected). - [x] I have verified that the issue exists against the master branch of Celery. software -> celery:4.2.0 (windowlicker) kombu:4.2.1 py:2.7.15 billiard:3.5.0.3 py-amqp:2.2.2 platform -> system:Darwin arch:64bit imp:CPython loader -> celery.loaders.app.AppLoader settings -> transport:pyamqp results:redis://localhost:6379/0 task_serializer: 'pickle' task_soft_time_limit: 3600 redis_max_connections: None task_bucket: 'tasks' enable_utc: True beat_scheduler: beat_max_loop_interval: 5 worker_redirect_stdouts: False accept_content: ['pickle'] result_serializer: 'pickle' broker_pool_limit: 10 db_port: '8091' task_acks_late: False result_expires: 86400 task_track_started: True task_routes: { '': { 'queue': 'priority.high'}} str_to_bool: <function str_to_bool at 0x10744daa0> db_user: 'i' db_host: 'localhost' broker_url: u'amqp://guest:********@localhost:5672//' result_backend: u'redis://localhost:6379/0' ## Steps to reproduce ### testrunner.py  ### tasks.py  ## Expected behavior Tasks exception is handled properly and the state of the task is set to failed on the backend. No crash to the celery worker. ## Actual behavior  This seems to be an issue with the request.chord dict getting passed to the maybe_signature() method in celery/backends/redis.py on line 358.  This is then passed into Signature.from_dict(). Here there is no verification code on the dictionary so it creates a signature without an ID property. This then gets passed into self.chord_error_from_stack()  Then this method tries a few times to use callback.id to set the failed state on the backend. Since there is no callback.id property these generates and internal exception. In celery 4.1.1 this does not crash the worker but the task is never set to a failed state it is always stuck in a started state. In celery 4.2.0 (master) this causes the worker to crash but the cleanup processes sets the state of the task to failed. A hacky work around is to add this to check if callback.id is None and if so set it to the desired task id held inside of callback.tasks[n].id",Do you happen to have a test case that will help me reproduce the problem?,### testrunner.py  ### tasks.py 
pkp/pkp-lib,https://github.com/pkp/pkp-lib/issues/3721,pkp_pkp-lib_issues_3721,"[OJS 3.1.1] missing locale key grid.action.schedulePublication

In Production stage, is missing a string of text grid.action.schedulePublication for html title: ![captura de pantalla 2018-05-22 00 22 57](https://user-images.githubusercontent.com/5422984/40342283-d9ce84ca-5d57-11e8-8bcb-df4caf840ff8.png) (Edit: also grid.action.viewEmail https://github.com/pkp/pkp-lib/issues/3742)",What version are you on?,(Edit: also grid.action.viewEmail https://github.com/pkp/pkp-lib/issues/3742)
ohmyzsh/ohmyzsh,https://github.com/ohmyzsh/ohmyzsh/issues/6907,ohmyzsh_ohmyzsh_issues_6907,"git plugin not working

not sure what I'm doing wrong but I installed zsh with homebrew using brew install zsh I'm running this in **OS X Sierra** I'm trying these commends in **iTerm2** echo $SHELL returns /bin/bash zsh --version returns: **zsh 5.2 (x86_64-apple-darwin16.0)** $SHELL --version gives me **zsh 5.2 (x86_64-apple-darwin16.0)** **curl** is installed Here's my ~/.zshrc file:  Tried a command such as gco and got zsh: command not found: gco","What's the output of alias gco, echo $plugins, and ls $ZSH/plugins?",** I'm trying these commends in **iTerm2
ohmyzsh/ohmyzsh,https://github.com/ohmyzsh/ohmyzsh/issues/5162,ohmyzsh_ohmyzsh_issues_5162,"failed to load module zsh/datetime'

**TL;DR:** should be fixed by using the same zsh binary as the one being executed ---- I installed oh-my-zsh on a remote machine. It gives the following error every time a zsh shell starts:  This is odd because when I run zmodload zsh/datetime in the shell, it works. What could be causing this?",How did you solve it? Was it a difference between the zsh used in $SHELL and the zsh used by respecting the $PATH order?,**TL;DR:** should be fixed by using the same zsh binary as the one being executed ----
ohmyzsh/ohmyzsh,https://github.com/ohmyzsh/ohmyzsh/issues/8452,ohmyzsh_ohmyzsh_issues_8452,"cd command sometimes hangs

<!-- Fill this out before posting. You can delete irrelevant sections, but an issue where no sections have been filled will be deleted without comment. --> **Describe the bug** when I input cd command , the cursor stays at the start of line , then type any key can make it continue working. it happens sometimes. when I set ZSH_THEME to """", cd command works well . **To Reproduce** hard to reproduce, it happens randomly **Screenshots or recordings** the cursor stays at the start of line ![image](https://user-images.githubusercontent.com/4841386/70144096-281da580-16d8-11ea-8f48-7e6cddacbb8d.png) **System:** - OS: macOS - Zsh version: 5.7.1 - Terminal emulator: Term2","Can you explain more? It's not clear what you mean by stopped, or what it's supposed to happen in that screenshot.","to """" **To Reproduce** hard to reproduce, it happens randomly **Screenshots or recordings** the cursor stays at the start of line **System:** - OS: macOS - Zsh version: 5.7.1 - Terminal emulator: Term2"
expressjs/express,https://github.com/expressjs/express/issues/4246,expressjs_express_issues_4246,"Router should honour disabling 'X-Powered-By' on app

NOTE: Not an issue. I was importing express instead of Router, like import Router from 'express'. This was due to using node experimental esm support via ""type"": ""module"" set in package.json. If I try to import Router the correct way - import { Router } from 'express' - it yells at me: SyntaxError: The requested module 'express' does not provide an export named 'Router'. Using express.Router() solves the issue. I can set app.disable('x-powered-by') and it will stop emitting the header alright. But then I have to do it for every instance for Router() as well, as in: router.disable('x-powered-by'). This is not a major issue in a small scale projects but becomes a PITA when there's many router instance spread across numerous files. Code repetition, even if it is one line is not fun ;-) Can Router() read global app settings and use those _if_ they differ from the defaults or those not explicitly set on the Router()'s instance? Or perhaps I missed something here? [Repo to reproduce the issue](https://gitlab.com/b00jum/x-powered-by).",Can you provide an example reproduction case?,[Repo to reproduce the issue](https://gitlab.com/b00jum/x-powered-by).
sbt/sbt,https://github.com/sbt/sbt/issues/5427,sbt_sbt_issues_5427,"Macros in Scala.js No Longer Access the JVM

**Steps** 1. Write a Macro that makes use of java.io.File 2. Use that Macro in a Scala.js enabled project **Problems** As of 1.3.4, I get errors Referring to non-existent class java.io.File **Expectations** Previous to 1.3.4, this would work just fine and any JVM operation could be dealt with inside the Macro.",Could you follow the issue template and the bug report guideline please? https://github.com/sbt/sbt/blob/develop/CONTRIBUTING.md#what-to-report,**Steps** 1. Write a Macro that makes use of java.io.File 2. Use that Macro in a Scala.js enabled project **Problems** 
sbt/sbt,https://github.com/sbt/sbt/issues/5400,sbt_sbt_issues_5400,"Upgrading Play Project to 1.3.7 triggers incorrect warnings

sbt version: *1.3.7* ## steps https://github.com/hmrc/cgt-property-disposals-frontend Clone it, open up an sbt session and run compile. You will see the warnings. ## problem Upgrading a Play 2.6 project to use sbt 1.3.7 is triggering warnings on Twirl templates having unused imports:  That is actually a Twirl comment. and  This Play route is actually used. ## expectation Should not trigger warnings in these cases.",Could you follow https://github.com/sbt/sbt/blob/develop/CONTRIBUTING.md#what-to-report and describe the steps needed to reproduce the problems on our machines please?,"steps https://github.com/hmrc/cgt-property-disposals-frontend Clone it, open up an sbt session and run compile. You will see the warnings. ##"
GoldenCheetah/GoldenCheetah,https://github.com/GoldenCheetah/GoldenCheetah/issues/2891,GoldenCheetah_GoldenCheetah_issues_2891,"HRV Measurements from iOS

Hi, im getting format error from a CSV imported from an iOS phone. Its file from HRV4training. Downloading of HRV measurements failed with error: invalid 'timestamp' - Date/Time not ISO 8601 format - in line 2 7:33:00 Using Version 3.5-DEV1806 If i could help anything, let me know rault_ruiz@hotmail.com This is the file from the athlete i have issues. If you cant recreate, then is my computer. Thanks anyway! [2018_7_8_myMeasurements.zip](https://github.com/GoldenCheetah/GoldenCheetah/files/2193535/2018_7_8_myMeasurements.zip)",Can you attach the file for debugging?,"This is the file from the athlete i have issues. If you cant recreate, then is my computer. Thanks anyway! [2018_7_8_myMeasurements.zip](https://github.com/GoldenCheetah/GoldenCheetah/files/2193535/2018_7_8_myMeasurements.zip)"
rzwitserloot/lombok,https://github.com/rzwitserloot/lombok/issues/1907,rzwitserloot_lombok_issues_1907,"@Builder annotation causes compile error with Java 11

Hello! My code using the @Builder annotation does not compile with Java 11 (or Java 10) while it compiles properly with Java 8. As issue #1896 was closed and I could not reopen it, I create this issue to provide the code; please see here: https://github.com/jrmyr/lombokbuilder. To reproduce the problem, just clone the repo and execute a simple gradle compileJava in it. Depending on the Java version in use, it will pass (8) or fail (10, 11). Tested on Mac. Please tell me if you need more info. Thanks.",Can you try to assign the result of the build method to a local variable first?,gradle and gradle and
jquery/jquery,https://github.com/jquery/jquery/issues/4527,jquery_jquery_issues_4527,"Textbox focus select is not working in 3.4.1

<!-- Feature Requests: Please read https://github.com/jquery/jquery/wiki/Adding-new-features Most features should start as plugins outside of jQuery. Bug Reports: Note that we only can fix bugs in the latest version of jQuery. Briefly describe the issue you've encountered * What do you expect to happen? * What actually happens? * Which browsers are affected? Provide a *minimal* test case, see https://webkit.org/test-case-reduction/ Use the latest shipping version of jQuery in your test case! We prefer test cases on JS Bin (https://jsbin.com/qawicop/edit?html,css,js,output) or CodePen (https://codepen.io/mgol/pen/wNWJbZ) Frequently Reported Issues: * Selectors with '#' break: See https://github.com/jquery/jquery/issues/2824 --> ### Description ### As you can see below, this code is to select entire textbox value on focus but it doesn't work in 3.4.1 which was working in 3.3.1. $('#textbox).focus(function () { $(this).one('mouseup', function (event) { event.preventDefault(); }).select(); }); ### Link to test case ###","Can you provide a test case on [JS Bin](https://jsbin.com/qawicop/edit?html,css,js,output) as the issue template requested? Thanks!","As you can see below, this code is to select entire textbox value on focus but it doesn't work in 3.4.1 which was working in 3.3.1. $('#textbox).focus(function () { $(this).one('mouseup', function (event) { event.preventDefault(); }).select(); });"
SDWebImage/SDWebImage,https://github.com/SDWebImage/SDWebImage/issues/2471,SDWebImage_SDWebImage_issues_2471,"images didn't cached

### New Issue Checklist * [X] I have read and understood the [CONTRIBUTING guide](https://github.com/rs/SDWebImage/blob/master/.github/CONTRIBUTING.md) * [X] I have read the [Documentation](http://cocoadocs.org/docsets/SDWebImage/) * [X] I have searched for a similar issue in the [project](https://github.com/rs/SDWebImage/issues) and found none ### Issue Info Info | Value | -------------------------|-------------------------------------| Platform Name | ios Platform Version | 11.4 SDWebImage Version | 4.2.2 Integration Method | cocoapods Xcode Version | Xcode 9 Repro rate | e.g. all the time (100%) / sometimes x% / only once Repro with our demo prj | e.g. does it happen with our demo project? Demo project link | e.g. link to a demo project that highlights the issue Dears, I have a problem when cache image in table view or collection view the problem is when load first cells the images has been loaded then move scroll down then back to up the images already loaded it's clear and download again, there issue i'm not sure if related a problem, when move to down some images return error from cache ""canceled"" to more details please see the [video](https://drive.google.com/open?id=1N4HHyGJRGOWUc2Bc5Cy97ZbzVvfjkA7l) Many Thanks Regards",Did you load the image with FLAnimatedImageView in 4.x ? It's also recommend to provide a code snippet of your usage or a demo project to reproduce this issue.,"### New Issue Checklist * [ Yes] I have read and understood the [CONTRIBUTING guide](https://github.com/rs/SWebImage/blob/master/.github/CONTRIBUTING.md) * [ Yes] I have read the [Documentation](http://cocoadocs.org/docsets/SDWebImage/) * [Yes ] I have searched for a similar issue in the [project](https://github.com/rs/SDWebImage/issues) and found none ### Issue Info Info | Value | -------------------------|-------------------------------------| Platform Name | ios Platform Version | 11.4 SDWebImage Version | 4.2.2 Integration Method | cocoapods Xcode Version | Xcode 9 Repro rate | e.g. all the time (100%) / sometimes x% / only once Repro with our demo prj | e.g. does it happen with our demo project? Demo project link | e.g. link to a demo project that highlights the issue D my code : self.productImage.sd_setImage(with: url1, placeholderImage:Helper.sharedInstance.getPlaceholderImg())"
pymc-devs/pymc3,https://github.com/pymc-devs/pymc3/issues/3330,pymc-devs_pymc3_issues_3330,"TypeError: zip argument #1 must support iteration

## Description of your problem **Please provide a minimal, self-contained, and reproducible example.**  **Please provide the full traceback.**  **Please provide any additional information below.** I wanted to sample sums over columns of counts, but sum() didn't work for me and then even simple creation of Deterministic node led to a Type error. ## Versions and main components * PyMC3 Version: 3.6 * Theano Version: 1.0.3 * Python Version: 3.6.6 * Operating system: osX * How did you install PyMC3: (conda/pip) pip with the latest version from github",Maybe @lucianopaz has an idea?,theano.tensor as tt import import numpy as np from scipy.special import gammainc
rakudo/rakudo,https://github.com/rakudo/rakudo/issues/1905,rakudo_rakudo_issues_1905,"Not able to build on Windows 7 - 64bit.

***See [Newcomer Guide to Contributing to Core Perl 6](https://rakudo.party/post/Newcomer-Guide-to-Contributing-to-Core-Perl-6) for tips on fixing this Issue*** ---------------- ## The Problem Not able to build on Windows 7 - 64bit. ## Expected Behavior ## Actual Behavior Fail message  ## Steps to Reproduce 1) get source package 2018.05 2) Run configure command perl Configure.pl --gen-moar --gen-nqp --backends=moar ## Environment * Operating system: Win7 64bit ## Log build message",Which C compiler do you have?,***See [Newcomer Guide to Contributing to Core Perl 6](https://rakudo.party/post/Newcomer-Guide-to-Contributing-to-Core-Perl-6) for tips on fixing this Issue*** ----------------
greasemonkey/greasemonkey,https://github.com/greasemonkey/greasemonkey/issues/3020,greasemonkey_greasemonkey_issues_3020,"XMLHttpRequest/GM.xmlHttpRequest ignore cookies

My script works fine in scratchpad without any problem regardless without the workaround I was making a script and and i needed to fetch data from another page on the same site but this page requires you to be logged in to get the data, but when the request is made it acts as though i am not logged in when the request is made with XMLHttpRequest/GM.xmlHttpRequest everything is same origin complaint, but if i make a XMLHttpRequest via scratchpad it works as expected Changing this setting to this makes my script work: ![screenshot_2019-02-09_19-27-39](https://user-images.githubusercontent.com/564653/52528023-fcd94e00-2ca0-11e9-914c-20fb87d44bae.png) when a request is made via XMLHttpRequest/GM.xmlHttpRequest via GM cookie data is not transmitted when the browsers is set to only block cookies from sites that have not been visited, in spite of the target domain being the same as current domain, again this is not reproduce-able in a scratchpad only in GM",Could you clean up your question? It is a bit hard to read now...,", but when the request is made it acts as though i am not logged in when the request is made with XMLHttpRequest/GM.xmlHttpRequest everything is same origin complaint"
greasemonkey/greasemonkey,https://github.com/greasemonkey/greasemonkey/issues/3065,greasemonkey_greasemonkey_issues_3065,"Script does not load on raw.githubusercontent.com pages

**Update**: see https://github.com/greasemonkey/greasemonkey/issues/3065#issuecomment-583770358 <br> Try the following snippet of code on https://raw.githubusercontent.com/greasemonkey/greasemonkey/master/peg.txt and https://releases.llvm.org/2.8/LICENSE.TXT  It works on the latter license.txt file , but not the peg.txt (files on raw githubusercontent) Why? both have document.contentType == ""text/plain"" ![image](https://user-images.githubusercontent.com/5124946/70874610-e666e780-1f80-11ea-8fbd-dec340c3dd05.png) Firefox 71.0 Ubuntu 18.04","What (value of document.contentType) are you seeing, and what do you expect?",**Update**: see https://github.com/greasemonkey/greasemonkey/issues/3065#issuecomment-583770358 <br>
tornadoweb/tornado,https://github.com/tornadoweb/tornado/issues/2827,tornadoweb_tornado_issues_2827,"Python 3.7.5 AttributeError: module 'asyncio' has no attribute 'Future'

When running the starter code provided on the main page:  I get this error:  Installed tornado using: pip3 install tornado running Python 3.7.5","Could you provide more context? asyncio most definitely does have a Future attribute, so there's probably something else going on that we're not seeing in the above traceback.",When running the starter code provided on the main page: I get this error:  Installed tornado using: pip3 install tornado running Python 3.7.5
middleman/middleman,https://github.com/middleman/middleman/issues/2280,middleman_middleman_issues_2280,"url_for produces the wrong link in some cases with localized pages

## Expected behavior and actual behavior url_for produces the wrong link in some cases when used from localized pages. Apparently this happens when the following conditions are met: * url_for is being used from a localized URL for a language that is *not* mapped at root * The page name passed to url_for includes the page extension (e.g. products.html) ## Steps to reproduce the problem (from a clean middleman installation) Here's a test repo: https://github.com/guillerodriguez/middleman-url_for-test If you browse the index.html page in this repo, the results are as follows: - URL for index.html,en: / - URL for index.html,fr: /fr/ - URL for index.html,es: /es/ - URL for test.html,en: /test.html - URL for test.html,fr: /fr/test.html - URL for test.html,es: /es/test.html - URL for index,en: / - URL for index,fr: /fr/ - URL for index,es: /es/ - URL for test,en: /test.html - URL for test,fr: /fr/test.html - URL for test,es: /es/test.html These are all OK. However if you browse /es/index.html, the results are as follows: - URL for index.html,en: /es/ - URL for index.html,fr: /es/ - URL for index.html,es: /es/ - URL for test.html,en: /es/test.html - URL for test.html,fr: /es/test.html - URL for test.html,es: /es/test.html - URL for index,en: / - URL for index,fr: /fr/ - URL for index,es: /es/ - URL for test,en: /test.html - URL for test,fr: /fr/test.html - URL for test,es: /es/test.html In this case, the first 6 results are wrong. ## Additional information - Ruby version: 2.3.8p459 - Middleman version: 4.3.4 - OS version: Ubuntu 14",Would you be able to contribute a failing test case (like a tiny middleman repo which shows this behavior)? It'll make it much quicker to diagnose.," is being used from a localized URL for a language that is *not* mapped at root * The page name passed to url_for includes the page extension Here's a test repo: https://github.com/guillerodriguez/middleman-url_for-test If you browse the index.html page in this repo, the results are as follows: - URL for index.html,en: / - URL for index.html,fr: /fr/ - URL for index.html,es: /es/ - URL for test.html,en: /test.html - URL for test.html,fr: /fr/test.html - URL for test.html,es: /es/test.html - URL for index,en: / - URL for index,fr: /fr/ - URL for index,es: /es/ - URL for test,en: /test.html - URL for test,fr: /fr/test.html - URL for test,es: /es/test.html These are all OK. However if you browse /es/index.html, the results are as follows: - URL for index.html,en: /es/ - URL for index.html,fr: /es/ - URL for index.html,es: /es/ - URL for test.html,en: /es/test.html - URL for test.html,fr: /es/test.html - URL for test.html,es: /es/test.html - URL for index,en: / - URL for index,fr: /fr/ - URL for index,es: /es/ - URL for test,en: /test.html - URL for test,fr: /fr/test.html - URL for test,es: /es/test.html In this case, the first 6 results are wrong."
SpecFlowOSS/SpecFlow,https://github.com/SpecFlowOSS/SpecFlow/issues/1920,SpecFlowOSS_SpecFlow_issues_1920,"Feature files vanishes from project hierarchy on mac (.net Core)

### SpecFlow Version: - [X] 3.1 - [ ] 3.0 - [ ] 2.4 - [ ] 2.3 - [ ] 2.2 - [ ] 2.1 - [ ] 2.0 - [ ] 1.9 ### Used Test Runner - [ ] SpecFlow+Runner - [ ] MSTest - [X] NUnit - [ ] Xunit Version number: 3.15.1 ### Project Format of the SpecFlow project - [ ] Classic project format using packages.config - [X] Classic project format using <PackageReference> tags - [ ] Sdk-style project format ### .feature.cs files are generated using - [X] SpecFlow.Tools.MsBuild.Generation NuGet package - [ ] SpecFlowSingleFileGenerator custom tool ### Visual Studio Version - [X] VS 2019 - [ ] VS 2017 - [ ] VS 2015 ### Enable SpecFlowSingleFileGenerator Custom Tool option in Visual Studio extension settings - [ ] Enabled - [X] Disabled ### Are the latest Visual Studio updates installed? - [X] Yes - [ ] No, I use Visual Studio version <Major>.<Minor>.<Patch> <!-- e.g. 16.1.0 --> ### .NET Framework: - [ ] >= .NET 4.5 - [ ] before .NET 4.5 - [ ] .NET Core 2.0 - [ ] .NET Core 2.1 - [ ] .NET Core 2.2 - [ ] .NET Core 3.0 - [X] .NET Core 3.1 ### Test Execution Method: - [X] Visual Studio Test Explorer - [ ] TFS/VSTS/Azure DevOps – Task – PLEASE SPECIFY THE NAME OF THE TASK - [ ] Command line – PLEASE SPECIFY THE FULL COMMAND LINE ### Repro Project It's a simple .net core 3.1 with the packages added. I can provide if really necessary ### Issue Description Basically, whenever I open my solution on Mac and/or add a new feature file, they simply vanish from the project. If I navigate to the folder I can see them and I can readd it to the project (but if I close the solution they are gone again). If I build the solution, 2 feature.cs files are added to the project. This shows the project AFTER building (and when the feature has already vanished, meaning I opened the solution): <img width=""1414"" alt=""Screenshot 2020-03-23 at 10 38 09"" src=""https://user-images.githubusercontent.com/4673110/77308391-c9b21c80-6cf2-11ea-91e5-b5952634f103.png""> This shows the project in the folder <img width=""616"" alt=""Screenshot 2020-03-23 at 10 38 18"" src=""https://user-images.githubusercontent.com/4673110/77308412-d20a5780-6cf2-11ea-99ee-68ad496701ad.png""> ### Steps to Reproduce - Create a .net core 3.1 project on Mac - Add specflow, specflow.nunit, specflow.tools.msbuild.generation nuget packages - Create a new feature file. - Build solution. You won't see the feature.cs in the project hierarchy anymore and 2 feature.cs (code behind) added to it",Could you please post a screenshot?,"This shows the project AFTER building (and when the feature has already vanished, meaning I opened the solution): <img width=""1414"" alt=""Screenshot 2020-03-23 at 10 38 09"" src=""https://user-images.githubusercontent.com/4673110/77308391-c9b21c80-6cf2-11ea-91e5-b5952634f103.png""> This shows the project in the folder <img width=""616"" alt=""Screenshot 2020-03-23 at 10 38 18"" src=""https://user-images.githubusercontent.com/4673110/77308412-d20a5780-6cf2-11ea-99ee-68ad496701ad.png"">"
SpecFlowOSS/SpecFlow,https://github.com/SpecFlowOSS/SpecFlow/issues/1283,SpecFlowOSS_SpecFlow_issues_1283,"specflow error

<!-- PLEASE GIVE YOUR ISSUE A SENSIBLE NAME. This makes it easier to identify issues at a glance --> <!-- PLEASE CHECK THE OPTIONS THAT APPLY TO YOU BY ADDING AN 'x' TO THE CORRESPONDING CHECKBOX ('[ ]') --> ### SpecFlow Version: - [x ] 2.4 - [ ] 2.3 - [ ] 2.2 - [ ] 2.1 - [ ] 2.0 - [ ] 1.9 ### Used Test Runner - [x] SpecFlow+Runner - [ ] MSTest - [ ] NUnit - [ ] Xunit <!-- PLEASE INCLUDE THE VERSION NUMBER OF YOUR TEST RUNNER --> Version number: ### Visual Studio Version - [x] VS 2017 - [ ] VS 2015 - [ ] VS 2013 ### Are the latest Visual Studio updates installed? - [x] Yes - [ ] No ### .NET Framework: - [x] >= .NET 4 - [ ] before .NET 4 ### Test Execution Method: - [x] Visual Studio Test Explorer - [ ] TFS/VSTS – Task – PLEASE SPECIFY THE NAME OF THE TASK - [ ] Command line – PLEASE SPECIFY THE FULL COMMAND LINE ### &lt;SpecFlow> Section in app.config <!-- PLEASE COPY THE ENTRIE <SpecFlow> SECTION IN YOUR .config FILE AND PASTE IT BETWEEN THE TWO CODE MARKERS () BELOW -->  ### Repro Project <!-- PLEASE INCLUDE A LINK TO A PROJECT THAT DEMONSTRATES THE ISSUE YOU ARE REPORTING, IF POSSIBLE For information on how to include a useful repro, refer to https://stackoverflow.com/help/mcve --> ### Issue Description <!-- not sure why I get 290 syntax errors, my files have no syntax errors. all I did was create specflow default feature and generate its step definition by clicking such button--> ### Steps to Reproduce <!-- trying to build solution--> We could not find a data exchange file at the path System.Configuration.ConfigurationErrorsException: The element <unitTestProvider> may only appear once in this section. Please open an issue at https://github.com/techtalk/SpecFlow/issues/ Complete output: System.Configuration.ConfigurationErrorsException: The element <unitTestProvider> may only appear once in this section. Command: c:\users\miguel\appdata\local\microsoft\visualstudio\15.0_950c5d43\extensions\cyzqpedl.g1s\TechTalk.SpecFlow.VisualStudio.CodeBehindGenerator.exe Parameters: GenerateTestFile --featurefile C:\Users\Miguel\AppData\Local\Temp\tmp193F.tmp --outputdirectory C:\Users\Miguel\AppData\Local\Temp --projectsettingsfile C:\Users\Miguel\AppData\Local\Temp\tmp192F.tmp Working Directory: C:\Users\Miguel\source\repos\UnitTestProject4\packages\SpecFlow.2.4.0\tools *appconfig* ) BELOW -->  ### Repro Project <!-- PLEASE INCLUDE A LINK TO A PROJECT THAT DEMONSTRATES THE ISSUE YOU ARE REPORTING, IF POSSIBLE For information on how to include a useful repro, refer to https://stackoverflow.com/help/mcve --> ### Issue Description <!-- not sure why I get 290 syntax errors, my files have no syntax errors. all I did was create specflow default feature and generate its step definition by clicking such button--> ### Steps to Reproduce <!-- trying to build solution--> We could not find a data exchange file at the path System.Configuration.ConfigurationErrorsException: The element <unitTestProvider> may only appear once in this section. Please open an issue at https://github.com/techtalk/SpecFlow/issues/ Complete output: System.Configuration.ConfigurationErrorsException: The element <unitTestProvider> may only appear once in this section. Command: c:\users\miguel\appdata\local\microsoft\visualstudio\15.0_950c5d43\extensions\cyzqpedl.g1s\TechTalk.SpecFlow.VisualStudio.CodeBehindGenerator.exe Parameters: GenerateTestFile --featurefile C:\Users\Miguel\AppData\Local\Temp\tmp193F.tmp --outputdirectory C:\Users\Miguel\AppData\Local\Temp --projectsettingsfile C:\Users\Miguel\AppData\Local\Temp\tmp192F.tmp Working Directory: C:\Users\Miguel\source\repos\UnitTestProject4\packages\SpecFlow.2.4.0\tools *appconfig*",Do you have perhaps 2 entries for unitTestProvider?,"*appconfig* <?xml version=""1.0"" encoding=""utf-8""?> <configuration> <configSections> <section name=""specFlow"" type=""TechTalk.SpecFlow.Configuration.ConfigurationSectionHandler, TechTalk.SpecFlow"" /> </configSections> <specFlow> <!-- For additional details on SpecFlow configuration options see http://go.specflow.org/doc-config --> <!-- For additional details on SpecFlow configuration options see http://go.specflow.org/doc-config --><!-- use unit test provider SpecRun+NUnit or SpecRun+MsTest for being able to execute the tests with SpecRun and another provider --><!-- For additional details on SpecFlow configuration options see http://go.specflow.org/doc-config --><unitTestProvider name=""SpecRun"" /><plugins> <add name=""SpecRun"" /> </plugins><unitTestProvider name=""NUnit"" /></specFlow> </configuration>"
SpecFlowOSS/SpecFlow,https://github.com/SpecFlowOSS/SpecFlow/issues/1801,SpecFlowOSS_SpecFlow_issues_1801,"SpecFlow 3 tests don't success when executed (works when debugging)

<!-- PLEASE CHECK THE OPTIONS THAT APPLY TO YOU BY ADDING AN 'x' TO THE CORRESPONDING CHECKBOX ('[ ]') --> ### SpecFlow Version: - [x] 3.1.76 - [x] 3.1.67 - [ ] 3.0 - [ ] 2.4 - [ ] 2.3 - [ ] 2.2 - [ ] 2.1 - [ ] 2.0 - [ ] 1.9 Checked with both versions ### Used Test Runner - [ ] SpecFlow+Runner - [ ] MSTest - [x] NUnit - [ ] Xunit <!-- PLEASE INCLUDE THE VERSION NUMBER OF YOUR TEST RUNNER --> Version number: ### Project Format of the SpecFlow project - [x] Classic project format using packages.config - [ ] Classic project format using <PackageReference> tags - [ ] Sdk-style project format ### .feature.cs files are generated using - [x] SpecFlow.Tools.MsBuild.Generation NuGet package - [ ] SpecFlowSingleFileGenerator custom tool ### Visual Studio Version - [x] VS 2019 - [ ] VS 2017 - [ ] VS 2015 ### Enable SpecFlowSingleFileGenerator Custom Tool option in Visual Studio extension settings - [ ] Enabled - [X] Disabled ### Are the latest Visual Studio updates installed? - [x] Yes - [ ] No, I use Visual Studio version <Major>.<Minor>.<Patch> <!-- e.g. 16.1.0 --> ### .NET Framework: - [x] >= .NET 4.5 - [ ] before .NET 4.5 - [ ] .NET Core 2.0 - [ ] .NET Core 2.1 - [ ] .NET Core 2.2 - [ ] .NET Core 3.0 ### Test Execution Method: - [x] Visual Studio Test Explorer - [ ] TFS/VSTS/Azure DevOps – Task – PLEASE SPECIFY THE NAME OF THE TASK - [ ] Command line – PLEASE SPECIFY THE FULL COMMAND LINE ### &lt;SpecFlow> Section in app.config or content of specflow.json <!-- PLEASE COPY THE ENTIRE <SpecFlow> SECTION IN YOUR .config FILE or THE ENTRIE specflow.json AND PASTE IT BETWEEN THE TWO CODE MARKERS () BELOW -->  ### Repro Project None ### Issue Description This issue only started after migration to specflow 3.1. With multiple Features, when running the tests in TestExplorer, the first set of steps pass successfully under the feature, however when the next feature starts it fails (and for all subsequent features). The strange part is if I run in debug (without any breakpoints anywhere) the tests all execute successfully. ### Steps to Reproduce <!-- PLEASE DESCRIBE THE STEPS REQUIRED TO REPRODUCE THIS ISSUE, IF POSSIBLE --> My issue could very well be an incompatible package that is being loaded. ### packages.config  ### Exception Message  ### Disclaimer Please note that this an older specflow implementation specific to our use case. However I've cut down the whole project to two features, Feature1 and Feature2. To rule out any other strange bits such as incorrectly implemented attributes or anything. I have removed all references to other tests and any hooks. This is basically back to two barebone feature files.) BELOW -->  ### Repro Project None ### Issue Description This issue only started after migration to specflow 3.1. With multiple Features, when running the tests in TestExplorer, the first set of steps pass successfully under the feature, however when the next feature starts it fails (and for all subsequent features). The strange part is if I run in debug (without any breakpoints anywhere) the tests all execute successfully. ### Steps to Reproduce <!-- PLEASE DESCRIBE THE STEPS REQUIRED TO REPRODUCE THIS ISSUE, IF POSSIBLE --> My issue could very well be an incompatible package that is being loaded. ### packages.config  ### Exception Message  ### Disclaimer Please note that this an older specflow implementation specific to our use case. However I've cut down the whole project to two features, Feature1 and Feature2. To rule out any other strange bits such as incorrectly implemented attributes or anything. I have removed all references to other tests and any hooks. This is basically back to two barebone feature files.",Could you create a small example project where this issue is reproducible?,76 - [x] 3.1.Checked with both versions
thorsten/phpMyFAQ,https://github.com/thorsten/phpMyFAQ/issues/1489,thorsten_phpMyFAQ_issues_1489,"Wrong Counter for inactive FAQs

The inactive counter in the overview of the faqs doesn't count correctly. See at the attached screenshot. ![screenshot 2018-09-09 um 10 54 21](https://user-images.githubusercontent.com/9133042/45262903-51417e80-b420-11e8-8b57-4569ab38d30a.png) The problem occur under 2.9.10 and 2.9.11 with the browser Safari 11.1.2 if you mix the manually activate faq and all activate in the title.",Did you checked the issue against 2.9.11 as well?,and 2.9.11 with the browser Safari 11.1.2
zeromq/libzmq,https://github.com/zeromq/libzmq/issues/3517,zeromq_libzmq_issues_3517,"configure errors because of syntax errors in the use of test shell command

*Please use this template for reporting suspected bugs or requests for help.* # Issue description On my system, where /bin/sh -> /bin/dash, runnung ./configure produces errors which I tracked down to configure.ac and acinclude.m4 using the test shell command with a == operator, syntax which is not valid in a POSIX-like shell like dash. See [man 1p test](https://www.unix.com/man-page/posix/1P/TEST/) for details. # Environment * libzmq version (commit hash if unreleased): cef80655 * OS: Void Linux x86_64 # Minimal test code / Steps to reproduce the issue 1. Run ./autogen.sh 2. Run ./configure # What's the actual result? (include assertion message & call stack if applicable)  Full output of running the repro steps on my system: step 1: [output](https://gist.github.com/f7762a406c62228a0012c55d49c6f754) step 2: [output](https://gist.github.com/65dea53e4a9a48c1131af6d0de7b2069). # What's the expected result? Running configure should produce no such errors.",Could you please send a PR to fix it?,See [man 1p test](https://www.unix.com/man-page/posix/1P/TEST/) for details.
ranger/ranger,https://github.com/ranger/ranger/issues/1214,ranger_ranger_issues_1214,"Having issues installing Ranger with anaconda python3.6

#### Runtime Environment <!-- Retrieve Python/ranger version and locale with ranger --version --> - Operating system and version: macOS High Sierra, Version 10.13.5 - Terminal emulator and version: Terminal, Version 2.8.2 (404) - Python version: Python 3.6.4, Anaconda custom (64-bit) - Ranger version/commit: 1.9.1 - Locale: I have no idea about this #### Current Behavior Whenever I try to do 'make install' in the ranger folder, I get the following error: https://imgur.com/a/g2XkVNp #### Expected Behavior I expect to install ranger using this. However running ranger with 'python2.7 ranger.py' I can succesfully run ranger. My version of python2.7 is 2.7.10 (not installed using anaconda). This in turn does not work using python3.6 nor python3 (they give the same error as shown in the screenshot). #### Traceback <!-- If ranger crashes, paste the traceback in the quotes below. -->  Best regards Jacob SOLUTION: In Terminal Preferences -> Profiles -> Advanced -> Uncheck ""Set locale environment variables on startup""",What's the output for locale on the command line?,"SOLUTION: In Terminal Preferences -> Profiles -> Advanced -> Uncheck ""Set locale environment variables on startup"""
ranger/ranger,https://github.com/ranger/ranger/issues/1890,ranger_ranger_issues_1890,"Multiple issues with kitty image preview.

#### Runtime Environment - Operating system and version: Archlinux - Terminal emulator and version: kitty 0.16 - ranger version: ranger 1.9.3 - Python version: 3.8.2 (default, Feb 26 2020, 22:21:03) [GCC 9.2.1 20200130] - Locale: en_US.UTF-8 #### Steps to reproduce 1. navigate to an image 1. open_width [dragon-drag-and-drop](https://github.com/schne324/dragon-drop) / or virtually any program. 1. ctrl+c to close dragon-drop / close the program in any way",Does this happen if you close dragon-drop a different way?,/or virtually any program.
ranger/ranger,https://github.com/ranger/ranger/issues/1437,ranger_ranger_issues_1437,"Expanded images disappear

<!-- Thank you for contributing to ranger by opening this issue. Please check through this list, so you can be as helpful as possible: 1. Was this issue already reported? Please do a quick search. 2. Maybe the problem is solved in the current master branch already? Simply clone ranger's git repository and run ./ranger.py to find out. 3. Provide all the relevant information, as outlined in this template. Feel free to remove any sections you don't need. --> #### Runtime Environment <!-- Retrieve Python/ranger version and locale with ranger --version --> - Operating system and version: Arch - Terminal emulator and version: urxvt 9.22 - Python version: 3.7.2 - Ranger version/commit: 1.9.2 - Locale: US #### Current Behavior Pressing i on an image will not show it expanded. It appears like an empty ranger. Edit: this is with the w3m method. #### Expected Behavior To view the image expanded #### Context <!-- How has this issue affected you? What are you trying to accomplish? --> I believe there was an issue about this posted here but I couldn't find it so apologies if it's true. I've tried urxvt/urxvt-full with pixbuf support which both showed the image on expansion but it looked choppy, and rendered over the text unlike below image which is perfectly aligned and centered. ![1547060685001](https://user-images.githubusercontent.com/40843060/50974541-2e8eb600-1537-11e9-8677-9d7a0e8115c9.png) ^ is a nice desktop of what I'm trying to achieve w3m simply won't work even though I asked the owner of this desktop what method he used and he replied w3m IIRC. Cheers.","Do you have w3mimgdisplay installed, should be in the w3m package on arch?",. Edit: this is with the w3m method
apache/trafficserver,https://github.com/apache/trafficserver/issues/4440,apache_trafficserver_issues_4440,"ats7.1.4 get method sometime has no response

**request** ![image](https://user-images.githubusercontent.com/17612824/47210146-c069a580-d3c4-11e8-838f-7efa64c133be.png) **ats do not responds in 500ms** ![image](https://user-images.githubusercontent.com/17612824/47275672-ec16a680-d5e3-11e8-8aa2-15b74f010cb0.png) **some key debug log (in another request** ![image](https://user-images.githubusercontent.com/17612824/47276719-bd9cc980-d5eb-11e8-9043-65424225e65f.png) **detail debug log** [Oct 19 14:22:19.944] Server {0x2ad0325cc700} DEBUG: <HttpTransact.cc:2431 (HandleCacheOpenReadHitFreshness)> (http_trans) [26480] [HandleCacheOpenReadHitFreshness] response_received_time : 1539921470 [Oct 19 14:22:19.944] Server {0x2ad0325cc700} DEBUG: <HttpTransact.cc:7296 (calculate_document_freshness_limit)> (http_match) [26480] calculate_document_freshness_limit --- max_age set, freshness_limit = 157680000 [Oct 19 14:22:19.944] Server {0x2ad0325cc700} DEBUG: <HttpTransact.cc:7387 (calculate_document_freshness_limit)> (http_match) [26480] calculate_document_freshness_limit --- final freshness_limit = 31536000 [Oct 19 14:22:19.944] Server {0x2ad0325cc700} DEBUG: <HttpTransact.cc:7517 (what_is_document_freshness)> (http_match) [26480] [what_is_document_freshness] fresh_limit: 31536000 current_age: 8669 [Oct 19 14:22:19.944] Server {0x2ad0325cc700} DEBUG: <HttpTransact.cc:7564 (what_is_document_freshness)> (http_match) [26480] [..._document_freshness] initial age limit: 31536000 [Oct 19 14:22:19.944] Server {0x2ad0325cc700} DEBUG: <HttpTransact.cc:7620 (what_is_document_freshness)> (http_match) [26480] document_freshness --- current_age = 8669 [Oct 19 14:22:19.944] Server {0x2ad0325cc700} DEBUG: <HttpTransact.cc:7621 (what_is_document_freshness)> (http_match) [26480] document_freshness --- age_limit = 31536000 [Oct 19 14:22:19.944] Server {0x2ad0325cc700} DEBUG: <HttpTransact.cc:7622 (what_is_document_freshness)> (http_match) [26480] document_freshness --- fresh_limit = 31536000 [Oct 19 14:22:19.944] Server {0x2ad0325cc700} DEBUG: <HttpTransact.cc:7623 (what_is_document_freshness)> (http_seq) [26480] document_freshness --- current_age = 8669 [Oct 19 14:22:19.944] Server {0x2ad0325cc700} DEBUG: <HttpTransact.cc:7624 (what_is_document_freshness)> (http_seq) [26480] document_freshness --- age_limit = 31536000 [Oct 19 14:22:19.945] Server {0x2ad0325cc700} DEBUG: <HttpTransact.cc:7625 (what_is_document_freshness)> (http_seq) [26480] document_freshness --- fresh_limit = 31536000 [Oct 19 14:22:19.945] Server {0x2ad0325cc700} DEBUG: <HttpTransact.cc:7644 (what_is_document_freshness)> (http_match) [26480] [..._document_freshness] document is fresh; returning FRESHNESS_FRESH [Oct 19 14:22:19.945] Server {0x2ad0325cc700} DEBUG: <HttpTransact.cc:2441 (HandleCacheOpenReadHitFreshness)> (http_seq) [26480] [HttpTransact::HandleCacheOpenReadHitFreshness] Fresh copy [Oct 19 14:22:19.945] Server {0x2ad0325cc700} DEBUG: <HttpTransact.cc:2522 (need_to_revalidate)> (http_seq) [26480] [HttpTransact::HandleCacheOpenReadHit] Authentication not needed [Oct 19 14:22:19.945] Server {0x2ad0325cc700} DEBUG: <HttpSM.cc:7204 (call_transact_and_set_next_state)> (http) [26480] State Transition: SM_ACTION_API_READ_CACHE_HDR -> SM_ACTION_API_CACHE_LOOKUP_COMPLETE [Oct 19 14:22:19.945] Server {0x2ad0325cc700} DEBUG: <HttpSM.cc:1417 (state_api_callout)> (http) [26480] calling plugin on hook TS_HTTP_CACHE_LOOKUP_COMPLETE_HOOK at hook 0x2ad034028fe0 [Oct 19 14:22:19.945] Server {0x2ad0325cc700} DEBUG: <HttpSM.cc:1296 (state_api_callback)> (http) [26480] [&HttpSM::state_api_callback, HTTP_API_CONTINUE] [Oct 19 14:22:19.945] Server {0x2ad0325cc700} DEBUG: <HttpSM.cc:1336 (state_api_callout)> (http) [26480] [&HttpSM::state_api_callout, HTTP_API_CONTINUE] [Oct 19 14:22:19.945] Server {0x2ad0325cc700} DEBUG: <HttpTransact.cc:2620 (HandleCacheOpenReadHit)> (http_seq) [26480] [HttpTransact::HandleCacheOpenReadHit] Authentication not needed [Oct 19 14:22:19.945] Server {0x2ad0325cc700} DEBUG: <HttpTransact.cc:2690 (HandleCacheOpenReadHit)> (http_trans) [26480] CacheOpenRead --- needs_auth = 0 [Oct 19 14:22:19.945] Server {0x2ad0325cc700} DEBUG: <HttpTransact.cc:2691 (HandleCacheOpenReadHit)> (http_trans) [26480] CacheOpenRead --- needs_revalidate = 0 [Oct 19 14:22:19.945] Server {0x2ad0325cc700} DEBUG: <HttpTransact.cc:2692 (HandleCacheOpenReadHit)> (http_trans) [26480] CacheOpenRead --- response_returnable = 1 [Oct 19 14:22:19.945] Server {0x2ad0325cc700} DEBUG: <HttpTransact.cc:2693 (HandleCacheOpenReadHit)> (http_trans) [26480] CacheOpenRead --- needs_cache_auth = 0 [Oct 19 14:22:19.945] Server {0x2ad0325cc700} DEBUG: <HttpTransact.cc:2694 (HandleCacheOpenReadHit)> (http_trans) [26480] CacheOpenRead --- send_revalidate = 0 [Oct 19 14:22:19.945] Server {0x2ad0325cc700} DEBUG: <HttpTransact.cc:2826 (HandleCacheOpenReadHit)> (http_trans) [26480] CacheOpenRead --- HIT-FRESH [Oct 19 14:22:19.945] Server {0x2ad0325cc700} DEBUG: <HttpTransact.cc:2828 (HandleCacheOpenReadHit)> (http_seq) [26480] [HttpTransact::HandleCacheOpenReadHit] Serve from cache [Oct 19 14:22:19.945] Server {0x2ad0325cc700} DEBUG: <HttpTransact.cc:2925 (build_response_from_cache)> (http_trans) [26480] [build_response_from_cache] Match! Serving full document. [Oct 19 14:22:19.945] Server {0x2ad0325cc700} DEBUG: <HttpTransact.cc:6873 (handle_content_length_header)> (http_trans) [26480] [handle_content_length_header] RESPONSE cont len in hdr is 2097152 -- State Machine Id: 26480 -- State Machine Id: 26480 [Oct 19 14:22:19.945] Server {0x2ad0325cc700} DEBUG: <HttpSM.cc:7204 (call_transact_and_set_next_state)> (http) [26480] State Transition: SM_ACTION_API_CACHE_LOOKUP_COMPLETE -> SM_ACTION_SERVE_FROM_CACHE [Oct 19 14:22:19.945] Server {0x2ad0325cc700} DEBUG: <HttpSM.cc:5775 (perform_cache_write_action)> (http) [26480] perform_cache_write_action CACHE_DO_SERVE [Oct 19 14:22:19.945] Server {0x2ad0325cc700} DEBUG: <HttpTunnel.cc:683 (add_producer)> (http_tunnel) [26480] adding producer 'cache read' [Oct 19 14:22:19.945] Server {0x2ad0325cc700} DEBUG: <HttpTunnel.cc:738 (add_consumer)> (http_tunnel) [26480] adding consumer 'user agent' [Oct 19 14:22:19.945] Server {0x2ad0325cc700} DEBUG: <HttpTunnel.cc:1162 (producer_handler)> (http_tunnel) [26480] producer_handler [cache read VC_EVENT_READ_COMPLETE] [Oct 19 14:22:19.945] Server {0x2ad0325cc700} DEBUG: <HttpSM.cc:3352 (tunnel_handler_cache_read)> (http) [26480] [&HttpSM::tunnel_handler_cache_read, VC_EVENT_READ_COMPLETE] [Oct 19 14:22:19.946] Server {0x2ad0325cc700} DEBUG: <HttpTunnel.cc:1359 (consumer_handler)> (http_tunnel) [26480] consumer_handler [user agent VC_EVENT_WRITE_READY] [Oct 19 14:22:19.946] Server {0x2ad0325cc700} DEBUG: <HttpTunnel.cc:1359 (consumer_handler)> (http_tunnel) [26480] consumer_handler [user agent VC_EVENT_ERROR] [Oct 19 14:22:19.946] Server {0x2ad0325cc700} DEBUG: <HttpSM.cc:3183 (tunnel_handler_ua)> (http) [26480] [&HttpSM::tunnel_handler_ua, VC_EVENT_ERROR] [Oct 19 14:22:19.946] Server {0x2ad0325cc700} DEBUG: <HttpSM.cc:2607 (main_handler)> (http) [26480] [HttpSM::main_handler, HTTP_TUNNEL_EVENT_DONE] [Oct 19 14:22:19.946] Server {0x2ad0325cc700} DEBUG: <HttpSM.cc:2879 (tunnel_handler)> (http) [26480] [&HttpSM::tunnel_handler, HTTP_TUNNEL_EVENT_DONE] [Oct 19 14:22:19.946] Server {0x2ad0325cc700} DEBUG: <HttpSM.cc:6947 (kill_this)> (http) [26480] deallocating sm **source code** void write_to_net_io(NetHandler *nh, UnixNetVConnection *vc, EThread *thread) { NetState *s = &vc->write; ProxyMutex *mutex = thread->mutex.get(); MUTEX_TRY_LOCK_FOR(lock, s->vio.mutex, thread, s->vio._cont); if (!lock.is_locked() || lock.get_mutex() != s->vio.mutex.get()) { write_reschedule(nh, vc); return; } // This function will always return true unless // vc is an SSLNetVConnection. if (!vc->getSSLHandShakeComplete()) { int err, ret; if (vc->get_context() == NET_VCONNECTION_OUT) { ret = vc->sslStartHandShake(SSL_EVENT_CLIENT, err); } else { ret = vc->sslStartHandShake(SSL_EVENT_SERVER, err); } if (ret == EVENT_ERROR) { vc->write.triggered = 0; write_signal_error(nh, vc, err); } else if (ret == SSL_HANDSHAKE_WANT_READ || ret == SSL_HANDSHAKE_WANT_ACCEPT) { vc->read.triggered = 0; nh->read_ready_list.remove(vc); read_reschedule(nh, vc); } else if (ret == SSL_HANDSHAKE_WANT_CONNECT || ret == SSL_HANDSHAKE_WANT_WRITE) { vc->write.triggered = 0; nh->write_ready_list.remove(vc); write_reschedule(nh, vc); } else if (ret == EVENT_DONE) { vc->write.triggered = 1; if (vc->write.enabled) { nh->write_ready_list.in_or_enqueue(vc); } } else { write_reschedule(nh, vc); } return; } // If it is not enabled,add to WaitList. if (!s->enabled || s->vio.op != VIO::WRITE) { write_disable(nh, vc); return; } // If there is nothing to do, disable int64_t ntodo = s->vio.ntodo(); if (ntodo <= 0) { write_disable(nh, vc); return; } MIOBufferAccessor &buf = s->vio.buffer; ink_assert(buf.writer()); // Calculate the amount to write. int64_t towrite = buf.reader()->read_avail(); if (towrite > ntodo) { towrite = ntodo; } int signalled = 0; // signal write ready to allow user to fill the buffer if (towrite != ntodo && buf.writer()->write_avail()) { if (write_signal_and_update(VC_EVENT_WRITE_READY, vc) != EVENT_CONT) { return; } ntodo = s->vio.ntodo(); if (ntodo <= 0) { write_disable(nh, vc); return; } signalled = 1; // Recalculate amount to write towrite = buf.reader()->read_avail(); if (towrite > ntodo) { towrite = ntodo; } } // if there is nothing to do, disable ink_assert(towrite >= 0); if (towrite <= 0) { write_disable(nh, vc); return; } int needs = 0; int64_t total_written = 0; int64_t r = vc->load_buffer_and_write(towrite, buf, total_written, needs); if (total_written > 0) { NET_SUM_DYN_STAT(net_write_bytes_stat, total_written); s->vio.ndone += total_written; net_activity(vc, thread); } // A write of 0 makes no sense since we tried to write more than 0. ink_assert(r != 0); // Either we wrote something or got an error. // check for errors if (r < 0) { // if the socket was not ready, add to WaitList if (r == -EAGAIN || r == -ENOTCONN || -r == EINPROGRESS) { NET_INCREMENT_DYN_STAT(net_calls_to_write_nodata_stat); if ((needs & EVENTIO_WRITE) == EVENTIO_WRITE) { vc->write.triggered = 0; nh->write_ready_list.remove(vc); write_reschedule(nh, vc); } if ((needs & EVENTIO_READ) == EVENTIO_READ) { vc->read.triggered = 0; nh->read_ready_list.remove(vc); read_reschedule(nh, vc); } return; } vc->write.triggered = 0; write_signal_error(nh, vc, (int)-total_written); return; } else { // Wrote data. Finished without error int wbe_event = vc->write_buffer_empty_event; // save so we can clear if needed. // If the empty write buffer trap is set, clear it. if (!(buf.reader()->is_read_avail_more_than(0))) { vc->write_buffer_empty_event = 0; } // If there are no more bytes to write, signal write complete, ink_assert(ntodo >= 0); if (s->vio.ntodo() <= 0) { write_signal_done(VC_EVENT_WRITE_COMPLETE, nh, vc); return; } int e = 0; if (!signalled) { e = VC_EVENT_WRITE_READY; } else if (wbe_event != vc->write_buffer_empty_event) { // @a signalled means we won't send an event, and the event values differing means we // had a write buffer trap and cleared it, so we need to send it now. e = wbe_event; } if (e) { if (write_signal_and_update(e, vc) != EVENT_CONT) { return; } // change of lock... don't look at shared variables! if (lock.get_mutex() != s->vio.mutex.get()) { write_reschedule(nh, vc); return; } } if ((needs & EVENTIO_READ) == EVENTIO_READ) { read_reschedule(nh, vc); } if (!(buf.reader()->is_read_avail_more_than(0))) { write_disable(nh, vc); return; } if ((needs & EVENTIO_WRITE) == EVENTIO_WRITE) { write_reschedule(nh, vc); } return; } } **in this function load_buffer_and_write return -32 EPIPE error** int64_t r = vc->load_buffer_and_write(towrite, buf, total_written, needs);",What does get method error mean ? Can you share more information?,"**debug log** **in write_to_net_io function** void write_to_net_io(NetHandler *nh, UnixNetVConnection *vc, EThread *thread) { NetState *s = &vc->write; ProxyMutex *mutex = thread->mutex.get(); MUTEX_TRY_LOCK_FOR(lock, s->vio.mutex, thread, s->vio._cont); if (!lock.is_locked() || lock.get_mutex() != s->vio.mutex.get()) { write_reschedule(nh, vc); return; } // This function will always return true unless // vc is an SSLNetVConnection. if (!vc->getSSLHandShakeComplete()) { int err, ret; if (vc->get_context() == NET_VCONNECTION_OUT) { ret = vc->sslStartHandShake(SSL_EVENT_CLIENT, err); } else { ret = vc->sslStartHandShake(SSL_EVENT_SERVER, err); } if (ret == EVENT_ERROR) { vc->write.triggered = 0; write_signal_error(nh, vc, err); } else if (ret == SSL_HANDSHAKE_WANT_READ || ret == SSL_HANDSHAKE_WANT_ACCEPT) { vc->read.triggered = 0; nh->read_ready_list.remove(vc); read_reschedule(nh, vc); } else if (ret == SSL_HANDSHAKE_WANT_CONNECT || ret == SSL_HANDSHAKE_WANT_WRITE) { vc->write.triggered = 0; nh->write_ready_list.remove(vc); write_reschedule(nh, vc); } else if (ret == EVENT_DONE) { vc->write.triggered = 1; if (vc->write.enabled) { nh->write_ready_list.in_or_enqueue(vc); } } else { write_reschedule(nh, vc); } return; } // If it is not enabled,add to WaitList. if (!s->enabled || s->vio.op != VIO::WRITE) { write_disable(nh, vc); return; } // If there is nothing to do, disable int64_t ntodo = s->vio.ntodo(); if (ntodo <= 0) { write_disable(nh, vc); return; } MIOBufferAccessor &buf = s->vio.buffer; ink_assert(buf.writer()); // Calculate the amount to write. int64_t towrite = buf.reader()->read_avail(); if (towrite > ntodo) { towrite = ntodo; } int signalled = 0; // signal write ready to allow user to fill the buffer if (towrite != ntodo && buf.writer()->write_avail()) { if (write_signal_and_update(VC_EVENT_WRITE_READY, vc) != EVENT_CONT) { return; } ntodo = s->vio.ntodo(); if (ntodo <= 0) { write_disable(nh, vc); return; } signalled = 1; // Recalculate amount to write towrite = buf.reader()->read_avail(); if (towrite > ntodo) { towrite = ntodo; } } // if there is nothing to do, disable ink_assert(towrite >= 0); if (towrite <= 0) { write_disable(nh, vc); return; } int needs = 0; int64_t total_written = 0; int64_t r = vc->load_buffer_and_write(towrite, buf, total_written, needs); if (total_written > 0) { NET_SUM_DYN_STAT(net_write_bytes_stat, total_written); s->vio.ndone += total_written; net_activity(vc, thread); } // A write of 0 makes no sense since we tried to write more than 0. ink_assert(r != 0); // Either we wrote something or got an error. // check for errors if (r < 0) { // if the socket was not ready, add to WaitList if (r == -EAGAIN || r == -ENOTCONN || -r == EINPROGRESS) { NET_INCREMENT_DYN_STAT(net_calls_to_write_nodata_stat); if ((needs & EVENTIO_WRITE) == EVENTIO_WRITE) { vc->write.triggered = 0; nh->write_ready_list.remove(vc); write_reschedule(nh, vc); } if ((needs & EVENTIO_READ) == EVENTIO_READ) { vc->read.triggered = 0; nh->read_ready_list.remove(vc); read_reschedule(nh, vc); } return; } vc->write.triggered = 0; write_signal_error(nh, vc, (int)-total_written); return; } else { // Wrote data. Finished without error int wbe_event = vc->write_buffer_empty_event; // save so we can clear if needed. // If the empty write buffer trap is set, clear it. if (!(buf.reader()->is_read_avail_more_than(0))) { vc->write_buffer_empty_event = 0; } // If there are no more bytes to write, signal write complete, ink_assert(ntodo >= 0); if (s->vio.ntodo() <= 0) { write_signal_done(VC_EVENT_WRITE_COMPLETE, nh, vc); return; } int e = 0; if (!signalled) { e = VC_EVENT_WRITE_READY; } else if (wbe_event != vc->write_buffer_empty_event) { // @a signalled means we won't send an event, and the event values differing means we // had a write buffer trap and cleared it, so we need to send it now. e = wbe_event; } if (e) { if (write_signal_and_update(e, vc) != EVENT_CONT) { return; } // change of lock... don't look at shared variables! if (lock.get_mutex() != s->vio.mutex.get()) { write_reschedule(nh, vc); return; } } if ((needs & EVENTIO_READ) == EVENTIO_READ) { read_reschedule(nh, vc); } if (!(buf.reader()->is_read_avail_more_than(0))) { write_disable(nh, vc); return; } if ((needs & EVENTIO_WRITE) == EVENTIO_WRITE) { write_reschedule(nh, vc); } return; } } **in this function load_buffer_and_write return -32 EPIPE error** int64_t r = vc->load_buffer_and_write(towrite, buf, total_written, needs); **request****ats do not responds in 500ms** ![image](https://user-images.githubusercontent.com/17612824/47275672-ec16a680-d5e3-11e8-8aa2-15b74f010cb0.png)"
tpope/vim-fugitive,https://github.com/tpope/vim-fugitive/issues/1446,tpope_vim-fugitive_issues_1446,"Wall-of-text issue with refreshing git hooks

Hey :wave:, I'm using https://github.com/typicode/husky for some of my projects and when triggering a hook fugitive outputs *a lot* of text. I have to press <Space> to go down a full screen more than a couple of times. The text is also blank until I scroll up and down a bit. The following is an example of the output generated:  The above text is repeated atleast a 50-100 times (except the first one of course). Is is possible for fugitive to handle these cases where the git hook is refreshing the output? Or is it possible to fugitive to skip the output?",Can you try with let g:fugitive_pty = 0?,Or is it possible to fugitive to skip the output?
slick/slick,https://github.com/slick/slick/issues/2099,slick_slick_issues_2099,"createIfNotExists doesn't create auto increment sequences

**DESCRIPTION:** Using createIfNotExists does not create auto increment sequences, whereas create works just fine in creating those sequences. A symptom of this issue is if you use createIfNotExists with O.AutoInc fields, you will get these errors:  This doesn't happen if you have the correct sequence. **STEPS TO REPRODUCE:** Clone [mdibaiee/slick-bug-sequences](https://github.com/mdibaiee/slick-bug-sequences) and run the code as is initially:  **EXPECTED RESULT:** Sequence should be created and no error should be thrown. **ACTUAL RESULT:** You will get an error:  Also try connecting to the database (psql slick_bug) and check for sequences and tables:  You will see tables existing, but no sequences. Now try changing the src/main/scala/example/Hello.scala file and replacing createIfNotExists with create. Drop the database and recreate and run again:  This time there will be no errors and you can check that the sequence is created successfully in postgres shell (psql slick_bug):  **SETUP:** Libraries:  Postgres 12.1",Can you please check? @gnufied @jkutner @marcospereira,"UAL RESULT:** You will get an error:  Also try connecting to the database (psql slick_bug) and check for sequences and tables:  You will see tables existing, but no sequences. Now t"
FluentValidation/FluentValidation,https://github.com/FluentValidation/FluentValidation/issues/1226,FluentValidation_FluentValidation_issues_1226,"Validation same object with different name.

I have following case.   My Validators;  However when I execute this code, I get validation message for all my elements. But the validation message is same for InvoiceAddress and BillingAddress. How can I fix this issue?",Can you be more specific? (or provide a test case that illustrates the problem). Thanks.,"public class BillingAddressValidator : AbstractValidator<Address > { public BillingAddressValidator() { RuleFor(x => x.AddressName) .Cascade(CascadeMode.StopOnFirstFailure) .NotEmpty() .WithMessage(""Billing address name is required."") .Length(5, 100) .WithMessage(""Address must be between 5 to 100 characters.""); RuleFor(x => x.Address1) .Cascade(CascadeMode.StopOnFirstFailure) .NotEmpty() .WithMessage(""Billing address1 is required."") .Length(5, 100) .WithMessage(""Address1 must be between 5 to 100 characters.""); RuleFor(x => x.Address2) .Length(5, 100) .WithMessage(""Address2 must be between 5 to 100 characters.""); RuleFor(x => x.City) .Cascade(CascadeMode.StopOnFirstFailure) .NotEmpty() .WithMessage(""Billing city is required."") .Length(3, 100) .WithMessage(""City must be between 3 to 100 characters.""); RuleFor(x => x.PostCode) .Cascade(CascadeMode.StopOnFirstFailure) .NotEmpty() .WithMessage(""Billing post code is required."") .Length(4, 10) .WithMessage(""Post code must be between 4 to 10 characters.""); } } public class InvoiceAddressValidator : AbstractValidator<Address> { public InvoiceAddressValidator() { RuleFor(x => x.AddressName) .Cascade(CascadeMode.StopOnFirstFailure) .NotEmpty() .WithMessage(""Invoice address name is required."") .Length(5, 100) .WithMessage(""Address must be between 5 to 100 characters.""); RuleFor(x => x.Address1) .Cascade(CascadeMode.StopOnFirstFailure) .NotEmpty() .WithMessage(""Invoice address1 is required."") .Length(5, 100) .WithMessage(""Address1 must be between 5 to 100 characters.""); RuleFor(x => x.Address2) .Length(5, 100) .WithMessage(""Address2 must be between 5 to 100 characters.""); RuleFor(x => x.City) .Cascade(CascadeMode.StopOnFirstFailure) .NotEmpty() .WithMessage(""Invoice city is required."") .Length(3, 100) .WithMessage(""City must be between 3 to 100 characters.""); RuleFor(x => x.PostCode) .Cascade(CascadeMode.StopOnFirstFailure) .NotEmpty() .WithMessage(""Invoice post code is required."") .Length(4, 10) .WithMessage(""Post code must be between 4 to 10 characters.""); } }"
apache/couchdb,https://github.com/apache/couchdb/issues/2045,apache_couchdb_issues_2045,"Stale couch view after purging document with conflicted revision tree

A couple of days back, I raised a question regarding a sporadic issue we observed related to Couch views returning stale data. (Reference: https://github.com/apache/couchdb/issues/2040) I have found out the reason for this and would like some insight from the community to understand this problem better. ## Description We have a production setup with multiple local Couch databases (replicating amongst each other) and syncing with a master cloud Couch database. Our Couch databases also go through a scheduled daily purge where specific documents (after a grace period) gets purged from the database. The issue we have observed is as follows: When a document with a conflicting revision tree is purged, it gets reverted back to the leaf revision of the conflicting revision tree. (I understand this happens because we do not specify the conflicting revision in the _purge POST request) This reverted document is caught in the next purge window and is purged again which gets rid of the document entirely from the database as expected. However, when related views are queried for data, this is not reflected; The document Id is still visible in the queried data. ## Steps to Reproduce Pre-requisites: * A document with a conflicting revision tree * A view that retrieves the above document Id Step 1: Purge the document, specifying the latest revision. This will revert the document to the leaf of the conflicting tree Step 2: Purge the document again, this time specifying the leaf of the conflicting tree. Step 3: Perform a purge of another document to ensure the diff between the purge sequence of the database is greater than the purge sequence of the view by more than 1. (Based on documentation, this will trigger a complete rebuilding of the indexes) Step 4: Access the view. The purged document Id (now completely removed from the database) will still be returned. ## Expected Behaviour The view should not return Ids of documents that are purged completely. ## Your Environment * CouchDB Version used: CouchDB 1.6.1 * Browser name and version: Chrome * Operating System and version: Ubuntu ## Additional context The fix that I plan on implementing is to specify all leaf revisions (including that of conflicting branches) when initiating the purge. However, I would also like to understand what exactly is happening here for the document Id to not get removed from the index when the conflicting revision tree is purged from the database. **UPDATE** This issue does not re-create in CouchDB 2.3.1",Did you try the scenario with current 2.x version?,**UPDATE** This issue does not re-create in CouchDB 2.3.1
sparkle-project/Sparkle,https://github.com/sparkle-project/Sparkle/issues/1447,sparkle-project_Sparkle_issues_1447,"Make copying private key easier

Right now AFAICT there is no easy way to copy the ed25519 private key from one machine to another! (I'm assuming the recommended workflow where the key stored as a ""generic password"" in Keychain Access. As distinct from an ""Internet password"" or a ""key"", for instance.) Maybe I'm missing something, as this seems like a major problem for many uses cases such as: 1. More than one build machine 2. Only one build machine, but desire to develop the Sparkle process off the build machine before deploying it to the build machine 3. Only one build machine _at a time_ but got a new one I.e. AFAICT neither the generic features provided by Apple's Keychain Access, nor specific features provided by Sparkle make such key copies easy. (Perhaps the generic features to export & import an entire keychain could be used? But that would seem to import a lot of undesired data, unless somehow the import could be filtered down to a single ""password"".) Not only are key copies not easy, in fact I haven't really yet found a feasible way in which such a copy is even _possible_! For example, attempting to manually create a copy of the key (""password"") on another machine using Keychain Access seems to fail, perhaps because I can't set kSecAttrProtocol to kSecAttrProtocolSSH using the Keychain Access GUI. Perhaps generate_keys could be extended to optionally accept a private key on the command line, allowing it to generate a copy instead of a new, unique key? Perhaps in that case it should require the public key, too, so it can generate the proper/expected comment in the ""password"" entry. Or perhaps somehow /usr/bin/security can be used to generate a copy including the proper kSecAttrProtocol attribute?",Could you make a PR for it?,"generic . As distinct from an ""Internet password"" or a ""key"", for instance"
biopython/biopython,https://github.com/biopython/biopython/issues/2559,biopython_biopython_issues_2559,"MMCIFIO rewrites chimera-incompatible 6dwu.cif - perhaps related to 1784 quotes issue

### Setup I am reporting a problem with Biopython version, Python version, and operating system as follows: Python 3.7.4 (default, Aug 13 2019, 20:35:49) [GCC 7.3.0] :: Anaconda, Inc. on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. 3.7.4 (default, Aug 13 2019, 20:35:49) [GCC 7.3.0] CPython Linux-3.10.0-1062.9.1.el7.x86_64-x86_64-with-centos-7.7.1908-Core 1.76 ALSO note: chimera production version 1.14 (build 42094) 2019-11-13 06:05:13 UTC (*Please copy and run the above in your Python, and copy-and-paste the output*) ### Expected behaviour I should be able to read in '6dwu.cif' to a mmCIF dictionary, and write it back out with MMCIFIO.*, and be able to open the resulting ""unchanged"" file in chimera ### Actual behaviour Chimera fails to open the saved file with the message: Error Reading 6dwu_2.cif line 3 '.' appears in tag name entry.id Line 3 is unchanged from the original. However, the saved .cif file has 750 differences from the original, many of which seem due to loss of single quote characters. (*Please fill this in, and provide any exception message in full*) ### Steps to reproduce Create a directory and Download [6dwu.cif](https://files.rcsb.org/view/6DWU.cif). Run this program I have pasted in below. Attempt to open 6dwu_2.cif in chimera: (base) [mothcw@localhost tmp]$ cat read_and_write.py  ======== The reason this is important is that I am writing an updated Biopytho-based biounit creator, (based on legacy python2 code at http://mmcif.wwpdb.org/docs/sw-examples/python/html/assemblies.html If I had this working on all the cif files in the pdb, this could be valuable for a relatively large community of users. (Edited by @MarkusPiotrowski to include link to cif file and to correct code formatting)",Can you link to or attach the test files here?,[](https://files.rcsb.org/view/6DWU.cif) (Edited by @MarkusPiotrowski to include link to cif file and to correct code formatting)
biopython/biopython,https://github.com/biopython/biopython/issues/2843,biopython_biopython_issues_2843,"extractDataBlastXMLBiopython --> ('*', 'M') KeyError: ('*', 'M')

### Setup I am reporting a problem with Biopython version, Python version, and operating system as follows:  '2.7.2 (default, Apr 21 2015, 13:01:47) \n[GCC 4.4.7 20120313 (Red Hat 4.4.7-4)]' 'CPython';'Linux-2.6.32-696.30.1.el6.x86_64-x86_64-with-redhat-6.9-Santiago' '1.60' ### Expected behaviour The extractDataBlastXMLBiopython has to parse the blast xml generated by psiblast and return the extracted data ### Actual behaviour (Peter: Edited to use triple back-ticks, fixes display of asterisks)  ### Steps to reproduce  Let me know, if you need any additional data..!! Regards, Vijay N",Could you make a self contained example as requested earlier?,"(Peter: Edited to use triple back-ticks, fixes display of asterisks) "
chocolate-doom/chocolate-doom,https://github.com/chocolate-doom/chocolate-doom/issues/1269,chocolate-doom_chocolate-doom_issues_1269,"[FIXED] Wrong music being played on Final Doom TNT Evilution MAP25

<!-- Thank you for reporting a bug in Chocolate Doom. Please complete the following template so that we can better diagnose the source of your problem. To save yourself some time, you may want to check the FAQ and the NOT-BUGS list for solutions to some common problems: https://www.chocolate-doom.org/wiki/index.php/FAQ https://www.chocolate-doom.org/not-bugs --> ### Background Version of Chocolate Doom:3.0.0 Operating System and version: Windows 10 1909 Game: (Doom/Heretic/Hexen/Strife/other) Final Doom: TNT Evilution Any loaded WADs and mods (please include full command line): None ### Bug description Observed behavior: Playing a different music in MAP25, Baron's Den. It plays the MAP13's Doom II music Expected behavior: Playing the correct one (d_ampie, damn is d_adrian sorry again)",Which map is it? Can you clarify?,", damn is d_adrian sorry again"
ReactionMechanismGenerator/RMG-Py,https://github.com/ReactionMechanismGenerator/RMG-Py/issues/1711,ReactionMechanismGenerator_RMG-Py_issues_1711,"Comment in chem_annotated.inp caused ChemkinError

### Bug Description The comment made in the chem_annotated.inp file caused ChemkinError. It seems that the _removeLineBreaks function in chemkin.pyx could not recognize the comment starting with ""Total Standard Deviation in ln(k)"". Here is the comment in the chem_annotated.inp file: (edit: I just found that I've pasted a wrong part; this is the part that has a ChemkinError.) ! Reaction index: Chemkin #5; RMG #1064 ! Template reaction: R_Recombination ! Flux pairs: C8H9(256), S(542); C4H5(1), S(542); ! BM rule fitted to 2 training reactions at node Root_N-1R->H_N-1CClNOSSi->N_N-1COS->O_1CS->C_1C-inRing_Ext-1C-R_Sp-3R!H-1C_N-2R-inRing_Ext-2R-R ! Total Standard Deviation in ln(k): 11.5401827615 ! Exact match found for rate rule [Root_N-1R->H_N-1CClNOSSi->N_N-1COS->O_1CS->C_1C-inRing_Ext-1C-R_Sp-3R!H-1C_N-2R-inRing_Ext-2R-R] ! Euclidian distance = 0 ! Multiplied by reaction path degeneracy 2.0 ! family: R_Recombination C8H9(256)+C4H5(1)=S(542) 3.535860e+16 -1.003 0.000",Can you post the trace error message you got?,"(edit: I just found that I've pasted a wrong one; this is the part that has a ChemkinError.) C8H9(256), S(542);15401827 ! Multiplied by reaction path degeneracy 2.0 ! family: R_Recombination C8H9(256)+C4H5(1)=S(542) 3.535"
VowpalWabbit/vowpal_wabbit,https://github.com/VowpalWabbit/vowpal_wabbit/issues/1699,VowpalWabbit_vowpal_wabbit_issues_1699,"holdout_after does not work with value 40,000,000? works with 4,000,000

[EDIT - markdown hid the command line params] I am trying to use vowpalwabbit on the criteo dataset (~45 million rows - 7 days of impressions data) what I wanted to happen is that I use first 40 million rows as training and last 5 million as holdout. however, it does not seem to work - it claims no hold out set defined. If I use a data set of 5 million, it seems to work fine?? see log below: a) no hold out loss reported ( unknown) b) average loss = undefined (no holdout) c) number of examples per pass = 21576486, (i would have expected holdout_after - 1; for 4 million hold out_after I get 3,999,999 full data set is 45,840,617 lines. I am using vowpalwabbit 8.6.1. as packaged in ubuntu 18.10. vw --final_regressor /opt/ml/model/model.bin -c --data train.vw.gz --holdout_after 40000000 --passes 30 --l2 1e-08 --link logistic --loss_function logistic --compressed ['/opt/ml/input/data/training/train.vw.gz']",Can you provide full directions for making this reproducible?,"[EDIT - markdown hid the command line params] see log below: a) no hold out loss reported ( unknown) b) average loss = undefined (no holdout) c) number of examples per pass = 21576486, (i would have expected holdout_after - 1; for 4 million hold out_after I get 3,999,999 full data set is 45,840,617 lines. dataset is at (https://s3.eu-central-1.amazonaws.com/test-criteo-vw/train.vw.gz) using l2 regularization = 1e-08 final_regressor = /opt/ml/model/model.bin Num weight bits = 18 learning rate = 0.5 initial_t = 0 power_t = 0.5 decay_learning_rate = 1 creating cache_file = train.vw.gz.cache Reading datafile = train.vw.gz num sources = 1 average since example example current current current loss last counter weight label predict features 0.693147 0.693147 1 1.0 -1.0000 0.5000 37 0.526412 0.359676 2 2.0 -1.0000 0.3021 38 0.392392 0.258372 4 4.0 -1.0000 0.2581 25 0.525705 0.659019 8 8.0 1.0000 0.1841 29 0.512478 0.499252 16 16.0 -1.0000 0.1843 30 0.634170 0.755862 32 32.0 -1.0000 0.2725 27 0.589017 0.543863 64 64.0 1.0000 0.0978 33 0.575090 0.561164 128 128.0 -1.0000 0.3968 33 0.520033 0.464976 256 256.0 -1.0000 0.0890 29 0.521504 0.522975 512 512.0 -1.0000 0.1619 36 0.481930 0.442356 1024 1024.0 -1.0000 0.1679 33 0.475071 0.468213 2048 2048.0 -1.0000 0.3518 33 0.469413 0.463754 4096 4096.0 1.0000 0.4830 32 0.474600 0.479787 8192 8192.0 -1.0000 0.0240 33 0.472812 0.471024 16384 16384.0 -1.0000 0.6055 35 0.466881 0.460951 32768 32768.0 -1.0000 0.2717 34 0.458549 0.450216 65536 65536.0 -1.0000 0.0418 36 0.455606 0.452663 131072 131072.0 -1.0000 0.1896 37 0.471236 0.486866 262144 262144.0 -1.0000 0.4708 34 0.477819 0.484401 524288 524288.0 -1.0000 0.7471 36 0.473423 0.469028 1048576 1048576.0 -1.0000 0.5565 32 0.467879 0.462334 2097152 2097152.0 -1.0000 0.1400 32 0.465152 0.462425 4194304 4194304.0 -1.0000 0.1021 31 0.463388 0.461623 8388608 8388608.0 -1.0000 0.0832 33 0.464135 0.464883 16777216 16777216.0 -1.0000 0.1414 35 unknown unknown 33554432 33554432.0 -1.0000 0.1821 36 h unknown unknown 67108864 67108864.0 -1.0000 0.2682 38 h finished run number of examples per pass = 21576486 passes used = 4 weighted example sum = 86305944.000000 weighted label sum = -42519848.000000 average loss = undefined (no holdout) best constant = -1.079145 best constant's loss = 0.566329 total feature number = 2886790752 Training complete in 693.3070759773254 seconds."
VowpalWabbit/vowpal_wabbit,https://github.com/VowpalWabbit/vowpal_wabbit/issues/2201,VowpalWabbit_vowpal_wabbit_issues_2201,"Heisenbug w/ docker container running Vowpal Wabbit daemon.

#### Describe the bug Heisenbug when making requests to docker container running Vowpal Wabbit daemon. #### To Reproduce Steps to reproduce the behavior: 1. Login to ECR $(aws ecr get-login --no-include-email --region <region> --registry-ids 462105765813) 2. Pull Vowpal Wabbit docker image docker pull 462105765813.dkr.ecr.<region>.amazonaws.com/sagemaker-rl-vw-container:vw-8.7.0-cpu 3. Start Vowpal Wabbit daemon docker run --rm -it -p 26542:26542 <image_id> vw --daemon --cb_explore 4 --foreground 4. Send requests to daemon echo ""'tag| a:1 b:1"" | nc 0.0.0.0 26542 5. Repeat (step 4.) a dozen times #### Expected behavior I expect step 4 to respond with something like0.962500 0.012500 0.012500 0.012500 tag every time. #### Observed Behavior However, after the 6th (or so) repetition of step 4, there is a random chance that there is no response, and the vowpal daemon throws a write error: Bad File descriptor. #### Environment ECR container of Vowpal Wabbit version 8.7.0 cpu. Hosted on default AWS sagemaker instance, ml.t2.medium NAME=""Amazon Linux AMI"" VERSION=""2018.03"" #### Additional context I observed the AWS example code flushing std-out and std-in just after each request, and their code is running fine w/ the same container. This makes me think the issue could be related to a buffer overflow. I'm not sure how to force the container to flush to test this hypothesis...",Can you try adding -q 1 to the netcat command?,"NAME=""Amazon Linux AMI"" VERSION=""2018.03"""
company-mode/company-mode,https://github.com/company-mode/company-mode/issues/856,company-mode_company-mode_issues_856,"Grouping not working as expected when using :separate

**Issue** - When using :separate keyword in backend grouping, the suggestions are not grouped properly **Expected** - The suggestions from different backends should be separately grouped **Actual** - The suggestion are not grouped by backends **Backends group** ((:separate company-lsp :separate company-tabnine :separate company-yasnippet) company-lsp) **This is what I see:** image ![image](https://user-images.githubusercontent.com/12972892/50512930-b2b45880-0ab6-11e9-8f7f-c724c2b232f8.png) Ideally the retry completion should come after return completion. It was working fine before recent releases. **Company Diag:**  **Transformers**  Oddly when I disable company-sort-by-statistics the grouping works fine",What does M-x company-diag say?,**This is what I see:** image ![image](https://user-images.githubusercontent.com/12972892/50512930-b2b45880-0ab6-11e9-8f7f-c724c2b232f8.png) Ideally the retry completion should come after return completion. It was working fine before recent releases. **Company Diag:** 
junit-team/junit4,https://github.com/junit-team/junit4/issues/1573,junit-team_junit4_issues_1573,"Exception running JUnit4 tests

My project work yesterday and i did build and release, unit tests were running and then i open today and the app is not running unit tests showing error: I am using jre 1.8.0_121 and <junit-version>4.12</junit-version>  Also Maven deps with test scope:  Most of all are generated by swagger generator:  wiremock has been added manually.",Which version of JUnit are you using?,"My project work yesterday and i did build and release, unit tests were running and then i open today and the app is not running unit tests showing error: I am using Also Maven deps with test scope:  Most of all are generated by swagger generator:  wiremock has been added manually."
r-lib/testthat,https://github.com/r-lib/testthat/issues/776,r-lib_testthat_issues_776,"Adding test_that_file() function

We used the following function in styler and elsewhere to create the absolute path to testthat files when running R CMD check and executing test interactively. Because if you run R CMD check, the working directory is changed to ./tests. I know it's a pretty small function and it needs the rprojroot dependency but I found it rather helpful. Do you think it fits into testthat?  cc: @krlmlr",Would we Suggests: or Imports: the _rprojroot_ package?,"e absolutexecuting test . Because if you run R CMD check, the working directory is changed to ./tests."
jackmoore/colorbox,https://github.com/jackmoore/colorbox/issues/865,jackmoore_colorbox_issues_865,"Grouping in SVG links

Hi, I have several links in an SVG that should display HTML texts in the lightbox. The display works, but I am not able to build in a grouping for one before <--> back. The code looks something like this:  any text blocks  and jQuery",Does this work?," <div class=""colorbox_container"" style=""display:none;""> <div id=""text_1"" style=""margin: 20px;""> <div>Text 1....</div> </div> </div> <div class=""colorbox_container"" style=""display:none;""> <div id=""text_2"" style=""margin: 20px;""> <div>Text 2....</div> </div> </div>"
HOL-Theorem-Prover/HOL,https://github.com/HOL-Theorem-Prover/HOL/issues/546,HOL-Theorem-Prover_HOL_issues_546,"“match_term” returns spurious matches when type instantiation aliases variables

Tested in 4f2fc0f852c2307a with experimental kernel: val t = “(f :α → β → γ) (x :α) (x :β)”: term val it = ([{redex = “(f :α → α → β)”, residue = “(g :α → α → β)”}, {redex = “(x :α)”, residue = “(a :α)”}, {redex = “(x :α)”, residue = “(b :α)”}], [{redex = “:γ”, residue = “:β”}, {redex = “:β”, residue = “:α”}]): (term, term) Term.subst * (hol_type, hol_type) Term.subst Edit: added type annotations to output.","Could you say more about what is spurious/incorrect about this? It looks like a correct result to me, but I haven't looked hard.",(( :α → α → β) Edit: added type annotations to output.
clientIO/joint,https://github.com/clientIO/joint/issues/1263,clientIO_joint_issues_1263,"Double click event not being fired

**Note:** Jump down to the **Update** below, as the case without multiple toFront calls is expected behaviour for double clicks. _____ I found that when I click multiple times on an element, the cell:pointerclick event is fired every single time, but the cell:pointerdblclick event is not always fired, and if I click fast enough, it doesn't seem to fire at all. This is causing issues for me. Here is a GIF showing what's happening: ![double-click-not-always-fired](https://user-images.githubusercontent.com/7561061/76327170-08da7980-62c0-11ea-8282-6a53f9b7730c.gif) I created the above in CodeSandbox: https://codesandbox.io/s/cocky-surf-lbs76. The code is below:  I played around with it, and it appears this behaviour gets worse (the double click event is fired less often) if I add more shapes and links between them. This is testing with JointJS 3.1.1, in Chrome 80, FF 74, and Safari 13. **Update:** I created another CodeSandbox: https://codesandbox.io/s/friendly-mendeleev-6d8tp. If I call toFront multiple times, e.g.:  Then the double click event isn't even fired once for any of the elements (in Safari 13.0.4).",Does it happen in any other browser for you?,"**Note:** Jump down to the **Update** below, as the case without multiple toFront calls is expected behaviour for double clicks. _____"
octokit/octokit.rb,https://github.com/octokit/octokit.rb/issues/1128,octokit_octokit.rb_issues_1128,"Octokit.contents fails with URI-encoded filepath

Hi, I find a bit strange behavior of the Octokit.contents method. When passing a URI-encoded file path, the method raises an error.  <details> <summary>Error details</summary>  </details> When encoding the file path before calling the method, the method works. For example:  I think useful to encode a given file path inside the contents method, but I'm worrying the behavior change may be breaking. What do you think about this idea? ### Reproduction environment: - Ruby 2.6.3 - Octokit 4.14.0 - macOS Movaje 10.14.5 ### Reproduction code: https://github.com/ybiquitous/octokit-ruby-contents-reproduction",Would you mind reaching out to GitHub Support to confirm? Thanks!,"http%253A%252F%252Fexample.com/x%20y.rb Octokit.contents(""ybiquitous/octokit-ruby-contents-reproduction"", path: test_file) #=>"
redis/redis-rb,https://github.com/redis/redis-rb/issues/891,redis_redis-rb_issues_891,"scan_each getting NPE

 Looks like a dupe of https://github.com/redis/redis-rb/issues/624 but that closed 3.5y ago and I assume is in 4.1.3.","Could you provide some context? A repro script would be ideal, but at the very least a non truncated backtrace, and the command and parameters?","test/common_test_methods.rb:539:in block in purge_tasks' lib/v2/updates/redis_updates_processor.rb:15:in block in with_redis' /usr/local/bundle/gems/redis-4.1.3/lib/redis.rb:2415:in block in pipelined' /usr/local/bundle/gems/redis-4.1.3/lib/redis.rb:52:in block in synchronize' /usr/local/lib/ruby/2.3.0/monitor.rb:214:in mon_synchronize' /usr/local/bundle/gems/redis-4.1.3/lib/redis.rb:52:in synchronize' /usr/local/bundle/gems/redis-4.1.3/lib/redis.rb:2411:in pipelined' lib/v2/updates/redis_updates_processor.rb:15:in with_redis' test/common_test_methods.rb:525:in purge_tasks' test/unit/client_remarketing_campaign_factory_test.rb:93:in block (2 levels) in <class:ClientRemarketingCampaignFactoryTest>' test/common_test_methods.rb:166:in as_dealer_sem' test/unit/client_remarketing_campaign_factory_test.rb:44:in block in <class:ClientRemarketingCampaignFactoryTest>' # test/common_test_methods.rb:539 redis.scan_each(match: ""#{account.id}:*"") do |key| redis.del(key) end"
haraldk/TwelveMonkeys,https://github.com/haraldk/TwelveMonkeys/issues/451,haraldk_TwelveMonkeys_issues_451,"Use Document in SVGImageReader's setInput

(I thought it was better to create a new topic for this, see #442 for background) The change you proposed to let setInput receive a Document object does in fact work (with ~~a few limitations that I will enumerate later~~ one limitation - more on that later). I do think it would be a good thing to implement in TwelveMonkeys, since it is quite easy and makes the reader more friendly. So the changes that need to be done in TwelveMonkeys are: 1) In SVGImageReader.java exactly as you said:  2) In SVGImageReaderSpi.java, change the canDecode to something like (a more specific type of Document may be needed, later on limitations):  3) And finally, in SVGProviderInfo.java, this must be added (a more specific type of Document may be needed, later on limitations):  And that's it! Works (almost) perfectly. You may want to add some test cases to check the new feature. I'll share the code on how to create a Document object from a file next, but first the ~~limitations~~ limitation: ~~1) The CSS problem still exists when using a Document as input. CSS will not be used if the xml-stylesheet tag is before the SVG tag.~~ 2) Using the standard org.w3c.dom.Document does not work. It gives ClassCastExceptions, here is an example:  There is another person with the same problem: https://stackoverflow.com/questions/44029119/using-vanilla-w3c-document-and-apache-batik-svg-rasterizer So, instead of creating a Document like this:  one mut use the [Batik way](https://xmlgraphics.apache.org/batik/using/dom-api.html):  For this reason, it may be a good idea to force Document input to be of type SVGOMDocument (or maybe the more generic SVGDocument). While it is very saddening that one cannot use a plain org.w3c.dom.Document (thus making the code plugin-aware / Batik-libraries-aware) ~~and that it does not fix the CSS problem that I was already experiencing when converting the Document to a byte array stream~~, I think it is a nice feature to add in TweleveMonkeys as it wouldn't hurt anyone.","Can you add a test case too, and perhaps create a pull request for this? I think this would be a useful contribution. :-) Best regards, -- Harald K",", see #442 for background"
net-ssh/net-ssh,https://github.com/net-ssh/net-ssh/issues/747,net-ssh_net-ssh_issues_747,"could not settle on kex algorithm both client and server have : curve25519-sha256@libssh.org

### Expected behavior When deploying/connecting to remote server using latest netssh from git (or 6.0 beta2) ### Actual behavior Exception while executing as user@example.com: could not settle on kex algorithm (SSHKit::Runner::ExecuteError) Server kex preferences: curve25519-sha256@libssh.org,diffie-hellman-group18-sha512,diffie-hellman-group14-sha256,diffie-hellman-group16-sha512 Client kex preferences: curve25519-sha256,curve22519-sha256@libssh.org,ecdh-sha2-nistp521,ecdh-sha2-nistp384,ecdh-sha2-nistp256,diffie-hellman-group-exchange-sha256,diffie-hellman-group14-sha1 ### System configuration - net-ssh version 6.0 - Ruby version : 2.6.5 ed25519 , bcrypt_pbkdf , x25519 are all added to gemfile ### Example App When i run ssh -vv i see that open ssh is using server->client cipher: chacha20-poly1305@openssh.com ssh -v debug1: kex: algorithm: curve25519-sha256@libssh.org debug1: kex: host key algorithm: ssh-ed25519 debug1: kex: server->client cipher: chacha20-poly1305@openssh.com MAC: <implicit> compression: none debug1: kex: client->server cipher: chacha20-poly1305@openssh.com MAC: <implicit> compression: none So this may be related https://github.com/net-ssh/net-ssh/issues/477",Should be fixed by #741 can you try with current master?!,"ed25519 , bcrypt_pbkdf , x25519 are all added to gemfile"
twilio/twilio-csharp,https://github.com/twilio/twilio-csharp/issues/509,twilio_twilio-csharp_issues_509,"twilio security not working on my .net core 3.0/3.1 application properly

I have implemented validation as per https://www.twilio.com/docs/usage/tutorials/how-to-secure-your-csharp-aspnet-core-app-by-validating-incoming-twilio-requests I have my .net core 3.0 application where callback url is like https://mydomain.com:81/Twilio/TwilioAction Request validator seems always rejecting the validation. Same code with my other deployment where my address is like https://myotherdomain.com/Twilio/TwilioAction is working fine. It looks like due to following implementation in twilio RequestValidator  **I have even upgraded to release 5.37.1 but issue still exists**",What sort of callback is this not working for?,**I have even upgraded to release 5.37.1 but issue still exists**
memcached/memcached,https://github.com/memcached/memcached/issues/576,memcached_memcached_issues_576,"dtrace probes emit byte arrays for keys, not null terminated strings

# TL;DR After some discussion, it seems the various dtrace commands implemented in memcached presently return a void *, not actually a char * by conventional standards. This means that it may be arbitrary binary data, not necessarily a null terminated string, and you may read more data for the key than intended. # Why is this a problem? Reading this data with eBPF right now, the keylen will be shorter than the text actually read. This is because the entire buffer is returned, containing extra data from the adjacent struct field members. This is problematic when loading the data into an eBPF map for performance analysis, keys cannot directly correspond to eBPF map keys. While this can be cleaned up, it leads to a less efficient sampling of memcached. # Solution ## bpftrace Use the nkey argument from dtrace probes to read only n bytes, eg, use str($buf, $len) in bpftrace. ## bcc Use the & operator (or other workaround) to prove to the eBPF verifier that the USDT argument read is less than the size of the buffer. If the buffer size is 0xff or a similar sentinal value, it is easy to prove this with a bitwise comparison. # Original issue I've been working on instrumenting memcached using iovisor/bcc and building memcached with --enable-dtrace, in order to build performance analysis tools for memcached using USDT probes on linux. Ordinarily, data for arguments is read like this https://github.com/iovisor/bcc/blob/master/docs/reference_guide.md#6-usdt-probes You can also use https://github.com/iovisor/bcc/blob/master/docs/reference_guide.md#2-bpf_probe_read_str, which is what I'm using. However, it assumes that the key is a null terminated string. After scratching my head a lot, trying to figure out why the keys I were pulling out from my sample script had payload data seemingly arbitrarily tacked onto them, I started to look into why this might be happening in memcached. The probe I am trying to use is for command__set : https://github.com/memcached/memcached/blob/master/memcached_dtrace.d#L205-L214 It is called here: https://github.com/memcached/memcached/blob/e7c7a1089c066d105bf0a6c1a812668644a4a6f0/memcached.c#L1378 And you can see that the from the definition of ITEM_key, it is just calculating the pointer address for the start of the key data: https://github.com/memcached/memcached/blob/e7c7a1089c066d105bf0a6c1a812668644a4a6f0/memcached.h#L116 However, it makes no attempt to actually limit the bytes sent to it->nkey, though it **does** pass this to the dtrace probe as keylen. Looking elsewhere in the memcached source, I see this code, with a comment indicating exactly what I suspected - that the key data is not / may not be stored with a null terminator: https://github.com/memcached/memcached/blob/e7c7a1089c066d105bf0a6c1a812668644a4a6f0/items.c#L621-L622 Luckily I am able to use the keylen argument passed to fix up the data in userspace after i have read it, but this isn't ideal. To reproduce the issue, I am able to use bpftrace from ubuntu eoan with the following bpftrace script (very similar to dtrace scripts):  Pointing this at a memached process with dtrace probes enabled, and running a benchmark tool like memtier_benchmark:  I see whet I print the key that it bleeds a bunch of the data out with it. Would it a feasible workaround for this be to do a similar strncpy of the key to a temporary buffer before emitting the data to the dtrace probe? There are some moderate performance implications to doing so, but I think they are justified.",Why can't the probe use the keylen that was passed to it?,"# TL;DR After some discussion, it seems the various dtrace commands implemented in memcached presently return a void *, not actually a char * by conventional standards. This means that it may be arbitrary binary data, not necessarily a null terminated string, and you may read more data for the key than intended. # Why is this a problem? Reading this data with eBPF right now, the keylen will be shorter than the text actually read. This is because the entire buffer is returned, containing extra data from the adjacent struct field members. This is problematic when loading the data into an eBPF map for performance analysis, keys cannot directly correspond to eBPF map keys. While this can be cleaned up, it leads to a less efficient sampling of memcached. # Original issue"
