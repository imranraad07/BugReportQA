repo,issue_link,issue_id,post,question,answer
rust-lang/rust,https://github.com/rust-lang/rust/issues/69757,rust-lang_rust_issues_69757,"Consider warning when comparing wide pointers with vtables (as their address identity is unstable)

vtable addresses [may differ cross codegen units](https://github.com/rust-lang/rust/issues/46139). To mitigate this, it would be good to have a lint that warns against comparing wide pointers with vtables. ### Original report This is a regression from the 2/27 to 2/28 nightly.  When this program is built with rustc main.rs, it runs without any trouble. When it's built with rustc main.rs -C incremental=, I receive the following output:  From the [regression window](https://github.com/rust-lang/rust/compare/abc3073c9...6d69caba1), I suspect https://github.com/rust-lang/rust/pull/67332.",Could you describe the expected behavior and how that differs from the actual behavior?,"vtable addresses [may differ cross codegen units](https://github.com/rust-lang/rust/issues/46139). To mitigate this, it would be good to have a lint that warns against comparing wide pointers with vtables. ### Original report"
rust-lang/rust,https://github.com/rust-lang/rust/issues/51418,rust-lang_rust_issues_51418,"Tracking issue for the 2018 edition’s prelude

Currently, every module has something like use std::prelude::v1::*; implicitly inserted into it by default. In https://github.com/rust-lang/rust/pull/49305 we added the TryFrom and TryInto traits to the prelude, and reverted that in https://github.com/rust-lang/rust/pull/49518 because that a breaking change for a significant number on crates that had their own TryFrom or TryInto traits. (Ironically, identical to the std ones and duplicated because those were still unstable.) Consensus is that we’d still like to have those traits in the prelude, but to avoid breakage we need to make that opt-in. The upcoming 2018 edition seems to be a good opportunity for that. For modules in crates that opt into the 2018 edition, we could replace v1 in that inserted code with edition2018 and create src/libstd/prelude/edition2018.rs and src/libcore/prelude/edition2018.rs like so:  Are there more items we considered adding to the prelude but didn’t because of breakage or breakage risk? ---- **Update:** implemented in https://github.com/rust-lang/rust/pull/51434.",Should we rename v1 to edition2015 and add an alias for it? Then the compiler can just plug in the edition year into the path.,---- **Update:** implemented in https://github.com/rust-lang/rust/pull/51434.
rust-lang/rust,https://github.com/rust-lang/rust/issues/51758,rust-lang_rust_issues_51758,"println! segfaults with nightly-2018-06-24 on macOS 10.10

**Update:** this is due to LLVM assuming 16-byte alignment for a #[thread_local] static that is in fact 8-bytes aligned. Bisected PR might not be directly related, and might only have moved the static from a location that happened to be 16-bytes aligned by chance? ----  Reproduced on three CI builders with macOS 10.10.5, but not on two with 10.11.6. Not reproduced with the previous Nightly, nightly-2018-06-23. Commit range: https://github.com/rust-lang/rust/compare/cbc4c8380...60efbdead. Of the PRs merged in this range, eliminating those I’m fairly certain are unrelated leaves https://github.com/rust-lang/rust/pull/51696 and https://github.com/rust-lang/rust/pull/51723 which I *think* are unrelated but am less confident about, and https://github.com/rust-lang/rust/pull/51580 which touched 69 files in 19 commits. (Found while upgrading Servo to today’s Nightly: https://github.com/servo/servo/pull/21089#issuecomment-399746614.)",Could possibly be #50586?,"**Update:** this is due to LLVM assuming 16-byte alignment for a #[thread_local] static that is in fact 8-bytes aligned. Bisected PR might not be directly related, and might only have moved the static from a location that happened to be 16-bytes aligned by chance? ----"
rust-lang/rust,https://github.com/rust-lang/rust/issues/63710,rust-lang_rust_issues_63710,"rustdoc renders re-exported async fns incorrectly

Relates to #58027, but this is the case where an async fn is re-exported by another crate. The docs for the original function shows async fn but the re-export shows fn -> impl Future. Example: - [Original function](https://docs.rs/tokio-sync/0.2.0-alpha.2/tokio_sync/mpsc/struct.Receiver.html#method.recv) - [Re-exported](https://docs.rs/tokio/0.2.0-alpha.2/tokio/sync/mpsc/struct.Receiver.html#method.recv) <!-- TRIAGEBOT_START --> <!-- TRIAGEBOT_ASSIGN_START --> <!-- TRIAGEBOT_ASSIGN_DATA_START$${""user"":""csmoe""}$$TRIAGEBOT_ASSIGN_DATA_END --> <!-- TRIAGEBOT_ASSIGN_END --> <!-- TRIAGEBOT_END -->",Should we also consider _always_ documenting Fn() -> impl Future<Output=_> as async Fn() -> _?,"<!-- TRIAGEBOT_START --> <!-- TRIAGEBOT_ASSIGN_START --> <!-- TRIAGEBOT_ASSIGN_DATA_START$${""user"":""csmoe""}$$TRIAGEBOT_ASSIGN_DATA_END --> <!-- TRIAGEBOT_ASSIGN_END --> <!-- TRIAGEBOT_END -->"
rust-lang/rust,https://github.com/rust-lang/rust/issues/53417,rust-lang_rust_issues_53417,"Hidden items in external crate are made public when inline glob re-exported

Consider a crate parent with the following items:  where module.rs contains:  Now consider a crate foo that inline, glob-exports parent in a module par:  The item Hidden is rendered in the rustdocs for foo::par when it shouldn't be. To work around this issue, #[doc(hidden)] can be applied to the Hidden item in module.rs:  This workaround only works when #[doc(hidden)] is applied to the _source definition_. cc @QuietMisdreavus",Can you confirm that this is still an issue?,parent with the pub use module::Visible; rust pub enum Hidden { } pub enum Visible { }
rust-lang/rust,https://github.com/rust-lang/rust/issues/43596,rust-lang_rust_issues_43596,"Tracking issue for oom=panic (RFC 2116)

**Update:** This now the tracking issue for a mechanism to make OOM / memory allocation errors panic (and so by default unwind the thread) instead of aborting the process. This feature was accepted in RFC https://github.com/rust-lang/rfcs/pull/2116. It was separated from https://github.com/rust-lang/rust/issues/48043, which tracks another feature of the same RFC, since the two features will likely be stabilized at different times. **Steps:** - [ ] Implement the RFC - [ ] std::alloc::set_allocation_error_hook (tracked at https://github.com/rust-lang/rust/issues/51245) could potentially be this mechanism, but that hook is currently document as not allowed to unwind. - [ ] Adjust documentation ([see instructions on forge][doc-guide]) - [ ] Stabilization PR ([see instructions on forge][stabilization-guide]) [stabilization-guide]: https://forge.rust-lang.org/stabilization-guide.html [doc-guide]: https://forge.rust-lang.org/stabilization-guide.html#updating-documentation Original feature request: ---- Several users who are invested in the ""thread is a task"" model that Rust was originally designed around have expressed a desire to unwind in the case of OOM. Specifically each task has large and unpredictable memory usage, and tearing down the first task that encounters OOM is considered a reasonable strategy. Aborting on OOM is considered unacceptable because it would be expensive to discard all the work that the other tasks have managed to do. We already (accidentally?) specified that all allocators can panic on OOM, including the global one, with the introduction of generic allocators on collections. So in theory no one ""should"" be relying on the default oom=abort semantics. This knob would only affect the default global allocator. Presumably there would just be a function somewhere in the stdlib which is cfg'd to be a panic or abort based on this flag? This is not a replacement for proper fallible allocation handling routines requested in #29802 because most of the potential users of **that** API build with panic=abort. Also the requesters of **this** API are unwilling to go through the effort to ensure all their dependents are using fallible allocations everywhere. I am dubious on this API but I'm filling an issue so that it's written down and everyone who wants it has a place to go and assert that it is in fact good and desirable.","Should we forbid, in general, panics from allocators? Should we only allow the oom method to panic? Unsure!","**Update:** This now the tracking issue for a mechanism to make OOM / memory allocation errors panic (and so by default unwind the thread) instead of aborting the process. This feature was accepted in RFC https://github.com/rust-lang/rfcs/pull/2116. It was separated from https://github.com/rust-lang/rust/issues/48043, which tracks another feature of the same RFC, since the two features will likely be stabilized at different times. **Steps:** - [ ] Implement the RFC - [ ] std::alloc::set_allocation_error_hook (tracked at https://github.com/rust-lang/rust/issues/51245) could potentially be this mechanism, but that hook is currently document as not allowed to unwind. - [ ] Adjust documentation ([see instructions on forge][doc-guide]) - [ ] Stabilization PR ([see instructions on forge][stabilization-guide]) [stabilization-guide]: https://forge.rust-lang.org/stabilization-guide.html [doc-guide]: https://forge.rust-lang.org/stabilization-guide.html#updating-documentation Original feature request: ----"
rust-lang/rust,https://github.com/rust-lang/rust/issues/70314,rust-lang_rust_issues_70314,"file not found for module

Hi, I have a [crate](https://github.com/rust-db/refinery) that [CI](https://app.circleci.com/pipelines/github/rust-db/refinery/312/workflows/ffc62d83-b920-40f4-960b-d5b607e6ef3a/jobs/4661) fails for all the tests on the latest nightly with the following output:  the refered code is [here](https://github.com/rust-db/refinery/blob/master/refinery/tests/mod_migrations/V1__initial.rs#L5) and the macro used is defined [here](https://github.com/rust-db/refinery/blob/master/refinery_macros/src/lib.rs#L68) the same tests pass on [stable channel](https://github.com/rust-db/refinery/pull/74). If there is there is anything else that I can do to help, I am happy to contribute thanks!","Which version are you on? Your ""CI"" link requires a login. Probably a duplicate of https://github.com/rust-lang/rust/issues/70185 and you need to wait for the nightly version to update.",) that on the latest nightly
rust-lang/rust,https://github.com/rust-lang/rust/issues/65816,rust-lang_rust_issues_65816,"Tracking issue for vec_into_raw_parts

https://github.com/rust-lang/rust/pull/65705 adds:  # Things to evaluate before stabilization - [ ] [Should they return NonNull<* mut T>](https://github.com/rust-lang/rust/issues/65816#issuecomment-546405936)? - [ ] [Should they be associated functions](https://github.com/rust-lang/rust/issues/65816#issuecomment-546448429)?",Should the returned pointer be a NonNull instead? Box::raw doesn’t do that because it was stabilized before NonNull was.,# Things to evaluate before stabilization - [ ] [Should they return be NonNull<* mut T>](https://github.com/rust-lang/rust/issues/65816#issuecomment-546405936)? - [ ] [Should they be associated functions](https://github.com/rust-lang/rust/issues/65816#issuecomment-546448429)?
rust-lang/rust,https://github.com/rust-lang/rust/issues/53920,rust-lang_rust_issues_53920,"ICE: Cannot create local mono-item within future returned as impl trait

A couple of odd requirements to trigger this one, I'm not sure how to summarize. I've boiled down the case and added comments to the code below, they all mention the circumstances which are necessary to trigger the ICE. **src/lib.rs:**  **src/main.rs:**  ## Meta RUST_BACKTRACE=1 cargo build:  rustc --version --verbose:",Can you do that and edit your post?,"cargo build:  error: internal compiler error: librustc_mir/monomorphize/collector.rs:765: Cannot create local mono-itemrs:554:9 stack backtrace: 0: std::sys::unix::backtrace::tracing::imp::unwind_backtrace 1: std::sys_common::backtrace::print 2: std::panicking::default_hook::{{closure}} 3: std::panicking::default_hook 4: rustc::util::common::panic_hook 5: std::panicking::rust_panic_with_hook 6: std::panicking::begin_panic 7: rustc_errors::Handler::bug 8: rustc::session::opt_span_bug_fmt::{{closure}} 9: rustc::ty::context::tls::with_opt::{{closure}} 10: rustc::ty::context::tls::with_context_opt 11: rustc::ty::context::tls::with_opt 12: rustc::session::opt_span_bug_fmt 13: rustc::session::bug_fmt 14: rustc_mir::monomorphize::collector::should_monomorphize_locally 15: rustc_mir::monomorphize::collector::visit_instance_use 16: <rustc_mir::monomorphize::collector::MirNeighborCollector<'a, 'tcx> as rustc::mir::visit::Visitor<'tcx>>::visit_terminator_kind 17: rustc_mir::monomorphize::collector::collect_items_rec 18: rustc_mir::monomorphize::collector::collect_items_rec 19: rustc_mir::monomorphize::collector::collect_items_rec 20: rustc_mir::monomorphize::collector::collect_items_rec 21: rustc_mir::monomorphize::collector::collect_items_rec 22: rustc_mir::monomorphize::collector::collect_items_rec 23: rustc_mir::monomorphize::collector::collect_items_rec 24: rustc_mir::monomorphize::collector::collect_items_rec 25: rustc_mir::monomorphize::collector::collect_crate_mono_items::{{closure}} 26: rustc_mir::monomorphize::collector::collect_crate_mono_items 27: rustc::util::common::time 28: rustc_codegen_llvm::base::collect_and_partition_mono_items 29: rustc::ty::query::<impl rustc::ty::query::config::QueryAccessors<'tcx> for rustc::ty::query::queries::collect_and_partition_mono_items<'tcx>>::compute 30: rustc::ty::context::tls::with_context 31: rustc::dep_graph::graph::DepGraph::with_task_impl 32: rustc::ty::context::tls::with_related_context 33: rustc::ty::query::plumbing::<impl rustc::ty::context::TyCtxt<'a, 'gcx, 'tcx>>::force_query_with_job 34: rustc::ty::query::plumbing::<impl rustc::ty::context::TyCtxt<'a, 'gcx, 'tcx>>::get_query 35: rustc_codegen_llvm::base::codegen_crate 36: <rustc_codegen_llvm::LlvmCodegenBackend as rustc_codegen_utils::codegen_backend::CodegenBackend>::codegen_crate 37: rustc::util::common::time 38: rustc_driver::driver::phase_4_codegen 39: rustc_driver::driver::compile_input::{{closure}} 40: rustc::ty::context::tls::enter_context 41: <std::thread::local::LocalKey<T>>::with 42: rustc::ty::context::TyCtxt::create_and_enter 43: rustc_driver::driver::compile_input 44: rustc_driver::run_compiler_with_pool 45: <scoped_tls::ScopedKey<T>>::set 46: syntax::with_globals 47: <std::panic::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once 48: __rust_maybe_catch_panic 49: rustc_driver::run 50: rustc_driver::main 51: std::rt::lang_start::{{closure}} 52: std::panicking::try::do_call 53: __rust_maybe_catch_panic 54: std::rt::lang_start_internal 55: main query stack during panic: #0 [collect_and_partition_mono_items] collect_and_partition_mono_items end of query stack"
rust-lang/rust,https://github.com/rust-lang/rust/issues/62896,rust-lang_rust_issues_62896,"Segfault compiling libc on armv7-unknown-linux-gnueabihf

Originally reported as rust-lang/libc#1441 but I was asked to report this here. # Steps to reproduce  Output:  # Workaround Run the cargo check command with RUSTFLAGS='-Ccodegen-units=1' or add the following to the Cargo.toml:",Can you obtain backtrace using gdb or lldb?,# # Workaround Add the following to the Cargo.toml: 
rust-lang/rust,https://github.com/rust-lang/rust/issues/69672,rust-lang_rust_issues_69672,"x = yield; makes generators larger than they need to be

This generator has a size of 4 Bytes:  This one is 16 Bytes in size, even though it also does not need to keep x alive across the yield:  (where makeit is a fn returning a usize, and useit is a fn taking &usize) This seems to be fallout from https://github.com/rust-lang/rust/pull/69302. Either the layout calculation soundness fix exposed it, or the visitor changes in there caused it. This means that https://github.com/rust-lang/rust/pull/69033 will increase the size of futures created from async fns, unless this bug is fixed. <!-- TRIAGEBOT_START --> <!-- TRIAGEBOT_ASSIGN_START --> <!-- TRIAGEBOT_ASSIGN_DATA_START$${""user"":""tmandry""}$$TRIAGEBOT_ASSIGN_DATA_END --> <!-- TRIAGEBOT_ASSIGN_END --> <!-- TRIAGEBOT_END -->",Could async fns after https://github.com/rust-lang/rust/pull/69033 have such an explicit drop too or would they still be affected if https://github.com/rust-lang/rust/issues/69663 was fixed?,"<!-- TRIAGEBOT_START --> <!-- TRIAGEBOT_ASSIGN_START --> <!-- TRIAGEBOT_ASSIGN_DATA_START$${""user"":""tmandry""}$$TRIAGEBOT_ASSIGN_DATA_END --> <!-- TRIAGEBOT_ASSIGN_END --> <!-- TRIAGEBOT_END -->"
rust-lang/rust,https://github.com/rust-lang/rust/issues/70851,rust-lang_rust_issues_70851,".try_into().unwrap() is suggested even when .into() would work

  It would be better to suggest x.into() instead, which is shorter, cannot fail, and doesn't require importing a trait. <!-- TRIAGEBOT_START --> <!-- TRIAGEBOT_ASSIGN_START --> This issue has been assigned to @ryr3 via [this comment](https://github.com/rust-lang/rust/issues/70851#issuecomment-610643984). <!-- TRIAGEBOT_ASSIGN_DATA_START$${""user"":""ryr3""}$$TRIAGEBOT_ASSIGN_DATA_END --> <!-- TRIAGEBOT_ASSIGN_END --> <!-- TRIAGEBOT_END -->",Can I take this up as my first contribution?,"<!-- TRIAGEBOT_START --> <!-- TRIAGEBOT_ASSIGN_START --> <!-- TRIAGEBOT_ASSIGN_DATA_START$${""user"":""ryr3""}$$TRIAGEBOT_ASSIGN_DATA_END --> <!-- TRIAGEBOT_ASSIGN_END --> <!-- TRIAGEBOT_END -->"
rust-lang/rust,https://github.com/rust-lang/rust/issues/66258,rust-lang_rust_issues_66258,"Confusing error message when calling f(AsRef<...>, &str) with (str, &str)

The following code produces a confusing error message:  [Rust Playground link](https://play.rust-lang.org/?version=stable&mode=debug&edition=2018&gist=a7239aa6d77e5a590057ad2b7758a6a6) The confusing error when compiling the above code will be:  I first observed this error message behavior with Rust 1.39.",Can you explain what exactly you find confusing and why?,I first observed this error message behavior with Rust 1.39.
rust-lang/rust,https://github.com/rust-lang/rust/issues/55724,rust-lang_rust_issues_55724,"Tracking issue for alloc_layout_extra

This issue tracks additional methods on Layout which allow layouts to be composed to build complex layouts.  The main use case is to construct complex allocation layouts for use with the stable global allocator API. For example: - [std::collections::HashMap](https://github.com/rust-lang/rust/blob/master/src/libstd/collections/hash/table.rs#L657) - [hashbrown](https://github.com/Amanieu/hashbrown/blob/2f2af1d1e9bea9d7315b5a85d9cf5b5514f532fe/src/raw/mod.rs#L143) - Not exactly an example of use, but it would be very useful in [crossbeam-skiplist](https://github.com/crossbeam-rs/crossbeam-skiplist/blob/master/src/base.rs#L99) One concern is that not many of these methods have been extensively used in practice. In the examples given above, only extend, array and align_to are used, and I expect that these will be the most used in practice. padding_needed_for is used in the implementation of Rc::from_raw and Arc::from_raw, but in theory could be superseded by the offset returned by extend.",Why isn't the same argument true for repeat? Isn't the size available via layout.size()?,") -> Result<Layout, LayoutErr>; pub fn pad_to_align(&self"
rust-lang/rust,https://github.com/rust-lang/rust/issues/64376,rust-lang_rust_issues_64376,"Document endianess of octets function on ipv4addr/ipv6addr

The docs of the octets function on [ipv6addr](https://doc.rust-lang.org/stable/std/net/struct.Ipv6Addr.html#method.octets) and on [ipv4addr](https://doc.rust-lang.org/stable/std/net/struct.Ipv4Addr.html#method.octets) don't tell about the endianess. Network or host order? <!-- TRIAGEBOT_START --> <!-- TRIAGEBOT_ASSIGN_START --> This issue has been assigned to @fhartwig via [this comment](https://github.com/rust-lang/rust/issues/64376#issuecomment-537218262). <!-- TRIAGEBOT_ASSIGN_DATA_START$${""user"":""fhartwig""}$$TRIAGEBOT_ASSIGN_DATA_END --> <!-- TRIAGEBOT_ASSIGN_END --> <!-- TRIAGEBOT_END -->",Don't the examples already answer that?,"<!-- TRIAGEBOT_START --> <!-- TRIAGEBOT_ASSIGN_START --> <!-- TRIAGEBOT_ASSIGN_DATA_START$${""user"":""fhartwig""}$$TRIAGEBOT_ASSIGN_DATA_END --> <!-- TRIAGEBOT_ASSIGN_END --> <!-- TRIAGEBOT_END -->"
rust-lang/rust,https://github.com/rust-lang/rust/issues/67557,rust-lang_rust_issues_67557,"[MIR] simd_shuffle codegen uses self.instance as const_eval_promoted source

https://github.com/rust-lang/rust/blob/a916ac22b9f7f1f0f7aba0a41a789b3ecd765018/src/librustc_codegen_ssa/mir/block.rs#L628 It should use Instance::new(def_is, self.monomorphize(substs). (substs is the second field of StaticKind::Promoted.) While both instances are normally equal, they can differ after running the mir inliner. In that case the promoted will refer to the inlined function. @rustbot modify labels: +A-mir +C-bug +requires-nightly <!-- TRIAGEBOT_START --> <!-- TRIAGEBOT_ASSIGN_START --> This issue has been assigned to @jumbatm via [this comment](https://github.com/rust-lang/rust/issues/67557#issuecomment-568506782). <!-- TRIAGEBOT_ASSIGN_DATA_START$${""user"":""jumbatm""}$$TRIAGEBOT_ASSIGN_DATA_END --> <!-- TRIAGEBOT_ASSIGN_END --> <!-- TRIAGEBOT_END -->",Can I give this a go? @rustbot claim,"<!-- TRIAGEBOT_START --> <!-- TRIAGEBOT_ASSIGN_START --> <!-- TRIAGEBOT_ASSIGN_DATA_START$${""user"":""jumbatm""}$$TRIAGEBOT_ASSIGN_DATA_END --> <!-- TRIAGEBOT_ASSIGN_END --> <!-- TRIAGEBOT_END -->"
rust-lang/rust,https://github.com/rust-lang/rust/issues/65409,rust-lang_rust_issues_65409,"Suggest as_mut to reborrow Pin when calling method

I have seen a lot of newcomers to futures stumble with Pin<&mut Self> methods consuming the pin, if possible this seems like a good candidate for a custom diagnostic. As a self-contained example ([playground](https://play.rust-lang.org/?version=stable&mode=debug&edition=2018&gist=8cc1535cae505280361d46f504896ee9)):  currently errors with  it would be useful if this error included something like  <!-- TRIAGEBOT_START --> <!-- TRIAGEBOT_ASSIGN_START --> <!-- TRIAGEBOT_ASSIGN_DATA_START$${""user"":""gilescope""}$$TRIAGEBOT_ASSIGN_DATA_END --> <!-- TRIAGEBOT_ASSIGN_END --> <!-- TRIAGEBOT_END -->","Should we tag this A-async-await? Seems to fall under the ""async foundations"" purview, anyway. (Slight mismatch between those labels, I guess)","<!-- TRIAGEBOT_START --> <!-- TRIAGEBOT_ASSIGN_START --> <!-- TRIAGEBOT_ASSIGN_DATA_START$${""user"":""gilescope""}$$TRIAGEBOT_ASSIGN_DATA_END --> <!-- TRIAGEBOT_ASSIGN_END --> <!-- TRIAGEBOT_END -->"
rust-lang/rust,https://github.com/rust-lang/rust/issues/63748,rust-lang_rust_issues_63748,"Tracking issue for linux_syscall

PR: #63745 Unresolved questions: - [ ] Should we add syscalls to core or keep only in std? - [ ] Do we still need the workaround for syscall6 on x86?",How is a syscall supported in a core environment? I don't think that that will make sense,- [ ] Do we still need the workaround for syscall6 on x86?
rust-lang/rust,https://github.com/rust-lang/rust/issues/60758,rust-lang_rust_issues_60758,"Weird compiler behavior when compiling a dynamic library

When the code below is compiled with crate-type=cdylib the function fooBar is excluded from the symbol table, however when the code below is compiled with crate-type=dylib the function fooBar is present:  When the function is decorated with the #[no_mangle] attribute it is present in both cases. [Edit: the following is my mistake, sorry] When the function is decorated with attribute #[cfg(allow_non_snake_case] it is absent in both cases even if it is also decorated with #[no_mangle]. Furthermore, if the code is modified like so:  the compiler doesn't even report an error, apparently ignoring the function entirely. Tested on linux stable and nightly.",Did you mean to use #[allow(non_snake_case)] instead?,"[Edit: the following is my mistake, sorry]"
rust-lang/rust,https://github.com/rust-lang/rust/issues/55847,rust-lang_rust_issues_55847,"[ICE] on Nightly 2018-11-09 with simple u128 shift

This code:  Compiling with just: rustc test.rs With the last nightly:  Gives:",Does it happen for you outside of mingw?, Compiling with just: rustc test.rs
rust-lang/rust,https://github.com/rust-lang/rust/issues/54256,rust-lang_rust_issues_54256,"NLL: Poor borrow checker error message when extension of borrow happens indirectly (e.g. via method)

**EDIT**: Go [here](https://github.com/rust-lang/rust/issues/54256#issuecomment-434450019) for a smaller example. Code:  Error:  The crux of the problem is that at line visit(child, desc), table starts borrowing tu because it has type HashMap<String, SymbolDesc<'tu>>. This prevents tus.push(index.parser(source).parse()?) because it needs a mutable borrow. I wish the error message mentioned that table borrows tu because that's far from obvious. But once you see it, it's understandable why the borrow checker is not happy.",Can we maybe get a standalone example that doesn't use clang?,**EDIT**: Go [here](https://github.com/rust-lang/rust/issues/54256#issuecomment-434450019) for a smaller example.
rust-lang/rust,https://github.com/rust-lang/rust/issues/51476,rust-lang_rust_issues_51476,"Tracking issue for Ref/RefMut::map_split

Tracking issue for Ref::map_split and RefMut::map_split (feature refcell_map_split), implemented in https://github.com/rust-lang/rust/pull/51466. ## Blocking stabilization - [x] Performance (https://github.com/rust-lang/rust/pull/51630) - [x] Proof of soundness (https://github.com/rust-lang/rust/pull/51466#issuecomment-397377338) cc @RalfJung @jhjourdan",Can I just say how awesome it is for people to actually be willing to block something on a proof of soundness? I love this community <3,- [x] Performance (https://github.com/rust-lang/rust/pull/51630)
rust-lang/rust,https://github.com/rust-lang/rust/issues/63962,rust-lang_rust_issues_63962,"Hint on missing tuple parens in pattern

It'd be nice to smartly hint about missing tuple parentheses in patterns, especially in positions where parentheses nesting is required. Example of incorrect code:  The error:  This error is confusing, especially since Some is a ""tuple pattern"" which is confusable with its tuple field. An error that suggests missing tuple parentheses would be helpful. <!-- TRIAGEBOT_START --> <!-- TRIAGEBOT_ASSIGN_START --> This issue has been assigned to @sam09 via [this comment](https://github.com/rust-lang/rust/issues/63962#issuecomment-526869836). <!-- TRIAGEBOT_ASSIGN_DATA_START$${""user"":""sam09""}$$TRIAGEBOT_ASSIGN_DATA_END --> <!-- TRIAGEBOT_ASSIGN_END --> <!-- TRIAGEBOT_END -->",Can I pick up this one? Doesn't look like anybody working on this.,"<!-- TRIAGEBOT_START --> <!-- TRIAGEBOT_ASSIGN_START --> <!-- TRIAGEBOT_ASSIGN_DATA_START$${""user"":""sam09""}$$TRIAGEBOT_ASSIGN_DATA_END --> <!-- TRIAGEBOT_ASSIGN_END --> <!-- TRIAGEBOT_END -->"
rust-lang/rust,https://github.com/rust-lang/rust/issues/61966,rust-lang_rust_issues_61966,"found two different crates with name rustc_demangle that are not distinguished by differing -C metadata

When building latest master with ./x.py build -i, I get the following error: (Sorry about the image, but multiline copy+paste is currently broken in my dev build of windows terminal.) ![image](https://user-images.githubusercontent.com/111092/59786053-ab804180-92c6-11e9-9157-5689b1d54c9d.png) Error pasted for search:  Note that I've also [applied this patch](https://github.com/rust-lang/rust/issues/61911#issuecomment-502932782) to solve the build breakage on msvc 2019. Forcibly downgrading or specifying the version to to =0.1.14 like this solves the issue, but I have no idea why it happens in the first place:",Does it happen after ./x.py clean as well?,"with ./x.py build -i,"
rust-lang/rust,https://github.com/rust-lang/rust/issues/54508,rust-lang_rust_issues_54508,"#[derive] Debug, PartialEq, Hash, etc. for any function pointers, regardless of type signature

Suppose you have a situation like this:  https://play.rust-lang.org/?gist=d1bd43980abfb37197a8aaf84ed7b529&version=stable&mode=debug&edition=2015 You can implement this manually by writing something like this:  ... but this is tedious to do and leads to a lot of boilerplaite code. Even worse, this is especially bad if you have a FunctionPointer<T> **used in a struct** like this:  ... because then #[derive] **doesn't work on the Something struct**! This means you have to copy-paste all over again:  .. and over and over and over again, for each struct that you wrap / use Something in. Nevermind that this is error-prone if you add a field to the Something struct, don't forget to update the hash() and fmt() functions! This leads to a whole bunch of code that I need to copy-paste because derive doesn't work. The real-world code where I encountered this problem is: https://github.com/maps4print/azul/blob/4f2ba2e6eebdd0718d1adb15aac34c643f0f94ca/src/dom.rs#L159-L228 https://github.com/maps4print/azul/blob/4f2ba2e6eebdd0718d1adb15aac34c643f0f94ca/src/dom.rs#L358-L394 https://github.com/maps4print/azul/blob/4f2ba2e6eebdd0718d1adb15aac34c643f0f94ca/src/dom.rs#L413-L452 It's just stupid, copy-pasted code and if possible, I'd like to get rid of it with derive, but right now I sadly can't. My manual code is just a workaround for now, and I'd like this to be properly fixed somehow. Thanks in advance for any help.",what's your local rustc meta info? the first snippet compiles actually https://play.rust-lang.org/?gist=2f8baac6a2adbf13ebf815ea1c839e13&version=stable&mode=debug&edition=2015,"work: #[(Debug,, f: &mut fmt::Formatterfmt::Result { write!(f, ""NotWorkingFunctionPointer(0x{:x})"", self.0 as usize) } } impl<T: Layout> Clone for NotWorkingFunctionPointer<T> { fn clone(&self) -> &self, state: &mut Hself.0 as usize impl<T: Layout> Eq for NotWorkingFunctionPointer<T> { } impl<T: Layout> Copy for NotWorkingFunctionPointer<T> { forget to update the hash() and fmt() functions! This leads to a d like this to be properly fixed somehow. Thanks in ad"
rust-lang/rust,https://github.com/rust-lang/rust/issues/61200,rust-lang_rust_issues_61200,"'rustc' panicked at 'internal error: entered unreachable code', src/libsyntax/ast.rs:668:6 while bootstrapping

git checkout 02f5786a324c40b2d8b2d0df98456e48fb45d30c ./x.py build git checkout f492693982d1e252f5411ae3e4d560ab0dfea48a ./x.py build => ICE'd <details> <summary> Click for build log and stack trace </summary>  </details> config.toml:",Must be something else?!,<details> <summary> Click for build log and stack trace </summary> </details>
rust-lang/rust,https://github.com/rust-lang/rust/issues/61740,rust-lang_rust_issues_61740,"nightly-2019-06-10: wf checking is too slow

STR 1. git clone -b typescript https://github.com/kdy1/swc && (cd swc && git checkout 4d64b3eb5040c5ef9d2d42e93a98e76b9999f24f) 2. Modify swc/.cargo/config to  3. cd swc/typescript/checker && cargo check <details> <summary>output</summary>  </details>",Did you try other nightlies as well?,&& (cd swc && git checkout 4d64b3eb5040c5ef9d2d42e93a98e76b9999f24f)
rust-lang/rust,https://github.com/rust-lang/rust/issues/65132,rust-lang_rust_issues_65132,"VecDeque: add with_exact_capacity

I've tried this code:  The observed behavior is vd.capacity() = 2047 It would be great if there was a call like fn with_exact_capacity<T>(capacity: usize) -> Result<VecDeque<T>, ()> that would return a VecDeque only and if only it can successfully create a VecDeque with exactly the requested capacity, and return an error otherwise. Some assertions for the proposed behavior:",What's the use of such an API that isn't covered by with_capacity already?,"fn <T>(capacity: usize) -> Result<VecDeque<T>, ()>"
rust-lang/rust,https://github.com/rust-lang/rust/issues/60726,rust-lang_rust_issues_60726,"rustdoc:ICE with field that has a conditional Send impl.

Edited: Please [look at this comment](https://github.com/rust-lang/rust/issues/60726#issuecomment-491535962) for a self-contained example that reproduces the ICE. <br> In the process of finishing the 0.3 release of abi_stable,I got this ICE:  Trying it out again in nightly (rustc 1.36.0-nightly (d595b1135 2019-05-10) running on i686-unknown-linux-gnu) ,I got:  For the abi_stable::std_types::map::IntoIter type,with this type definition:  It compiles if I add a PhantomData<Rc<()>> field to IntoIter. ### Context The Send impl of DynTrait:  [Definition of InterfaceBound](https://docs.rs/abi_stable/0.3.0/abi_stable/erased_types/trait.InterfaceBound.html) [Definition of InterfaceType](https://docs.rs/abi_stable/0.3.0/abi_stable/erased_types/traits/trait.InterfaceType.html) The InterfaceType impl of ValIterInterface:  The impl_InterfaceType macro just sets the unmentioned associated types to False.",Can you provide a small example (maybe on [the playground](https://play.rust-lang.org)) that will crash reproducibly?,Edited: Please [look at this comment](https://github.com/rust-lang/rust/issues/60726#issuecomment-491535962) for a self-contained example that reproduces the ICE. <br>
rust-lang/rust,https://github.com/rust-lang/rust/issues/66955,rust-lang_rust_issues_66955,"remap-path-prefix will not re-compile properly in dev build

<details><summary>Old post</summary>Using remap-path-prefix in development builds has no effect. I'm not sure if this is intended or not, but it's not [documented](https://doc.rust-lang.org/rustc/command-line-arguments.html#--remap-path-prefix-remap-source-names-in-output). Would write a documentation PR if this is intended.</details> Changing remap-path-prefix in dev mode will not properly cause re-compilation and the debug info will not change. In release mode changing the flag will always cause proper re-compilation and update the debug info accordingly. ## Meta",Can you provide an example of this? Is this due to debuginfo included in the binary?,<details><summary>Old post</summary>.</details> Changing remap-path-prefix in dev mode will not properly cause re-compilation and the debug info will not change. In release mode changing the flag will always cause proper re-compilation and update the debug info accordingly
rust-lang/rust,https://github.com/rust-lang/rust/issues/59542,rust-lang_rust_issues_59542,"regression in deterministic codegen from 1.32 to 1.33 when using --remap-path-prefix to compensate for change in source dir

I use a [gitian build environment](https://github.com/devrandom/gitian-builder) to compile rust source in a well-defined/stabile environment. When using rust 1.32 (rust-1.32.0-1.el7.x86_64.rpm on Centos 7), I can [deterministically build](https://github.com/rust-lang/rust/issues/34902) object/binary (with the help of RUSTFLAGS=""--remap-path-prefix=%{_builddir}=BUILDDIR -C link-arg=-Wl,--build-id=0x%{githash},-S"" while running inside an rpmbuild) -- like, the build process when using 1.32 is so deterministic between runs that I can use diff (or cmp or sha256sum) to verify that two products/executables produced on different runs are identical. However, as of 1.33 (rust-1.33.0-2.el7.x86_64.rpm on Centos 7), I get significant variation from one run to another:  Among other things, the layout of the address space seems to vary (sample):  Was something intentionally changed in 1.33 that might cause this behavior? EDIT [incorporating subsequent info](https://github.com/rust-lang/rust/issues/59542#issuecomment-484719854): I found a smallish open-source project that demonstrates the issue. Run [build.sh.txt](https://github.com/rust-lang/rust/files/3096459/build.sh.txt) -- under 1.32, I get all good; under 1.33, I get bad stuff.",Did we do anything wrt using lld or something like that? cc @alexcrichton,"EDIT [incorporating subsequent info](https://github.com/rust-lang/rust/issues/59542#issuecomment-484719854): I found a smallish open-source project that demonstrates the issue. Run [build.sh.txt](https://github.com/rust-lang/rust/files/3096459/build.sh.txt) -- under 1.32, I get all good; under 1.33, I get bad stuff."
rust-lang/rust,https://github.com/rust-lang/rust/issues/64084,rust-lang_rust_issues_64084,"cargo test gives up compiling after one error

I'm making a big refactor. When attempting to clean up my tests, cargo test appears to go file by file, alphabetically when compiling the tests. As soon as one #[test] has a compile error, the build is aborted. - #[test]s following the failure do not get compiled. - Subsequent compile errors in the #[test] that fails to build are not reported. This makes finding errors very tedious. --- Editing to provide concrete case: https://play.rust-lang.org/?version=nightly&mode=debug&edition=2018&gist=c040edc58966f3fcc662002e84ec57aa gives up after the first test but should continue to report errors","What error specifically? Can you post a build log and a link to a repo (or, ideally, playground) that reproduces this behavior?",--- Editing to provide concrete case: https://play.rust-lang.org/?version=nightly&mode=debug&edition=2018&gist=c040edc58966f3fcc662002e84ec57aa gives up after the first test but should continue to report errors
rust-lang/rust,https://github.com/rust-lang/rust/issues/71089,rust-lang_rust_issues_71089,"Provide better compiler output when using ? on Option in fn returning Result or vice-versa?

It seems that rustc should provide a better error message or hint when one tries to use the ? operator on an Option in a function returning a Result or vice-versa. Currently this is the error you get:  However, it would be better to suggest the use of Option::ok_or or Option::ok_or_else, or Result::ok, instead of referencing something dependent on the try_trait feature (NoneError). <!-- TRIAGEBOT_START --> <!-- TRIAGEBOT_ASSIGN_START --> This issue has been assigned to @Duddino via [this comment](https://github.com/rust-lang/rust/issues/71089#issuecomment-613575965). <!-- TRIAGEBOT_ASSIGN_DATA_START$${""user"":""Duddino""}$$TRIAGEBOT_ASSIGN_DATA_END --> <!-- TRIAGEBOT_ASSIGN_END --> <!-- TRIAGEBOT_END -->",Can I claim this issue?,"<!-- TRIAGEBOT_START --> <!-- TRIAGEBOT_ASSIGN_START --> <!-- TRIAGEBOT_ASSIGN_DATA_START$${""user"":""Duddino""}$$TRIAGEBOT_ASSIGN_DATA_END --> <!-- TRIAGEBOT_ASSIGN_END --> <!-- TRIAGEBOT_END -->"
rust-lang/rust,https://github.com/rust-lang/rust/issues/63417,rust-lang_rust_issues_63417,"Building LLVM with Clang fails

In config.toml, under [target.x86_64-unknown-linux-gnu], one can set cc and cxx to configure the C/C++ compiler used for building LLVM. When using clang/clang++, regardless of whether use-libcxx is set under [llvm], the LLVM_LIBSTDCXX_MIN compile test fails due to a #include error. The error happens because passing --target=x86_64-unknown-linux-gnu to clang/clang++ changes the default header search paths. See https://github.com/rust-lang/rust/issues/63417#issuecomment-520510331 My system uses LLVM 8.0.0 Release. (clang version 8.0.0 (tags/RELEASE_800/final) | Target: x86_64-solus-linux) Original Issue Text:",Can you build a checkout of LLVM following their instructions on your system? If not this is probably an upstream bug we can't do much about...,"whether , even if use-libcxx = true is set under [llvm]."
rust-lang/rust,https://github.com/rust-lang/rust/issues/63848,rust-lang_rust_issues_63848,"rustc could not initialize thread_rng (on pre-getrandom Linux kernel)

<short summary of the bug> nushell: https://book.nushell.sh/en/installation / no on-point issues seen here -- searched for ""nushell"" Tried to build nushell from source and it required ""nightly"" rust. Didn't know how to install that, so I used ""cargo install nu"" I expected to see this happen: <explanation> Build complete -- the cargo install has worked with other tools. Instead, this happened: <explanation>  ## Meta rustc --version --verbose:  <!-- TRIAGEBOT_START --> <!-- TRIAGEBOT_ASSIGN_START --> This issue has been assigned to @mati865 via [this comment](https://github.com/rust-lang/rust/issues/63848#issuecomment-526196758). <!-- TRIAGEBOT_ASSIGN_DATA_START$${""user"":""mati865""}$$TRIAGEBOT_ASSIGN_DATA_END --> <!-- TRIAGEBOT_ASSIGN_END --> <!-- TRIAGEBOT_END -->",What distro and hardware are you building on? Can you post the output of uname -a?,"<!-- TRIAGEBOT_START --> <!-- TRIAGEBOT_ASSIGN_START --> <!-- TRIAGEBOT_ASSIGN_DATA_START$${""user"":""mati865""}$$TRIAGEBOT_ASSIGN_DATA_END --> <!-- TRIAGEBOT_ASSIGN_END --> <!-- TRIAGEBOT_END -->"
rust-lang/rust,https://github.com/rust-lang/rust/issues/64169,rust-lang_rust_issues_64169,"No unnecessary parentheses warning in types

Types can be parenthesized, which is useful for e.g. dyn (Foo + 'static) or similar, but often these parentheses are unnecessary. Whenever a trivial type is parenthesized the compiler should suggest to remove the parentheses  ([Playground](https://play.rust-lang.org/?version=stable&mode=debug&edition=2018&gist=519fa5c8b61d29bc48beab3b914e4d04)) Errors:  <!-- TRIAGEBOT_START --> <!-- TRIAGEBOT_ASSIGN_START --> This issue has been assigned to @sjmann via [this comment](https://github.com/rust-lang/rust/issues/64169#issuecomment-528573692). <!-- TRIAGEBOT_ASSIGN_DATA_START$${""user"":""sjmann""}$$TRIAGEBOT_ASSIGN_DATA_END --> <!-- TRIAGEBOT_ASSIGN_END --> <!-- TRIAGEBOT_END -->",Would it be ok if I try to tackle this as a first time contributor? Seems like a nice starter issue.,"<!-- TRIAGEBOT_START --> <!-- TRIAGEBOT_ASSIGN_START --> <!-- TRIAGEBOT_ASSIGN_DATA_START$${""user"":""sjmann""}$$TRIAGEBOT_ASSIGN_DATA_END --> <!-- TRIAGEBOT_ASSIGN_END --> <!-- TRIAGEBOT_END -->"
rust-lang/rust,https://github.com/rust-lang/rust/issues/65163,rust-lang_rust_issues_65163,"Implement std::iter::FromIterator<char> for Box<str>

This would allow to construct a Box<str> from an iterator, example:  <!-- TRIAGEBOT_START --> <!-- TRIAGEBOT_ASSIGN_START --> This issue has been assigned to @Duddino via [this comment](https://github.com/rust-lang/rust/issues/65163#issuecomment-538747890). <!-- TRIAGEBOT_ASSIGN_DATA_START$${""user"":""Duddino""}$$TRIAGEBOT_ASSIGN_DATA_END --> <!-- TRIAGEBOT_ASSIGN_END --> <!-- TRIAGEBOT_END -->",Did you mean [FromIterator](https://doc.rust-lang.org/std/iter/trait.FromIterator.html)?,"<!-- TRIAGEBOT_START --> <!-- TRIAGEBOT_ASSIGN_START --> <!-- TRIAGEBOT_ASSIGN_DATA_START$${""user"":""Duddino""}$$TRIAGEBOT_ASSIGN_DATA_END --> <!-- TRIAGEBOT_ASSIGN_END --> <!-- TRIAGEBOT_END -->"
rust-lang/rust,https://github.com/rust-lang/rust/issues/68324,rust-lang_rust_issues_68324,"Exponential compile time for the amount of impls.

## Summary Hi! After generating over 200 impls with macros, my project **build time just grew from a few seconds to 3 hours**. Even worse, the compilation time grows exponentially in relation to the number of impls, so I'm reporting a bug. I do not have minimal repro, but I have an open-source code and a simple reproducible recipe. ## Description - There is pub struct Texture<StorageType,InternalFormat,ElemType>. - StorageType is one of Owned, GpuOnly, and RemoteImage. - Combination (InternalFormat, ElemType) is approx 70 possibilities. Then we generate the following impls (if an impl mentions 2 types, then we generate it for each combination), where (S,I,T) stands for (StorageType,InternalFormat,ElemType):  We also generate a single data type gathering all the texture types as variants:  And a bunch of conversions like  Basically, we can assume that for **EVERY TEXTURE TYPE WE GENERATE 3 VARIANTS AND APPROXIMATELY 18 IMPLS**. And now the not-fun part. For 8 texture types, it compiles 42s. For 16 types - 1.5 mins. For 32 types - 10 mins. We've got 70 texture types - it takes hours. ## Analysis It seems we've got exponential growth here. Here is a detailed statistics: ![image](https://user-images.githubusercontent.com/1623053/72646521-a8415700-3976-11ea-80af-7392ac3d963f.png) ## How to reproduce 1. Clone the project: https://github.com/luna/ide/tree/wip/wd/rust-bug-68324 2. Run ./scripts/watch.sh - it will compile it and report the time. 3. In the file lib/core/src/system/gpu/data/texture.rs uncomment some lines between 265 and 323. (Each line contains an array of types on the far right - the number of elements is the number of additional texture types we uncomment). 4. Observe the time. The generation of structs / impls is done in: - lib/core/src/system/gpu/data/texture.rs line 326 - lib/core/src/system/gpu/data/uniform.rs line 269 - lib/core/src/system/gpu/data/uniform.rs line 272 - lib/core/src/system/gpu/data/uniform.rs line 290",What versions of the compiler do you see this behaviour with? Stable? Nightly? Is it a regression? I believe that was the intent behind @Aaron1011’s question.,". Even worse, the compilation time grows exponentially in relation to the number of impls"
rust-lang/rust,https://github.com/rust-lang/rust/issues/71580,rust-lang_rust_issues_71580,"Suboptimal hint about converting numeric types: Suggests try_into().unwrap() when into() is possible.

 ([Playground](https://play.rust-lang.org/?version=stable&mode=debug&edition=2018&gist=45961bca32186db6d4918d8c610315e9)) Errors:  I would’ve expected something like:  _Edit:_ What actually should happen is the same as currently already with _: u64, i.e. it would look like  @rustbot modify labels: A-diagnostics, T-compiler, D-papercut, C-enhancement, E-easy <!-- TRIAGEBOT_START --> <!-- TRIAGEBOT_ASSIGN_START --> This issue has been assigned to @samrat via [this comment](https://github.com/rust-lang/rust/issues/71580#issuecomment-620088274). <!-- TRIAGEBOT_ASSIGN_DATA_START$${""user"":""samrat""}$$TRIAGEBOT_ASSIGN_DATA_END --> <!-- TRIAGEBOT_ASSIGN_END --> <!-- TRIAGEBOT_END -->",Maybe it would be more robust and also more general to rewrite this suggestion code to actually look for the impl in question instead of having just a hard-coded list?,"_Edit:_ What actually should happen is the same as with _: u64, i.e. it would look like "
rust-lang/rust,https://github.com/rust-lang/rust/issues/69390,rust-lang_rust_issues_69390,"Miscompilation with clashing symbol fn names of different types

Related to #67946. Currently, it's possible to perform a read to uninitialised memory by shadowing a function with an extern declaration with the same name. For example:  ([Playground](https://play.rust-lang.org/?version=stable&mode=debug&edition=2018&gist=c074466e9e0b122b4b880c226af0b7e0)) Output:  Errors:  You can see we're able to happily print garbage this way. This is because on symbol name clash (but with different types), instead of erroring or otherwise doing work to produce distinct symbols, the function is _bitcast_ instead. See https://github.com/rust-lang/rust/issues/67946#issuecomment-586672654. In our example above, it means we produce a call to bar _as if it takes no arguments_, meaning we read garbage if we access the function arguments. You can also produce garbage return values, by shadowing a function which has no return value with a declaration that does have a return value. <!-- TRIAGEBOT_START --> <!-- TRIAGEBOT_ASSIGN_START --> This issue has been assigned to @jumbatm via [this comment](https://github.com/rust-lang/rust/issues/69390#issuecomment-590024428). <!-- TRIAGEBOT_ASSIGN_DATA_START$${""user"":""jumbatm""}$$TRIAGEBOT_ASSIGN_DATA_END --> <!-- TRIAGEBOT_ASSIGN_END --> <!-- TRIAGEBOT_END -->",How do we not have a test for this though?!,"<!-- TRIAGEBOT_START --> <!-- TRIAGEBOT_ASSIGN_START --> <!-- TRIAGEBOT_ASSIGN_DATA_START$${""user"":""jumbatm""}$$TRIAGEBOT_ASSIGN_DATA_END --> <!-- TRIAGEBOT_ASSIGN_END --> <!-- TRIAGEBOT_END -->"
rust-lang/rust,https://github.com/rust-lang/rust/issues/71701,rust-lang_rust_issues_71701,"Inline asm: =m constraint on uninitialized object variable in impl block causes the SelectionDAGBuilder to crash rustc with SIGSEGV on Linux and 0xC0000005 on Windows

<!-- Thank you for filing a bug report! 🐛 Please provide a short summary of the bug, along with any information you feel relevant to replicating the bug. --> I tried this code:  I expected to see this happen: I get it, that's a lot of inline assembly, but also highly necessary if you're developing an OS ― and backing up registers to initiate a context switch is one of those areas in which developers have no choice but to get down and dirty. So, using cargo bootimage ― which definitely worked before I wrote this code ― shouldn't be a problem here, especially given that there are plenty of crates out there that serve, to a degree, as safe abstractions. If the inline assembly has any syntax problems in it (reversed $1 and $0 for example which might be the case here) then the compiler should throw an error, not crash. Instead, this happened: No compiler errors are output, but the compiler crashes ― with, exactly as the title says, a segmentation fault, using an Arch Linux host. Here's the GDB output ― and this is using the copied-and-pasted arguments that cargo bootimage is passing down to rustc whenever I try to build this project:  Note that it's actually rustc itself that's segfaulting here ― this here is why I didn't file any bug reports with the cargo-bootimage crate maintainers: cargo bootimage invokes rustc to actually do the compiling, using a custom set of parameters that the above gdb output is preserving for posterity. Also, the instruction that rustc is segfaulting in ― again, according to GDB ― is very clearly a line of LLVM inline assembly parsing code: llvm::SelectionDAGBuilder::visitInlineAsm(llvm::ImmutableCallSite) () Since this is really the only part of my project in which I use any inline assembly at all (the x86_64 crate is of great help elsewhere), I am fairly confident that the problem here is that something in Rust's LLVM implementation just can't handle this much inline assembly at one time. Hopefully there's a way to improve this upstream.",Could you post rustc -vV and nightly version which has worked before?,"definitely rote this code. If the inline assembly has any syntax problems in it (reversed $1 and $0 for example which might be the case here) then the compiler should throw an error, not crash."
elastic/elasticsearch,https://github.com/elastic/elasticsearch/issues/33080,elastic_elasticsearch_issues_33080,"SQL: generated nested documents queries return incorrect inner_hits

In a scenario involving a nested field, the inner_hits results can be incorrect.  When selecting all fields, a row is output for each nested doc.  But if we add a where clause that's satisfied by a combination of the nested docs, then only a single row for a single nested doc is returned.  This is like this because for multiple nested queries that have inner_hits, if the inner_hits blocks are not named (https://www.elastic.co/guide/en/elasticsearch/reference/6.4/search-request-inner-hits.html#_options) which happens now in SQL, their results will clash (ES will choose one inner_hit result from the two). Probably the solution is to name differently each inner_hits block when they are used for the same nested field in multiple statements and, when the results are retrieved, to combine the inner_hits results from the multiple named queries of the same ""parent"" document. As an example, for a query like select user.* from test where ""user.first""='Alice' and ""user.last""='Smith' the generated query (simplified) should be:  And the results would look like this:  Without naming the inner_hits, stuff would be missing from the result:  Original report: https://discuss.elastic.co/t/sql-queries-against-nested-datatypes-may-be-mis-translated/145180",Would this be considered? Or are there any other workarounds that can be used to give confidence that expected results won't be dropped?,"which happens now in SQL (ES will choose one inner_hit result from the two) differently of the same ""parent"" document As an example, for a query like  the generated query (simplified) should be:  ""query"": { ""bool"": { ""must"": [ { ""nested"": { ""query"": { ""term"": { ""user.first.keyword"": { ""value"": ""Alice"" } } }, ""path"": ""user"", ""inner_hits"": { ""size"": 99, ""name"": ""324"" } } }, { ""nested"": { ""query"": { ""term"": { ""user.last.keyword"": { ""value"": ""Smith"" } } }, ""path"": ""user"", ""inner_hits"": { ""size"": 99, ""name"": ""419"" } } } ] } }  ""hits"": [ { ""_index"": ""test"", ""_type"": ""_doc"", ""_id"": ""3"", ""_score"": 2.9267395, ""_source"": { ""groupName"": ""players"", ""user"": [ { ""first"": ""John"", ""last"": ""Smith"" }, { ""first"": ""Alice"" }, { ""first"": ""Tom"" } ] }, ""inner_hits"": { ""324"": { ""hits"": { ""total"": 1, ""max_score"": 1.3862944, ""hits"": [ { ""_index"": ""test"", ""_type"": ""_doc"", ""_id"": ""3"", ""_nested"": { ""field"": ""user"", ""offset"": 1 }, ""_score"": 1.3862944, ""_source"": { ""first"": ""Alice"" } } ] } }, ""419"": { ""hits"": { ""total"": 1, ""max_score"": 1.5404451, ""hits"": [ { ""_index"": ""test"", ""_type"": ""_doc"", ""_id"": ""3"", ""_nested"": { ""field"": ""user"", ""offset"": 0 }, ""_score"": 1.5404451, ""_source"": { ""first"": ""John"", ""last"": ""Smith"" } } ] } } } } ]  ""query"": { ""bool"": { ""must"": [ { ""nested"": { ""query"": { ""term"": { ""user.first.keyword"": { ""value"": ""Alice"" } } }, ""path"": ""user"", ""inner_hits"": { ""size"": 99, ""name"": ""324"" } } }, { ""nested"": { ""query"": { ""term"": { ""user.last.keyword"": { ""value"": ""Smith"" } } }, ""path"": ""user"", ""inner_hits"": { ""size"": 99, ""name"": ""419"" } } } ] } }  ""hits"": [ { ""_index"": ""test"", ""_type"": ""_doc"", ""_id"": ""3"", ""_score"": 2.9267395, ""_source"": { ""groupName"": ""players"", ""user"": [ { ""first"": ""John"", ""last"": ""Smith"" }, { ""first"": ""Alice"" }, { ""first"": ""Tom"" } ] }, ""inner_hits"": { ""324"": { ""hits"": { ""total"": 1, ""max_score"": 1.3862944, ""hits"": [ { ""_index"": ""test"", ""_type"": ""_doc"", ""_id"": ""3"", ""_nested"": { ""field"": ""user"", ""offset"": 1 }, ""_score"": 1.3862944, ""_source"": { ""first"": ""Alice"" } } ] } }, ""419"": { ""hits"": { ""total"": 1, ""max_score"": 1.5404451, ""hits"": [ { ""_index"": ""test"", ""_type"": ""_doc"", ""_id"": ""3"", ""_nested"": { ""field"": ""user"", ""offset"": 0 }, ""_score"": 1.5404451, ""_source"": { ""first"": ""John"", ""last"": ""Smith"" } } ] } } } } ] "
elastic/elasticsearch,https://github.com/elastic/elasticsearch/issues/31009,elastic_elasticsearch_issues_31009,"Painless: Roadmap

This issue is to track all upcoming feature/refactor work on the Painless project. * [ ] (1) Documentation - Improved documentation including getting started guide, Painless specification, API, available contexts, and examples. (jdconrad) (in progress) (https://github.com/elastic/elasticsearch/issues/23777) * [x] (1) Types Removal - Replace the Painless Type with Java Class to reduce code complexity and be able to potentially take advantage of Java's upcoming value types. (jdconrad) (https://github.com/elastic/elasticsearch/pull/27847) (https://github.com/elastic/elasticsearch/pull/28329) (https://github.com/elastic/elasticsearch/pull/28346) (https://github.com/elastic/elasticsearch/pull/28364) (https://github.com/elastic/elasticsearch/pull/28429) (https://github.com/elastic/elasticsearch/pull/28433) (https://github.com/elastic/elasticsearch/pull/28466) (https://github.com/elastic/elasticsearch/pull/28471) (https://github.com/elastic/elasticsearch/pull/31699) * [ ] (5) Jar Separation - Separate Painless into it's own jar for consumption within other projects. (rjernst/jdconrad) * [ ] (2) Multi-pass Compiler - Restructure compilation into a larger amount of passes doing a smaller amount of work. Also allow for custom passes/nodes. (jdconrad) * [ ] (3) Safety Features - Improved safety features to prevent runaway loops and large memory allocations. (https://github.com/elastic/elasticsearch/issues/30139) * [ ] (5) Benchmarks - Improved benchmarking allowing for individual scripts to compared against each other with changes to the compiler. * [x] (1) Bindings - Add ability to cache values for certain methods when the values are read-only. (jdconrad) (in progress) (https://github.com/elastic/elasticsearch/pull/33042) (https://github.com/elastic/elasticsearch/pull/33274) (https://github.com/elastic/elasticsearch/pull/33440) (https://github.com/elastic/elasticsearch/pull/33865) (https://github.com/elastic/elasticsearch/pull/34370) (https://github.com/elastic/elasticsearch/pull/34410) (https://github.com/elastic/elasticsearch/pull/34424) * [ ] (3) Factory Refactor - Clean up the code used to generate the factories for contexts. (jdconrad) * [x] (2) Definition Refactor - Clean up definition to have default types and data structures that make more sense. (jdconrad) (https://github.com/elastic/elasticsearch/pull/31879) (https://github.com/elastic/elasticsearch/pull/32017) (https://github.com/elastic/elasticsearch/pull/32105) (https://github.com/elastic/elasticsearch/pull/32110) (https://github.com/elastic/elasticsearch/pull/32141) (https://github.com/elastic/elasticsearch/pull/32142) (https://github.com/elastic/elasticsearch/pull/32177) (https://github.com/elastic/elasticsearch/pull/32258) (https://github.com/elastic/elasticsearch/pull/32305) (https://github.com/elastic/elasticsearch/pull/32346) (https://github.com/elastic/elasticsearch/pull/32441) (https://github.com/elastic/elasticsearch/pull/32447) (https://github.com/elastic/elasticsearch/pull/32476) (https://github.com/elastic/elasticsearch/pull/32525) (https://github.com/elastic/elasticsearch/pull/32565) (https://github.com/elastic/elasticsearch/pull/32599) (https://github.com/elastic/elasticsearch/pull/32644) (https://github.com/elastic/elasticsearch/pull/32689) (https://github.com/elastic/elasticsearch/pull/32754) (https://github.com/elastic/elasticsearch/pull/32791) (https://github.com/elastic/elasticsearch/pull/32817) (https://github.com/elastic/elasticsearch/pull/33963) * [ ] (3) Improved Debugging - Add improved debugging features. * [ ] (3) Better Error Messages - Most of the error messages in Painless that get hit are fairly abstract and could use improvement. * [ ] (6) IDE Support - Exploration of adding an IDE support in Elipse/Intellij for Painless scripts or possibly Kibana. * [x] (2) Improved Contexts - Refactor existing contexts to give them the appropriate types/variables and whitelists along with adding new ones where necessary. (rjernst) * [ ] (3) Grammar Improvements - Refactor the grammar to remove the need for type context/regex context if possible. (jdconrad) * [ ] (4) Template Language - Make Painless a JSON friendly template language. (https://github.com/elastic/elasticsearch/issues/24529) * [ ] (5) Whitelist Versioning - Explore ideas for how to properly deprecate classes/methods from the whitelist. * [ ] ~~(5) Internal Methods - Explore ideas for how to enable/disable internal methods within the whitelist.~~ * [x] (2) Script Size - Make the size of a script based on the context its run in. (https://github.com/elastic/elasticsearch/pull/35184) * [ ] (2) Documentation API - Create a REST API to return information about a specified context's available classes, methods, and fields along the execute method signature. Create a gradle task to convert these to a nicely documented API. * [ ] ~~(3) JSON AST - Create a version of the Painless AST in JSON that is easy to pass over a wire.~~ * [x] (2) Casting Consistency - Make casts in Painless have symmetry and consistency. (jdconrad) (https://github.com/elastic/elasticsearch/pull/36097) (https://github.com/elastic/elasticsearch/pull/36455) (https://github.com/elastic/elasticsearch/pull/36506) (https://github.com/elastic/elasticsearch/pull/36571) (https://github.com/elastic/elasticsearch/pull/36747) (https://github.com/elastic/elasticsearch/pull/36804) (https://github.com/elastic/elasticsearch/pull/36945) * [x] (3) Dynamic Map Access - Create a utility method to allow maps to be accessed across multiple levels using a String with a dot delimiter from params. (stu-elastic) (https://github.com/elastic/elasticsearch/pull/43606) * [ ] (3) Read Only Variables - Allow variables passed into Painless to be read-only to allow for more efficient usage.",What do you think?,* [ ] Template Language - Make Painless a JSON friendly template language. (https://github.com/elastic/elasticsearch/issues/24529)
elastic/elasticsearch,https://github.com/elastic/elasticsearch/issues/34829,elastic_elasticsearch_issues_34829,"[Painless] Context Doc Examples

Painless has some basic documentation for contexts at https://www.elastic.co/guide/en/elasticsearch/painless/master/painless-contexts.html . Each context has a separate page that describes the what variables are available to a Painless script such as doc and params. These contexts each need to be reviewed by someone familiar with the related code and have one or more examples added. There is a data set for examples already available for download and steps to setup the an ES cluster with that data set at https://www.elastic.co/guide/en/elasticsearch/painless/master/painless-context-examples.html . For consistency it would be ideal if any examples generated were based on this data set and the contexts put in order of dependency within the index contexts page. (Ingest needs to go first.) Each example also has a test along with the curl commands as comments in the ContextExampleTests under Painless. Several contexts have changed recently and will need to updated to reflect those changes such as search -> score. The following contexts need review and/or examples: * [x] ingest (https://www.elastic.co/guide/en/elasticsearch/painless/master/painless-ingest-processor-context.html) @jdconrad (#32302) (This could an expert review of the available variables still.) * [ ] reindex (https://www.elastic.co/guide/en/elasticsearch/painless/master/painless-reindex-context.html) @jdconrad (#34024) (This could an expert review of the available variables still.) * [x] update (https://www.elastic.co/guide/en/elasticsearch/painless/master/painless-update-context.html) @rjernst (#37943) * [x] update by query (https://www.elastic.co/guide/en/elasticsearch/painless/master/painless-update-by-query-context.html) @rjernst (#37943) * [x] sort (https://www.elastic.co/guide/en/elasticsearch/painless/master/painless-sort-context.html) @romseygeek * [x] similarity (https://www.elastic.co/guide/en/elasticsearch/painless/master/painless-similarity-context.html) @romseygeek * [x] weight (https://www.elastic.co/guide/en/elasticsearch/painless/master/painless-weight-context.html) @romseygeek * [x] score (https://www.elastic.co/guide/en/elasticsearch/painless/master/painless-score-context.html) @romseygeek * [x] field (https://www.elastic.co/guide/en/elasticsearch/painless/master/painless-field-context.html) (#35130) @mayya-sharipova * [x] filter (https://www.elastic.co/guide/en/elasticsearch/painless/master/painless-filter-context.html) (#35305) @mayya-sharipova * [x] minimum should match (https://www.elastic.co/guide/en/elasticsearch/painless/master/painless-min-should-match-context.html) (#35423) @mayya-sharipova * [ ] metric aggregation initialization/map/combine/reduce (https://www.elastic.co/guide/en/elasticsearch/painless/master/painless-metric-agg-init-context.html) @polyfractal * [x] bucket script aggregation (https://www.elastic.co/guide/en/elasticsearch/painless/master/painless-bucket-agg-context.html) (#35142) @polyfractal * [x] bucket_selector aggregation (https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-pipeline-bucket-selector-aggregation.html) (#35162) @polyfractal * [ ] general aggregation script @andyb-elastic * [ ] watcher condition (https://www.elastic.co/guide/en/elasticsearch/painless/master/painless-watcher-condition-context.html) @hub-cap * [ ] watcher transform https://www.elastic.co/guide/en/elasticsearch/painless/master/painless-watcher-transform-context.html @hub-cap",Do you think we should document these contexts along side the documentation for the API that they are used in?,@romseygeek -context.html) @romseygeek * [ ] weight (https://www.elastic.co/guide/en/elasticsearch/painless/master/painless-weight-context.html) @romseygeek * [ ] score (https://www.elastic.co/guide/en/elasticsearch/painless/master/painless-score-context.html) @romseygeek * [ ] field (https://www.elastic.co/guide/en/elasticsearch/painless/master/painless-fieldmetric aggregation initialization (https://www.elastic.co/guide/en/elasticsearch/painless/master/painless-metric-agg-init-context.html) @polymetric-agg-map-context.html) @polywatcher transwatcher-trans
elastic/elasticsearch,https://github.com/elastic/elasticsearch/issues/47678,elastic_elasticsearch_issues_47678,"Rest only High Level APIs completeness

There are a number of API that are exposed via REST but not via the Transport Client. They don't necessarily have to be implemented if the goal is feature parity with the Transport Client, yet we should probably have a look at why they were not added to the Transport Client and whether it makes sense to add their support to the high-level REST Client or not. I don't think it makes sense to add support for cat API and ingest processor grok, hence I took them out already. - [x] cluster remote info (@j-bean) - [x] get source (@timoninmaxim) - [x] delete alias (@hub-cap) ~~- [ ] get upgrade (@timoninmaxim)~~ - [ ] get grok patterns (GET /_ingest/processor/grok) (@jesinity) taken from #27205",Do you plan to complete those legacy RHLC api? @hub-cap,- [ ] get grok patterns (GET /_ingest/processor/grok)
elastic/elasticsearch,https://github.com/elastic/elasticsearch/issues/31719,elastic_elasticsearch_issues_31719,"Unable to access doc values by using expression script within nested aggregated top_hits.

<!-- Bug report --> **Elasticsearch version**: 5.4.1 **JVM version** (java -version): 1.8.0_131 **OS version** (uname -a if on a Unix-like system): 6.1 (Windows Server) **Description of the problem including expected versus actual behavior**: Unable to access doc values by using expression script within nested aggregated top_hits. It works fine if I remove parent aggregation(dealers)/ change script lang to painless. **Steps to reproduce**: Index structure:  Query:  Output:  **Provide logs (if relevant)**: org.elasticsearch.transport.RemoteTransportException: [93Y1Ybq][172.16.0.11:9300][indices:data/read/search[phase/query]] Caused by: java.lang.NullPointerException [2018-07-02T14:50:17,461][DEBUG][o.e.a.s.TransportSearchAction] [93Y1Ybq] [stagingstockv21][2], node[93Y1YbqaRVG8J3kuLnc5ow], [P], s[STARTED], a[id=ISv9ED1-TpecavcZ_PA84g]: Failed to execute [SearchRequest{searchType=QUERY_THEN_FETCH, indices=[stagingstock], indicesOptions=IndicesOptions[id=38, ignore_unavailable=false, allow_no_indices=true, expand_wildcards_open=true, expand_wildcards_closed=false, allow_alisases_to_multiple_indices=true, forbid_closed_indices=true], types=[], routing='null', preference='null', requestCache=null, scroll=null, source={ ""aggregations"" : { ""dealers"" : { ""terms"" : { ""field"" : ""dealerId"", ""size"" : 4, ""min_doc_count"" : 1, ""shard_min_doc_count"" : 0, ""show_term_doc_count_error"" : false, ""order"" : [ { ""_count"" : ""desc"" }, { ""_term"" : ""asc"" } ] }, ""aggregations"" : { ""stocks"" : { ""top_hits"" : { ""from"" : 0, ""size"" : 4, ""version"" : false, ""explain"" : false, ""sort"" : [ { ""_script"" : { ""script"" : { ""inline"" : ""doc['longitude'].value"", ""lang"" : ""expression"" }, ""type"" : ""number"", ""order"" : ""asc"" } } ] } } } } } }}] org.elasticsearch.transport.RemoteTransportException: [93Y1Ybq][172.16.0.11:9300][indices:data/read/search[phase/query]] Caused by: java.lang.NullPointerException [2018-07-02T14:50:17,461][DEBUG][o.e.a.s.TransportSearchAction] [93Y1Ybq] All shards failed for phase: [query] org.elasticsearch.ElasticsearchException$1: null ~[elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [?:1.8.0_131] [?:1.8.0_131] [?:1.8.0_131] Caused by: java.lang.NullPointerException [2018-07-02T14:50:17,465][WARN ][r.suppressed ] path: /stagingstock/_search, params: {index=stagingstock} org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [?:1.8.0_131] [?:1.8.0_131] [?:1.8.0_131] Caused by: org.elasticsearch.ElasticsearchException$1 ~[elasticsearch-5.4.1.jar:5.4.1] ~[elasticsearch-5.4.1.jar:5.4.1] ~[elasticsearch-5.4.1.jar:5.4.1] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-api-2.8.2.jar:2.8.2] [elasticsearch-5.4.1.jar:5.4.1] [log4j-api-2.8.2.jar:2.8.2] [log4j-api-2.8.2.jar:2.8.2] [log4j-api-2.8.2.jar:2.8.2] [log4j-api-2.8.2.jar:2.8.2] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] ... 16 more Caused by: java.lang.NullPointerException",Can you please attach that?,"**Provide logs (if relevant)**: org.elasticsearch.transport.RemoteTransportException: [93Y1Ybq][172.16.0.11:9300][indices:data/read/search[phase/query]] Caused by: java.lang.NullPointerException [2018-07-02T14:50:17,461][DEBUG][o.e.a.s.TransportSearchAction] [93Y1Ybq] [stagingstockv21][2], node[93Y1YbqaRVG8J3kuLnc5ow], [P], s[STARTED], a[id=ISv9ED1-TpecavcZ_PA84g]: Failed to execute [SearchRequest{searchType=QUERY_THEN_FETCH, indices=[stagingstock], indicesOptions=IndicesOptions[id=38, ignore_unavailable=false, allow_no_indices=true, expand_wildcards_open=true, expand_wildcards_closed=false, allow_alisases_to_multiple_indices=true, forbid_closed_indices=true], types=[], routing='null', preference='null', requestCache=null, scroll=null, source={ ""aggregations"" : { ""dealers"" : { ""terms"" : { ""field"" : ""dealerId"", ""size"" : 4, ""min_doc_count"" : 1, ""shard_min_doc_count"" : 0, ""show_term_doc_count_error"" : false, ""order"" : [ { ""_count"" : ""desc"" }, { ""_term"" : ""asc"" } ] }, ""aggregations"" : { ""stocks"" : { ""top_hits"" : { ""from"" : 0, ""size"" : 4, ""version"" : false, ""explain"" : false, ""sort"" : [ { ""_script"" : { ""script"" : { ""inline"" : ""doc['longitude'].value"", ""lang"" : ""expression"" }, ""type"" : ""number"", ""order"" : ""asc"" } } ] } } } } } }}] org.elasticsearch.transport.RemoteTransportException: [93Y1Ybq][172.16.0.11:9300][indices:data/read/search[phase/query]] Caused by: java.lang.NullPointerException [2018-07-02T14:50:17,461][DEBUG][o.e.a.s.TransportSearchAction] [93Y1Ybq] All shards failed for phase: [query] org.elasticsearch.ElasticsearchException$1: null ~[elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [?:1.8.0_131] [?:1.8.0_131] [?:1.8.0_131] Caused by: java.lang.NullPointerException [2018-07-02T14:50:17,465][WARN ][r.suppressed ] path: /stagingstock/_search, params: {index=stagingstock} org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [?:1.8.0_131] [?:1.8.0_131] [?:1.8.0_131] Caused by: org.elasticsearch.ElasticsearchException$1 ~[elasticsearch-5.4.1.jar:5.4.1] ~[elasticsearch-5.4.1.jar:5.4.1] ~[elasticsearch-5.4.1.jar:5.4.1] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-core-2.8.2.jar:2.8.2] [log4j-api-2.8.2.jar:2.8.2] [elasticsearch-5.4.1.jar:5.4.1] [log4j-api-2.8.2.jar:2.8.2] [log4j-api-2.8.2.jar:2.8.2] [log4j-api-2.8.2.jar:2.8.2] [log4j-api-2.8.2.jar:2.8.2] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] [elasticsearch-5.4.1.jar:5.4.1] ... 16 more Caused by: java.lang.NullPointerException"
elastic/elasticsearch,https://github.com/elastic/elasticsearch/issues/33242,elastic_elasticsearch_issues_33242,"Elastic search 6.4.0 test case execution failed due to an exception.

<!-- ** Please read the guidelines below. ** Issues that do not follow these guidelines are likely to be closed. 1. GitHub is reserved for bug reports and feature requests. The best place to ask a general question is at the Elastic [forums](https://discuss.elastic.co). GitHub is not the place for general questions. 2. Is this bug report or feature request for a supported OS? If not, it is likely to be closed. See https://www.elastic.co/support/matrix#show_os 3. Please fill out EITHER the feature request block or the bug report block below, and delete the other block. --> <!-- Bug report --> **Elasticsearch version** (bin/elasticsearch --version): v6.4.0 **JVM version** (java -version): openjdk version ""10.0.2-adoptopenjdk"" **OS version** (uname -a if on a Unix-like system): Linux ubuntu 4.4.0-21-generic #37-Ubuntu SMP Mon Apr 18 18:33:37 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux **Description of the problem including expected versus actual behavior**: Elastic search build is succesful but test case execution failed with an exception. **Actual:** all test cases should pass **Observed:**  Found the 1testtasks: :client:benchmark:test -> class org.gradle.api.tasks.testing.Test_Decorated BUILD FAILED 14:00:46.338 [DEBUG] [org.gradle.launcher.daemon.client.DaemonClientInputForwarder] Dispatching close input message: org.gradle.launcher.daemon.protocol.CloseInput@5996d8e4 14:00:46.338 [DEBUG] [org.gradle.launcher.daemon.client.DaemonClientConnection] thread 18: dispatching class org.gradle.launcher.daemon.protocol.CloseInput 14:00:46.339 [DEBUG] [org.gradle.launcher.daemon.client.DaemonClient] Received result Failure[value=org.gradle.initialization.ReportedException: org.gradle.internal.exceptions.LocationAwareException: Found the 1testtasks: :client:benchmark:test -> class org.gradle.api.tasks.testing.Test_Decorated] from daemon DaemonInfo{pid=14844, address=[00d5ce49-cbfb-46f4-ac5c-2706300e9b49 port:40934, addresses:[/0:0:0:0:0:0:0:1, /127.0.0.1]], state=Busy, lastBusy=1535896561769, context=DefaultDaemonContext[uid=94452509-2ac2-4fc5-b600-8e097e73fc94,javaHome=/home/ubuntu/ES640_oracle/jdk-10.0.2,daemonRegistryDir=/home/ubuntu/.gradle/daemon,pid=14844,idleTimeout=120000,daemonOpts=-Xmx2g,-Dfile.encoding=UTF8,-Duser.country=US,-Duser.language=en,-Duser.variant]} (build should be done). 14:00:46.339 [DEBUG] [org.gradle.launcher.daemon.client.DaemonClientConnection] thread 1: dispatching class org.gradle.launcher.daemon.protocol.Finished 14:00:46.339 [DEBUG] [org.gradle.launcher.daemon.client.DaemonClientConnection] Problem dispatching message to the daemon. Performing 'on failure' operation... 14:00:46.346 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 14:00:46.347 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] FAILURE: Build failed with an exception. 14:00:46.347 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 14:00:46.347 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong: 14:00:46.347 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Could not dispatch a message to the daemon. Elastic search 6.3.x is getting built and TC are passing with the same steps mentioned below. **Steps to reproduce**: 1. Install and export JAVA (adopt open jdk 10 hotspot) from https://github.com/AdoptOpenJDK/openjdk10-nightly/releases/download/201808211500/OpenJDK10_x64_Linux_201808211500.tar.gz 2. cd /home/test 3. git clone https://github.com/elastic/elasticsearch 4. cd elasticsearch 5. git checkout v6.4.0 6. ./gradlew assemble 7. ./gradlew test --continue -Dtests.haltonfailure=false -Dtests.es.node.mode=network **Provide logs (if relevant)**: Full log : [ES640Network_log.txt](https://github.com/elastic/elasticsearch/files/2332614/ES640Network_log.txt)",Would you open a topic on the forum?,"Found the 1testtasks: :client:benchmark:test -> class org.gradle.api.tasks.testing.Test_Decorated BUILD FAILED 14:00:46.338 [DEBUG] [org.gradle.launcher.daemon.client.DaemonClientInputForwarder] Dispatching close input message: org.gradle.launcher.daemon.protocol.CloseInput@5996d8e4 14:00:46.338 [DEBUG] [org.gradle.launcher.daemon.client.DaemonClientConnection] thread 18: dispatching class org.gradle.launcher.daemon.protocol.CloseInput 14:00:46.339 [DEBUG] [org.gradle.launcher.daemon.client.DaemonClient] Received result Failure[value=org.gradle.initialization.ReportedException: org.gradle.internal.exceptions.LocationAwareException: Found the 1testtasks: :client:benchmark:test -> class org.gradle.api.tasks.testing.Test_Decorated] from daemon DaemonInfo{pid=14844, address=[00d5ce49-cbfb-46f4-ac5c-2706300e9b49 port:40934, addresses:[/0:0:0:0:0:0:0:1, /127.0.0.1]], state=Busy, lastBusy=1535896561769, context=DefaultDaemonContext[uid=94452509-2ac2-4fc5-b600-8e097e73fc94,javaHome=/home/ubuntu/ES640_oracle/jdk-10.0.2,daemonRegistryDir=/home/ubuntu/.gradle/daemon,pid=14844,idleTimeout=120000,daemonOpts=-Xmx2g,-Dfile.encoding=UTF8,-Duser.country=US,-Duser.language=en,-Duser.variant]} (build should be done). 14:00:46.339 [DEBUG] [org.gradle.launcher.daemon.client.DaemonClientConnection] thread 1: dispatching class org.gradle.launcher.daemon.protocol.Finished 14:00:46.339 [DEBUG] [org.gradle.launcher.daemon.client.DaemonClientConnection] Problem dispatching message to the daemon. Performing 'on failure' operation... 14:00:46.346 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 14:00:46.347 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] FAILURE: Build failed with an exception. 14:00:46.347 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 14:00:46.347 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong: 14:00:46.347 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Could not dispatch a message to the daemon."
elastic/elasticsearch,https://github.com/elastic/elasticsearch/issues/47502,elastic_elasticsearch_issues_47502,"Add API keys with unique names using UI

We can consider the idea to provide a UI to add new API keys to Elasticsearch. API keys have a name attribute that is not enforced to be unique for technical constraints that can be found in https://github.com/elastic/elasticsearch/issues/46646. However, since name is the attribute that identifies a specific key from a high-level user perspective, having multiple keys with the same name could lead to confusion, for example when it's time to revoke one of them. In order to increase usability, we can consider to enforce uniqueness of name in the UI. It can be done by checking if a key with the same name exists before creating a new one. This will not ensure name to be unique, but if we make this check on a per-user basis, it is unlikely to happen that the same user is inserting two keys with the same name before the index is refreshed. There are two possible approaches if there is already a key with the same name field: 1. raise an error and abort the task: this approach is stronger to avoid duplicates, but it cannot guarantee it anyway since they can be added via API and operations are not atomic 1. raise a warning, and ask for confirmation that it is intentional to create it anyway: this is softer, but it doesn't give the feeling that there is a guarantee of uniqueness (since it's not)",Maybe worth summarizing them in order to facilitate the discussion and decision in this issue?,": this approach is stronger to avoid duplicates, but it cannot guarantee it anyway since they can be added via API and operations are not atomic"
spring-projects/spring-framework,https://github.com/spring-projects/spring-framework/issues/22732,spring-projects_spring-framework_issues_22732,"Can't use JNDI test context (SimpleNamingContext) with Hibernate 5.2

**Affects:** spring-test 5.1.6.RELEASE I am trying to create datasources to bind them in JNDI to run my JUnit tests.  When I do a lookup() it works, as the code bellow:  But when running Hibernate, it throws an exception: <details> <summary>Stack trace</summary>  </details> This happens because the mock is incomplete. SimpleNamingContext class has unimplemented methods, so we can't use it with Hibernate 5.2.",Does [Simple-JNDI](https://github.com/h-thurow/Simple-JNDI) work for you?,<details> <summary>Stack trace</summary> </details>
spring-projects/spring-framework,https://github.com/spring-projects/spring-framework/issues/23241,spring-projects_spring-framework_issues_23241,"Accept header with trailing comma produces HTTP ""406 Not Acceptable""

Spring version: 5.1.8.RELEASE Send a http request with header like this, containing ,  at the end.  HeaderContentNegotiationStrategy will produce a HttpMediaTypeNotAcceptableException.  That http request works before 5.1.2.RELEASE. Maybe the cause of this is the parsing method refactoring in this commit [f4b05dc](https://github.com/spring-projects/spring-framework/commit/f4b05dc2e730ca667daf8861c6eb2d9a6b83d534#diff-47ef64129825b0e047375d409c7d95a9L260). StringUtils.tokenizeToStringArray(mimeTypes, "","") will trim tokens and ignore empty tokens by default, but now, the replacement method not support that anymore.",What is your use case for that? What client is sending the Accept request header with a trailing comma?, the is the ing.
spring-projects/spring-framework,https://github.com/spring-projects/spring-framework/issues/23716,spring-projects_spring-framework_issues_23716,"Tomcat 8.0.33 crashing with Spring security

Hi, I am having issues with tomcat crash every time when load occurs on JVM, I am getting below error logs initially it is working fine. When some users are increased to access the server then server crashing every time. My java version is 8.0_222 please help me to get out of this issue I am using spring 4.0.0 version",What makes you believe the issue is related to Spring Security or to the Spring Framework? There is nothing Spring related in the log.,I am using spring 4.0.0 version
TrinityCore/TrinityCore,https://github.com/TrinityCore/TrinityCore/issues/23002,TrinityCore_TrinityCore_issues_23002,"Unable to send Quest Links in Chat

<!--- (**********************************) (** Fill in the following fields **) (**********************************) ---> **Description:** Players unable too send quest links in chat too groups, guilds or any near by player. **Current behaviour:** Players unable too send quest links in chat. Server appears too be blocking the links among sending them. Most likely related too the this PR made back in September https://github.com/TrinityCore/TrinityCore/pull/22417. **Expected behaviour:** Should be able send the links just like items and professions. **Steps to reproduce the problem:** 1. Enable UI.ShowQuestLevelsInDialogs and start/restart your server. 2. Make any class or race and log into the game or use an existing character. 3. Go to any Quest Giver and choose a quest. 4. Try to send the Quest Link in chat /s or any chat channel for that matter. **Log Output:**  **Branch(es):** 3.3.5 **TC rev. hash/commit:** https://github.com/TrinityCore/TrinityCore/commit/6026da0b6a0676d01def2583c56de856b88da389 **Operating system:** Debian 9 (Stretch) **Related To** https://github.com/TrinityCore/TrinityCore/pull/22417 <!--- Notes - This template is for problem reports. For other types of report, edit it accordingly. - For fixes containing C++ changes, create a Pull Request. --->",Do you have a custom config to show the quest level in the chat ?,Enable UI.ShowQuestLevelsInDialogs and start/restart your server. 2.
TrinityCore/TrinityCore,https://github.com/TrinityCore/TrinityCore/issues/23411,TrinityCore_TrinityCore_issues_23411,"[master] Update Creature Template Data for 8.1.5

<!--- (**********************************) (** Fill in the following fields **) (**********************************) ---> **Description:** **8.1.5 Bruteforce Data** 1. Apply:  2. Apply https://gist.github.com/funjoker/3c657c4fbfe70693820aebcc88eec7f0 3. VerifiedBuild: https://gist.github.com/funjoker/9f51048278f62a91ca6d6689dc6c65a8 Locales: deDE: https://gist.github.com/funjoker/c9144d256e220a643d77a234816d4f59 esES: https://gist.github.com/funjoker/5d790cb26981fe429f87ac8e1a823c18 esMX: https://gist.github.com/funjoker/3d389a196625325919e079aae5c909fa frFR: https://gist.github.com/funjoker/eca784db7c6c79583e79b2961915c8b7 itIT: https://gist.github.com/funjoker/10a9def170f3ce5c3743d24512cc9e50 koKR: https://gist.github.com/funjoker/4b3ed05e72df2cb59ff449a7f8db9aff ptBR: https://gist.github.com/funjoker/d19a207d452652ee56c4b68c13b7d423 ruRU: https://gist.github.com/funjoker/5bbccf3972557e9dbc7d5a62d92453a4 zhCN: https://gist.github.com/funjoker/400d65a472498d658b9f5019212784df zhTW: https://gist.github.com/funjoker/34b2ce043fcbb9aa4051dbfb92e26984 **TC rev. hash/commit:** 6b29c4da76fbdcb55b4a89f485a5cff58c4296b9",Can this be added along with mdX7 8.0.1 bruteforce https://github.com/TrinityCore/TrinityCore/issues/22848?,Locales: deDE: https://gist.github.com/funjoker/c9144d256e220a643d77a234816d4f59 esES: https://gist.github.com/funjoker/5d790cb26981fe429f87ac8e1a823c18 esMX: https://gist.github.com/funjoker/3d389a196625325919e079aae5c909fa frFR: https://gist.github.com/funjoker/eca784db7c6c79583e79b2961915c8b7 itIT: https://gist.github.com/funjoker/10a9def170f3ce5c3743d24512cc9e50 koKR: https://gist.github.com/funjoker/4b3ed05e72df2cb59ff449a7f8db9aff ptBR: https://gist.github.com/funjoker/d19a207d452652ee56c4b68c13b7d423 ruRU: https://gist.github.com/funjoker/5bbccf3972557e9dbc7d5a62d92453a4 zhCN: https://gist.github.com/funjoker/400d65a472498d658b9f5019212784df zhTW: https://gist.github.com/funjoker/34b2ce043fcbb9aa4051dbfb92e26984
TrinityCore/TrinityCore,https://github.com/TrinityCore/TrinityCore/issues/21331,TrinityCore_TrinityCore_issues_21331,"Core/Pools: Objects in the pool stop spawning

**Description:** Creatures (probably GameObjects too) that belong to a pool, will stop to respawn after some time. I can confirm this happens with Webbed Crusaders(30268 30273) and Serfex the Reaver(28083), for example. I only monitored it for a few hours, but I think they won't respawn again until a server restart. **Current behaviour:** After some number os respawns, the creature no longer spawn. **Expected behaviour:** Creatures should keep respawning within the right time. **Steps to reproduce the problem:** To force it happen: 1. Execute: DELETE FROM characters.respawn; UPDATE world.creature SET spawntimesecs = 3 where id IN (30273,30268); 2. Start server 3. .gm on 4. .go xyz 6394 229 395 571 5. .cast 44659 6. Repeat step 3 a few times, usally after the second/third time the Webbed Crusaders won't spawn anymore. **Branch(es):** 3.3.5 **TC rev. hash/commit:** e45d54d80e3f",Do you mean this issue or the one you linked?,"273,326);. Start server 3. .gm on 4"
TrinityCore/TrinityCore,https://github.com/TrinityCore/TrinityCore/issues/24255,TrinityCore_TrinityCore_issues_24255,"DB/Creature: Kirin Tor Mage in Ulduar

Description: Due to the error of NPC spawn point, the characters are overlapped. .go xyz -814.109985 -201.125 429.924988 603 NPC ID: 33672 Kirin Tor Mage in Ulduar Raid must be summoned via an event https://www.youtube.com/watch?v=UuPPW5jJaP4&feature=youtu.be Branch(es): 3.3.5 TC rev. hash/commit: 961adea3727e32c068245735e76db2445c2fcd2d Operating system: CentOS",could you describe the issue with more words ?,"Due to the error of NPC spawn point, the characters are overlapped."
TrinityCore/TrinityCore,https://github.com/TrinityCore/TrinityCore/issues/24351,TrinityCore_TrinityCore_issues_24351,"The NPC did not follow the waypoint

Description: It's a pervasive problem, not just in the Undercity. .go cr id 4571 ![bug1](https://user-images.githubusercontent.com/3367999/77447480-b1451d80-6e2a-11ea-92b1-8f864e5329aa.gif) .go cr id 16584 ![bug2](https://user-images.githubusercontent.com/3367999/79101298-d4644c80-7d9a-11ea-8266-fc290bd42fe4.gif) .go cr id 3209 ![bug9](https://user-images.githubusercontent.com/3367999/79101951-4ee19c00-7d9c-11ea-92e0-942206e5ae89.gif) Branch(es): 3.3.5 TC rev. hash/commit: b166d12cbef07a72828d8d20a2130926a5cae47e Operating system: CentOS",Should we disable path finding for all creatures in undercity?,.go cr id 16584 ![bug2](https://user-images.githubusercontent.com/3367999/79101298-d4644c80-7d9a-11ea-8266-fc290bd42fe4.gif) .go cr id 3209 ![bug9](https://user-images.githubusercontent.com/3367999/79101951-4ee19c00-7d9c-11ea-92e0-942206e5ae89.gif)
TrinityCore/TrinityCore,https://github.com/TrinityCore/TrinityCore/issues/24396,TrinityCore_TrinityCore_issues_24396,"The fat man swims in shallow water

Description: The fat man swims in shallow water. GUID: 128082 Entry: 16018 .go xyz 3034.489990 -3177.299316 294.063263 533 ![bug1](https://user-images.githubusercontent.com/3367999/78502077-c0fd2400-7791-11ea-892d-2fb417699f4a.gif) Branch(es): 3.3.5 TC rev. hash/commit: b166d12 Operating system: CentOS",Could you add creature info?,GUID: 128082 Entry: 16018
pandas-dev/pandas,https://github.com/pandas-dev/pandas/issues/31074,pandas-dev_pandas_issues_31074,"Pandas .plot() on regularly spaced timeseries result in slow plotting/interaction

#### Code Sample, a copy-pastable example if possible  #### Problem description I noticed that plotting time series using .plot() sometimes resulted in very slow and unresponsive plots, where it is difficult to interact with the figure (e.g. pan). I think this happens with regularly spaced time series where there either is a frequency defined or pandas is able to infer the frequency. Perhaps it has something to do with the plot tick labels on the x (time) axis? It does not happen when plotting time series that are irregular, and thus when pandas does not style the plot ticks and tick labels. In the example code the first and the third plot are smooth to interact with, while the second plot is lagging terribly. Also see screenshot of the second plot with the pandas styled tick labels: ![image](https://user-images.githubusercontent.com/2330659/72536770-2d474600-387b-11ea-9dac-0199ea5e0962.png) If changing the dt_range from minute frequency to hour frequency (replace minutes with hours) the pandas.plot() becomes much smooth to interact with despite having the same series size, and I think because it has much fewer ticks and labels: ![image](https://user-images.githubusercontent.com/2330659/72537014-a0e95300-387b-11ea-9b9a-6f83e2db24dd.png) So it might be a combination of the size of series plotted and how the ticks are drawn/updated? Are there ways to disable the styled pandas ticks somehow? I also notice th result in very different conversions of timestamps to numeric x values - is there a way to also disable that behavior in pandas so it is compatible with pyplot? #### Expected Output Equally responsive plotting as with pyplot.plot() #### Output of pd.show_versions() <details> INSTALLED VERSIONS ------------------ commit : None python : 3.7.6.final.0 python-bits : 64 OS : Windows OS-release : 10 machine : AMD64 processor : Intel64 Family 6 Model 78 Stepping 3, GenuineIntel byteorder : little LC_ALL : None LANG : en LOCALE : None.None pandas : 0.25.3 numpy : 1.17.4 pytz : 2019.3 dateutil : 2.8.1 pip : 19.3.1 setuptools : 44.0.0.post20200106 Cython : 0.29.14 pytest : 5.3.2 hypothesis : 4.54.2 sphinx : 2.3.1 blosc : None feather : None xlsxwriter : 1.2.7 lxml.etree : 4.4.2 html5lib : 1.0.1 pymysql : None psycopg2 : 2.8.4 (dt dec pq3 ext lo64) jinja2 : 2.10.3 IPython : 7.11.1 pandas_datareader: None bs4 : 4.8.2 bottleneck : 1.3.1 fastparquet : None gcsfs : None lxml.etree : 4.4.2 matplotlib : 3.1.1 numexpr : 2.7.0 odfpy : None openpyxl : 3.0.2 pandas_gbq : None pyarrow : None pytables : None s3fs : None scipy : 1.3.2 sqlalchemy : 1.3.12 tables : 3.6.1 xarray : 0.14.1 xlrd : 1.2.0 xlwt : 1.3.0 xlsxwriter : 1.2.7 </details> Edit: This performance issue might be related to #15071","maybe you want pd.plotting.plot_params[""x_compat""] = False? or df.plot(x_compat=False)?",Edit: This performance issue might be related to #15071
pandas-dev/pandas,https://github.com/pandas-dev/pandas/issues/26699,pandas-dev_pandas_issues_26699,"Replace flat columns with hierarchical while merge

Is there is any standard method to replace flat columns with hierarchical while merge the dataframes? I mean something like:  **UPD 2019-06-13** A bit more robust function on input data.",Can you provide sample data frames and the expected result?,"columns = [(c,) if isinstance(c, str) or not np.iterable(c) else c for c in columns] for c in columns] **UPD 2019-06-13** A bit more robust function on input data."
pandas-dev/pandas,https://github.com/pandas-dev/pandas/issues/22262,pandas-dev_pandas_issues_22262,"object of type 'CategoricalDtype' has no len()

#### Code Sample, a copy-pastable example if possible Using 0.23.3 My goal: I have an enum with 2 values Client/Server and would like to make use of the categorical dtype for performance reasons.  triggers  #### Problem description [this should explain **why** the current behaviour is a problem and why the expected output is a better solution.] I want to specify the dtype otherwise pandas convert my enums into their integer values. I prefer to keep enums since when printing the dataframe it's easier to read ""server"" than ""2"". #### Output of pd.show_versions() <details> [paste the output of pd.show_versions() here below this line] INSTALLED VERSIONS ------------------ commit: None python: 3.6.6.final.0 python-bits: 64 OS: Linux OS-release: 4.14.24 machine: x86_64 processor: byteorder: little LC_ALL: None LANG: fr_FR.UTF-8 LOCALE: fr_FR.UTF-8 pandas: 0.23.3 pytest: None pip: 18.0 setuptools: 40.0.0 Cython: 0.28.3 numpy: 1.14.5 scipy: 1.1.0 pyarrow: None xarray: None IPython: None sphinx: None patsy: None dateutil: 2.7.3 pytz: 2018.5 blosc: None bottleneck: 1.2.1 tables: 3.4.4 numexpr: 2.6.6 feather: None matplotlib: 2.2.2 openpyxl: 2.5.4 xlrd: 0.9.4 xlwt: 1.3.0 xlsxwriter: None lxml: 4.2.3 bs4: 4.6.0 html5lib: 1.0.1 sqlalchemy: 1.2.10 pymysql: None psycopg2: None jinja2: None s3fs: None fastparquet: None pandas_gbq: None pandas_datareader: None </details>",Can you make a reproducible example? http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports,"import pandas as pd from enum import IntEnum, auto"
pandas-dev/pandas,https://github.com/pandas-dev/pandas/issues/25742,pandas-dev_pandas_issues_25742,"DataFrame regex with dictionary will not work correctly.

#### Code Sample  #### Problem description Does not replace the newline character. Also checked with other escaped characters like spaces, tabs. However, the other regex expression works:  #### Expected Output  #### Additional information Working:  [Related SO question](https://stackoverflow.com/q/55190441/3765319). #### Output of pd.show_versions() <details> INSTALLED VERSIONS ------------------ commit: None python: 3.7.1.final.0 python-bits: 64 OS: Windows OS-release: 10 machine: AMD64 processor: Intel64 Family 6 Model 158 Stepping 9, GenuineIntel byteorder: little LC_ALL: None LANG: None LOCALE: None.None pandas: 0.24.1 pytest: None pip: 19.0.3 setuptools: 40.8.0 Cython: 0.29.5 numpy: 1.15.4 scipy: 1.1.0 pyarrow: None xarray: 0.11.3 IPython: 7.3.0 sphinx: None patsy: 0.5.1 dateutil: 2.8.0 pytz: 2018.9 blosc: None bottleneck: 1.2.1 tables: 3.4.4 numexpr: 2.6.9 feather: None matplotlib: 3.0.3 openpyxl: 2.6.0 xlrd: 1.2.0 xlwt: 1.3.0 xlsxwriter: 1.1.5 lxml.etree: None bs4: None html5lib: None sqlalchemy: 1.2.18 pymysql: None psycopg2: 2.7.6.1 (dt dec pq3 ext lo64) jinja2: 2.10 s3fs: None fastparquet: None pandas_gbq: None pandas_datareader: None gcsfs: None </details>",Can you try on 0.24.2? I think this was fixed by https://github.com/pandas-dev/pandas/pull/25266.,#### Additional information Working:  [Related SO question](https://stackoverflow.com/q/55190441/3765319).
pandas-dev/pandas,https://github.com/pandas-dev/pandas/issues/26288,pandas-dev_pandas_issues_26288,"DataFrame with datetime column cannot concat with non-identical columns

#### Code Sample, a copy-pastable example if possible",Can you provide some more code to allow us to run this?,': 2} pd.to_datetime(a.t) b = pd.Data
pandas-dev/pandas,https://github.com/pandas-dev/pandas/issues/25257,pandas-dev_pandas_issues_25257,"concat(..., copy=False) with datetime tz-aware data raises ValueError: cannot create a DatetimeTZBlock without a tz

#### Code Sample, a copy-pastable example if possible  #### Problem description Running pd.concat with a single dataframe and copy=False fails, giving an error about cannot create a DatetimeTZBlock without a tz. Closely related: - https://github.com/pandas-dev/pandas/issues/19420 - https://github.com/pandas-dev/pandas/issues?q=is%3Aopen+is%3Aissue+label%3AReshaping+label%3ATimezones #### Expected Output The original dataframe, since I'm only concat-ing a single dataframe. #### Output of pd.show_versions() <details> INSTALLED VERSIONS ------------------ commit: None python: 3.6.8.final.0 python-bits: 64 OS: Linux OS-release: 4.18.0-14-generic machine: x86_64 processor: x86_64 byteorder: little LC_ALL: None LANG: en_US.UTF-8 LOCALE: en_US.UTF-8 pandas: 0.24.1 pytest: 4.2.0 pip: 19.0.1 setuptools: 40.6.3 Cython: 0.29.4 numpy: 1.15.4 scipy: 1.2.0 pyarrow: 0.11.1 xarray: None IPython: 7.2.0 sphinx: None patsy: None dateutil: 2.7.5 pytz: 2018.7 blosc: None bottleneck: None tables: None numexpr: None feather: None matplotlib: 3.0.2 openpyxl: 2.4.10 xlrd: 1.2.0 xlwt: None xlsxwriter: 1.1.2 lxml.etree: None bs4: None html5lib: None sqlalchemy: 1.2.17 pymysql: None psycopg2: 2.7.6.1 (dt dec pq3 ext lo64) jinja2: 2.10 s3fs: None fastparquet: 0.2.1 pandas_gbq: None pandas_datareader: None gcsfs: None </details>",Can you post the full traceback? Any interest in working on this?,"Edit: added full traceback # Traceback (most recent call last): # File ""<stdin>"", line 1, in <module> # File "".../pandas/core/reshape/concat.py"", line 229, in concat # return op.get_result() # File "".../pandas/core/reshape/concat.py"", line 426, in get_result # copy=self.copy) # File "".../pandas/core/internals/managers.py"", line 2055, in concatenate_block_managers # b = b.make_block_same_class(values, placement=placement) # File "".../pandas/core/internals/blocks.py"", line 235, in make_block_same_class # klass=self.__class__, dtype=dtype) # File "".../pandas/core/internals/blocks.py"", line 3095, in make_block # return klass(values, ndim=ndim, placement=placement) # File "".../pandas/core/internals/blocks.py"", line 1680, in __init__ # values = self._maybe_coerce_values(values) # File "".../pandas/core/internals/blocks.py"", line 2266, in _maybe_coerce_values # raise "") # ValueError: cannot create a DatetimeTZBlock without a tz"
pandas-dev/pandas,https://github.com/pandas-dev/pandas/issues/21959,pandas-dev_pandas_issues_21959,"DataFrame.nunique is incorrect for DataFrame with no columns

(edit by @TomAugspurger) Current output:  Expected Output is an empty series:  Not sure what the expected dtype of that Series should be... probably object. original post below: --- #### Code Sample, a copy-pastable example if possible With Pandas 0.20.3  With pandas 0.23.3  Note:  #### Problem description The change of behavior is a bit disturbing, and seems like it is a bug: nunique() ends up creating a Series, and it should be a Series of the df columns, but that doesn't seem to be the case here, instead it is picking up the index of the df. This is likely related to: https://github.com/pandas-dev/pandas/issues/21932 https://github.com/pandas-dev/pandas/issues/21255 I am posting this because in my use case I use the list to drop the columns, but i end up with column names that do not exist in the df <details> INSTALLED VERSIONS ------------------ commit: None python: 2.7.10.final.0 python-bits: 64 OS: Darwin OS-release: 17.6.0 machine: x86_64 processor: i386 byteorder: little LC_ALL: None LANG: None LOCALE: None.None pandas: 0.23.3 pytest: None pip: 10.0.1 setuptools: 39.2.0 Cython: None numpy: 1.13.3 scipy: 0.19.1 pyarrow: None xarray: None IPython: None sphinx: 1.5.5 patsy: 0.5.0 dateutil: 2.6.1 pytz: 2015.7 blosc: None bottleneck: None tables: None numexpr: None feather: None matplotlib: None openpyxl: None xlrd: 0.9.4 xlwt: None xlsxwriter: None lxml: None bs4: None html5lib: None sqlalchemy: None pymysql: None psycopg2: 2.7.3.2 (dt dec pq3 ext lo64) jinja2: 2.10 s3fs: None fastparquet: None pandas_gbq: None pandas_datareader: None </details>",does it break after 0.20.3)?,(edit by @TomAugspurger) Current output:  Expected Output is an empty series:  Not sure what the expected dtype of that Series should be... probably object. original post below: ---
pandas-dev/pandas,https://github.com/pandas-dev/pandas/issues/22306,pandas-dev_pandas_issues_22306,"Fatal Python error: GC object already tracked : Linux Centos

#### Code Sample  #### Problem description Checked on Windows 10 - Works fine with no issues .But when this python script was deployed on Linux Centos Gave me the 'Fatal Python error: GC object already tracked' Some more description - Thread 0x00007f4a5bfff700 (most recent call first): File ""/home/usr/anaconda3/lib/python3.6/multiprocessing/connection.py"", line 379 in _recv File ""/home/usr/anaconda3/lib/python3.6/multiprocessing/connection.py"", line 407 in _recv_bytes File ""/home/usr/anaconda3/lib/python3.6/multiprocessing/connection.py"", line 250 in recv File ""/home/usr/anaconda3/lib/python3.6/multiprocessing/pool.py"", line 463 in _handle_results File ""/home/usr/anaconda3/lib/python3.6/threading.py"", line 864 in run File ""/home/usr/anaconda3/lib/python3.6/threading.py"", line 916 in _bootstrap_inner File ""/home/usr/anaconda3/lib/python3.6/threading.py"", line 884 in _bootstrap Thread 0x00007f4a67fff700 (most recent call first): File ""/home/usr/anaconda3/lib/python3.6/threading.py"", line 295 in wait File ""/home/usr/anaconda3/lib/python3.6/queue.py"", line 164 in get File ""/home/usr/anaconda3/lib/python3.6/multiprocessing/pool.py"", line 415 in _handle_tasks File ""/home/usr/anaconda3/lib/python3.6/threading.py"", line 864 in run File ""/home/usr/anaconda3/lib/python3.6/threading.py"", line 916 in _bootstrap_inner File ""/home/usr/anaconda3/lib/python3.6/threading.py"", line 884 in _bootstrap Current thread 0x00007f4a7132a700 (most recent call first): File ""/home/usr/anaconda3/lib/python3.6/multiprocessing/pool.py"", line 405 in _handle_workers File ""/home/usr/anaconda3/lib/python3.6/threading.py"", line 864 in run File ""/home/usr/anaconda3/lib/python3.6/threading.py"", line 916 in _bootstrap_inner File ""/home/usr/anaconda3/lib/python3.6/threading.py"", line 884 in _bootstrap Thread 0x00007f4a8750e740 (most recent call first): File ""/home/usr/anaconda3/lib/python3.6/threading.py"", line 295 in wait File ""/home/usr/anaconda3/lib/python3.6/threading.py"", line 551 in wait File ""/home/usr/anaconda3/lib/python3.6/multiprocessing/pool.py"", line 635 in wait File ""/home/usr/anaconda3/lib/python3.6/multiprocessing/pool.py"", line 638 in get File ""/home/usr/anaconda3/lib/python3.6/multiprocessing/pool.py"", line 266 in map File ""test.py"", line 124 in writeframes File ""test.py"", line 147 in main File ""test.py"", line 151 in <module> Running on Pandas - 0.23.1",Could you provide a minimal reproducible example? http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports,"data,lookupdataset): in lookupdataset.iterrows: print(""something as data won't matter "" def writeframes(lookupdata"
pandas-dev/pandas,https://github.com/pandas-dev/pandas/issues/22487,pandas-dev_pandas_issues_22487,"DataFrame.gropuby().mean() incorrect result

Anybody knows why I'm having different results when I apply the same operator to the same DataFrame but using groupby? When using groupby , It returned negative values while all values are positive.  connections 3.689349e+18 dtype: float64  user A -1906546.0 Name: connections, dtype: float64 INSTALLED VERSIONS ------------------ commit: None python: 3.6.5.final.0 python-bits: 64 OS: Windows OS-release: 10 machine: AMD64 processor: Intel64 Family 6 Model 142 Stepping 9, GenuineIntel byteorder: little LC_ALL: None LANG: None LOCALE: None.None pandas: 0.23.0 pytest: 3.5.1 pip: 10.0.1 setuptools: 39.1.0 Cython: 0.28.2 numpy: 1.14.3 scipy: 1.1.0 pyarrow: None xarray: None IPython: 6.4.0 sphinx: 1.7.4 patsy: 0.5.0 dateutil: 2.7.3 pytz: 2018.4 blosc: None bottleneck: 1.2.1 tables: 3.4.3 numexpr: 2.6.5 feather: None matplotlib: 2.2.2 openpyxl: 2.5.3 xlrd: 1.1.0 xlwt: 1.3.0 xlsxwriter: 1.0.4 lxml: 4.2.1 bs4: 4.6.0 html5lib: 1.0.1 sqlalchemy: 1.2.7 pymysql: None psycopg2: None jinja2: 2.10 s3fs: None fastparquet: None pandas_gbq: None pandas_datareader: None",Can you try on master? Looks like an int overflow somewhere if still present investigation and PRs are always welcome,"Anybody knows why I'm having different results when I apply the same operator to the same DataFrame but using groupby? When using groupby , It returned negative values while all values are positive. "
pandas-dev/pandas,https://github.com/pandas-dev/pandas/issues/22669,pandas-dev_pandas_issues_22669,"Feature: Qcut when passed labels and duplicates='drop' should drop corresponding labels

#### Code Sample, a copy-pastable example if possible  #### Problem description When using this function with quantiles that return repeated bins, the function raises ""ValueError: Bin labels must be one fewer than the number of bin edges"". When using the optional parameter ""duplicates"" the only way to pass a valid ""labels"" parameters is checking for duplicate bins beforehand, repeating code in order to calculate the bins. #### Expected Output Pd.qcut should return the quantilizated column with the labels corresponding to the indices of the unique bins. E.g output of add_quantiles.:  #### Output of pd.show_versions() <details> INSTALLED VERSIONS ------------------ commit: None pandas: 0.22.0 pytest: 3.5.0 pip: 18.0 setuptools: 40.0.0 Cython: 0.28.1 numpy: 1.14.2 scipy: 1.0.0 pyarrow: None xarray: None IPython: 6.2.1 sphinx: 1.7.2 patsy: 0.5.0 dateutil: 2.7.2 pytz: 2018.3 blosc: None bottleneck: 1.2.1 tables: 3.4.2 numexpr: 2.6.4 feather: None matplotlib: 2.2.2 openpyxl: 2.5.1 xlrd: 1.1.0 xlwt: 1.2.0 xlsxwriter: 1.0.2 lxml: 4.2.1 bs4: 4.6.0 html5lib: 1.0.1 sqlalchemy: 1.2.5 pymysql: None psycopg2: None jinja2: 2.10 s3fs: None fastparquet: None pandas_gbq: None pandas_datareader: None </details>",Could you update your issue to show what output you're getting currently and what output you would expect? That would be very helpful. cc @jreback,import pandas as pd import numpy as np print(df) #numbers #0 0.0 #1 0.0 #2 0.0 #3 33.0 #4 81.0 #5 13.0 print() E.g output of add_quantiles.: numbers numbers_50q numbers_75q numbers_100q 0 0.0 1 0 0 1 0.0 1 0 0 2 0.0 1 0 0 3 33.0 0 0 1 4 81.0 0 0 1 5 13.0 0 1 0
pandas-dev/pandas,https://github.com/pandas-dev/pandas/issues/22714,pandas-dev_pandas_issues_22714,"Segfaults on Python 3.8 (CPython master)

Segfaults: - [ ] indexing/test_indexing.py::TestFancy::test_setitem_list (see #22718) - [ ] ujson.encode(np.array(1)) (see #22718) - [ ] Issue detailed as follows: #### Code Sample, a copy-pastable example if possible  #### Problem description The above code works fine on Python 3.7 producing the below output  The same code when it runs on CPython master causes segfault. I have opened an [upstream issue](https://bugs.python.org/issue34688) that contains the stack trace when running under GDB. I am not sure if it's due to Pandas or Numpy since it's present in traceback. Since the above works fine on Python 3.7 but segfaults on Python 3.8 I want to report it here along with opening an upstream bug. I found this while running pytest -x pandas/frame/test_apply.py::TestDataFrameAggregate::test_nuiscance_columns #### Expected Output  #### Output of pd.show_versions()  Thanks",Do you get it with just column D?,Segfaults: - [ ] indexing/test_indexing.py::TestFancy::test_setitem_list (see #22718) - [ ] ujson.encode(np.array(1)) (see #22718) - [ ] Issue detailed as follows:
pandas-dev/pandas,https://github.com/pandas-dev/pandas/issues/23139,pandas-dev_pandas_issues_23139,"DOC: Implicit parallelism and how to disable it

**Questions:** Apparently OpenMP (or BLAS?) is used in pandas internals (or one of its dependencies), at least in some cases. For example, value_counts() uses all CPU cores on my machine (but does not benefit from it). 1. Is this expected behavior? 2. Is there any documentation on which functions are parallelized, and how to disable it if desired? --- **Details:** Here's a test program that repeatedly calls value_counts() on a large array:  From monitoring htop I discovered that value_counts() is using multiple CPU cores. Apparently the parallelism can be controlled via OMP_NUM_THREADS, as shown here in the difference between real and user time when the number of threads is changed.  This was a surprise to me. I know that some operations in np.linalg are implicitly parallelized via the BLAS implementation, but I would not have guessed that value_counts() used any of those functions. (Am I mistaken?) This behavior is especially undesirable when one is already parallelizing one's code via multiprocessing (or, in my case, pyspark). In that case, the multiple threads in each process compete with each other, causing thrashing, etc. Is setting the OMP_NUM_THREADS variable the recommended way to disable this behavior? Thanks in advance for any tips! (Side note: As you can see, adding more threads apparently does not provide much benefit even in this simple case -- it just eats more CPU time without saving wall time.) ---- **Appendix** This issue can be reproduced by simply installing the latest version of pandas with conda, but FWIW, here are the versions I'm using:",Can you try in an environment with nomkl?,"This issue can be reproduced by simply installing the latest version of pandas with conda, but"
pandas-dev/pandas,https://github.com/pandas-dev/pandas/issues/23335,pandas-dev_pandas_issues_23335,"'closed' parameter is not working for Rolling with duplicate Timestamp

The 'closed' parameter is not really working when the date is duplicated. Here is an example.  | | Data| amount| |----|--------|--------| |0 |2016-06-30 |100| |1 |2016-06-30 |200| |2 |2016-08-09 |10|  Output | | Data| amount| |----|--------|--------| |0 |2016-06-30 |NaN| |1 |2016-06-30 |100| |2 |2016-08-09 |300| Expected output: | | Data| amount| |----|--------|--------| |0 |2016-06-30 |NaN| |1 |2016-06-30 |NaN | |2 |2016-08-09 |300| Output of pd.show_versions() <details> <summary>show_version</summary> INSTALLED VERSIONS ------------------ commit: None python: 3.7.0.final.0 python-bits: 64 OS: Darwin OS-release: 17.7.0 machine: x86_64 processor: i386 byteorder: little LC_ALL: en_US.UTF-8 LANG: en_US.UTF-8 LOCALE: en_US.UTF-8 pandas: 0.23.4 pytest: None pip: 18.1 setuptools: 40.4.3 Cython: None numpy: 1.15.2 scipy: 1.1.0 pyarrow: None xarray: None IPython: 7.0.1 sphinx: None patsy: None dateutil: 2.7.3 pytz: 2018.5 blosc: None bottleneck: None tables: None numexpr: None feather: None matplotlib: 3.0.0 openpyxl: None xlrd: None xlwt: None xlsxwriter: None lxml: None bs4: None html5lib: None sqlalchemy: None pymysql: None psycopg2: None jinja2: 2.10 s3fs: None fastparquet: None pandas_gbq: None pandas_datareader: None </details>","Could you update your example to be reproducible (steps to crate the dataframe, execute the problematic method)? http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports And include the output of show_versions in a details tag.",-06-30 |2.sum() -06-30 |NaN| |5 |201.0.1 sphinx: None patsy: None dateutil: 2.
pandas-dev/pandas,https://github.com/pandas-dev/pandas/issues/24014,pandas-dev_pandas_issues_24014,"Unnecessary bare except at class Block, function interpolate hides actual error

#### Code Sample, a copy-pastable example if possible  ##### Expected output  #### Problem description If interpolation parameter not specified, it raises an error, which states invalid method internals.py:1152:1155 try: m = missing.clean_interp_method(method, **kwargs) except: m = None If there is no such try/except block around the missing.clean_interp_method function call, we would get the proper exception from mising.py/clean_interp_method. Pandas version: 0.23.4",Do you have a minimal example? http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports,"[0,1,pd.np.nan,3,4]) df.interpolate(method= ##### Expected output "
pandas-dev/pandas,https://github.com/pandas-dev/pandas/issues/24226,pandas-dev_pandas_issues_24226,"Slightly unintuitive converstion between timestamp and datetime during indexing / slicing

#### Code Sample import pandas as pd import numpy as np df = pd.DataFrame(index=[0],columns=['c'],dtype='datetime64[ns]') df['c'].iloc[0] = np.datetime64('2018-08-01 00:00:00') df['c'][df.index==0].values[0] Out[83]: numpy.datetime64('2018-08-01T00:00:00.000000000') df['c'].iloc[0] Out[80]: Timestamp('2018-08-01 00:00:00') (Python 3.6, pandas 0.23.4) #### Problem description Just thought it is a bit unintuitive that the above conversion occurs? Regardless of it being somewhat bad practice not having chosen a native date format, I feel this conversion should maybe not take place? Not been around handling dates with python for long, so disregard if it doesn't make sense. Cost me a bit of time to find out what's going on though, as this greatly complicated plotting as well as calculations. (Accessing in slicing notation is much more convenient in my case.) Logic comparisons between the formats are not unproblematic either, I think. (Equality comparison between timestamp and dt46ns does not throw an error. In[82]: df['c'][df.index==0].values[0] == df['c'].iloc[0] Out[82]: False",Could you please post a copy/pasteable example? Your example is not reproducible http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports,08-0 df['c'][df.index==0].values[0] In[80]: df['c'].iloc[0] Out[80]: 08-0(Accessing in slicing notation is much more convenient in my case.)
pandas-dev/pandas,https://github.com/pandas-dev/pandas/issues/24251,pandas-dev_pandas_issues_24251,"Error at groupby using apply ValueError: No axis named 1 for object type <class 'pandas.core.series.Series'>

I was working to perform a groupby apply operation by using pandas. I used the following code:  #### Problem description I tested under Python 2.7 and Python 3.5 pandas version (0.17.1 and 0.18.1) but only on pandas 0.17.1 - Python 2.7 Pandas 0.17.1 Passed - Python 2.7 Pandas 0.18.1 Failed - Python 3.5 Pandas 0.17.1 Passed - Python 3.5 Pandas 0.18.1 Failed - Python 3.5 Pandas 0.23.4 Failed I was able to get a result. On Pandas >=0.18.1 I got this error: ValueError: No axis named 1 for object type <class 'pandas.core.series.Series'> input dataframe: ![image](https://user-images.githubusercontent.com/20059518/49887982-6fde5980-fe03-11e8-8653-cfdacd7848b9.png) I expect something like this: ![image](https://user-images.githubusercontent.com/20059518/49895910-6b23a080-fe17-11e8-88d3-b884d7d676e8.png)",Can you provide a *minimal* code example to reproduce the issue? There are way too many moving parts here to effectively isolate the issue,- Python 3.5 Pandas 0.23.4 Failed
pandas-dev/pandas,https://github.com/pandas-dev/pandas/issues/24839,pandas-dev_pandas_issues_24839,"ValueError: cannot set WRITEABLE flag to True of this array

will need to revert the xfail decorator in: https://github.com/pandas-dev/pandas/pull/25517 when this is fixed #### Code Sample, a copy-pastable example if possible Im getting all of a sudden this Error, any idea?  #### Problem description Traceback :",When did this code last work for you? What version are you using now (related to the first question)? cc @jreback,will need to revert the xfail decorator in: https://github.com/pandas-dev/pandas/pull/25517 when this is fixed
pandas-dev/pandas,https://github.com/pandas-dev/pandas/issues/28769,pandas-dev_pandas_issues_28769,"DataFrame.append with empty list raises IndexError

#### Code Sample  #### Problem description Crash when passing empty sequence to DataFrame.append #### Expected Output No crash. The source DataFrame is returned intact. #### Version Version 0.25.1. Happens in master. Problem line https://github.com/pandas-dev/pandas/blob/master/pandas/core/frame.py#L7014",What output are you hoping to see here? Still an empty DataFrame?,The source DataFrame is returned intact.
pandas-dev/pandas,https://github.com/pandas-dev/pandas/issues/25825,pandas-dev_pandas_issues_25825,"Plotting pd.Series object does not show year correctly

I am graphing the results of the measurements of a humidity sensor over time. I'm using Python 3.7.1 and Pandas 0.24.2. I have a list called dateTimeList with date and time strings:  I wrote this code where index is a DatetimeIndex object and humList is a list of floats.  And I have this two results, one with data from February[1] and the other one with data from March[2]. The problem is that in March instead of leaving the year 2019, sequences of 00 12 00 12 appear on the x axis. I think it is important to note that this only happens on the data of March, since February is ok, and the data of both months have the same structure. Day and Month are shown correctly on both plots. [1]: https://www.dropbox.com/s/ckv6zeqb0psh6ih/Hum_General_Febrero.png?dl=0 [2]: https://www.dropbox.com/s/nzvbwnuf5fkuim1/Hum_General_Marzo.png?dl=0 I also tried with:  Now index is a list of Timestamps objects. Same Results. Full Code, Data and Outputs: [CodeandData.zip](https://github.com/pandas-dev/pandas/files/2997619/CodeandData.zip)",Can you provide screenshots of what you are seeing along with pd.show_versions?,"Full Code, data and results: [CodeandData.zip](https://github.com/pandas-dev/pandas/files/2997619/CodeandData.zip)"
pandas-dev/pandas,https://github.com/pandas-dev/pandas/issues/26041,pandas-dev_pandas_issues_26041,"_setitem_with_indexer changes type from datetime to object

#### Code Sample, a copy-pastable example if possible  #### Problem description Hello all! I am facing a particularly weird issue, which I have pinpointed to be caused by the [_setitem_with_indexer method](https://github.com/pandas-dev/pandas/blob/6d9b702a661891724e6c10dd96eb149404205c67/pandas/core/indexing.py#L298) From the code above, you can see that when loc is used inside perform_operations_that_change_dtype to retrieve a column and change its timezone, the resulting series is of dtype object instead of datetime. This behavior is not present when using [] as in perform_operations_that_do_not_change_dtype, and is only present for DataFrames with single rows. I know that the two approaches are different, but if both loc and [] return the same series, why is the resulting assignment different? From what I can see something happens inside the _setitem_with_indexer method that changes the type. One thing that might be the issue is the _try_coerce_args method inside ObjectBlock: [link to line](https://github.com/pandas-dev/pandas/blob/6d9b702a661891724e6c10dd96eb149404205c67/pandas/core/internals/blocks.py#L2705). Why is other's type changed to object when it is a DatetimeArray? The only similar issue I managed to find is [this StackOverflow question](https://stackoverflow.com/questions/50359164/pandas-tz-convert-using-apply-returns-object-rather-than-datetime) that has 0 answers. Thank you for looking at this issue and for creating and maintaining this amazing library! Hope my findings would be enough to guide someone to help me resolve this; I've been hammering at it the whole day. #### Expected Output The dtype of datetime is not changed to object. #### Output of pd.show_versions() <details> INSTALLED VERSIONS ------------------ commit: None python: 3.7.2.final.0 python-bits: 64 OS: Windows OS-release: 10 machine: AMD64 processor: Intel64 Family 6 Model 79 Stepping 1, GenuineIntel byteorder: little LC_ALL: None LANG: None LOCALE: None.None pandas: 0.24.2 pytest: 4.3.0 pip: 19.0.3 setuptools: 40.8.0 Cython: None numpy: 1.16.0 scipy: None pyarrow: None xarray: None IPython: 7.2.0 sphinx: None patsy: None dateutil: 2.8.0 pytz: 2018.9 blosc: None bottleneck: None tables: None numexpr: None feather: None matplotlib: 3.0.3 openpyxl: None xlrd: 1.2.0 xlwt: None xlsxwriter: None lxml.etree: None bs4: None html5lib: None sqlalchemy: None pymysql: None psycopg2: None jinja2: 2.10 s3fs: None fastparquet: None pandas_gbq: None pandas_datareader: None gcsfs: None </details>",Do you mind simplifying your example to a minimal set of operations? http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports,One thing that might be the issue is the _try_coerce_args inside ObjectBlock: [link to line](https://github.com/pandas-dev/pandas/blob/6d9b702a661891724e6c10dd96eb149404205c67/pandas/core/internals/blocks.py#L2705). Why is other's type changed to object when it is a DatetimeArray?
pandas-dev/pandas,https://github.com/pandas-dev/pandas/issues/26498,pandas-dev_pandas_issues_26498,"rename doesn't work with mutliindex

#### Code Sample, a copy-pastable example if possible  Currently both df1 and df2 are:  #### Problem description I was expecting df.columns[1] to change to ('C', 3) but it remains ('A', 1) #### Expected Output",Can you post the expected output?,Currently both df1 and df2 are:  
pandas-dev/pandas,https://github.com/pandas-dev/pandas/issues/26539,pandas-dev_pandas_issues_26539,"Incompatiblle min/max function with Rolling

I have dataframe with 3 columns.The objective is to calculate rolling sum of n consecutive observations and minimum time of those n observations. I got ValueError in following code:  Error details:",Can you please make the code to reproduce copy/pastable?,":00','09:20:00', '09:16:00','09:19:00','09:21:00','09:28:00','09:16:00', '09:22:00','09:24:00','09:16:00'], 'value': [i for i in range(2,14)], 'date': ['2018-05-03']*12}) df.time = pd.to_datetime(df.time,format= '%H:%M:%S' ).dt.time df.date = pd.to_datetime(df.date) df.rolling(4],'time':[min138798> in <module> 7 df.time = pd.to_datetime(df.time,format= '%H:%M:%S' ).dt.time 8 df.date = pd.to_datetime(df.date) ----> 9 df.rolling(4 ~/anaconda3/lib/python3.7/site-packages/pandas/core/window.py in aggregate(self, arg, *args, **kwargs) 1683 @Appender(_shared_docs['aggregate']) 1684 def aggregate(self, arg, *args, **kwargs): -> 1685 return super(Rolling, self).aggregate(arg, *args, **kwargs) 1686 1687 agg = aggregate ~/anaconda3/lib/python3.7/site-packages/pandas/core/window.py in aggregate(self, arg, *args, **kwargs) 310 311 def aggregate(self, arg, *args, **kwargs): --> 312 result, how = self._aggregate(arg, *args, **kwargs) 313 if result is None: 314 return self.apply(arg, raw=False, args=args, kwargs=kwargs) ~/anaconda3/lib/python3.7/site-packages/pandas/core/base.py in _aggregate(self, arg, *args, **kwargs) 496 497 try: --> 498 result = _agg(arg, _agg_1dim) 499 except SpecificationError: 500 ~/anaconda3/lib/python3.7/site-packages/pandas/core/base.py in _agg(arg, func) 447 result = compat.OrderedDict() 448 for fname, agg_how in compat.iteritems(arg): --> 449 result[fname] = func(fname, agg_how) 450 return result 451 ~/anaconda3/lib/python3.7/site-packages/pandas/core/base.py in _agg_1dim(name, how, subset) 430 raise SpecificationError(""nested dictionary is ambiguous "" 431 ""in aggregation"") --> 432 return colg.aggregate(how, _level=(_level or 0) + 1) 433 434 def _agg_2dim(name, how): ~/anaconda3/lib/python3.7/site-packages/pandas/core/window.py in aggregate(self, arg, *args, **kwargs) 1683 @Appender(_shared_docs['aggregate']) 1684 def aggregate(self, arg, *args, **kwargs): -> 1685 return super(Rolling, self).aggregate(arg, *args, **kwargs) 1686 1687 agg = aggregate ~/anaconda3/lib/python3.7/site-packages/pandas/core/window.py in aggregate(self, arg, *args, **kwargs) 310 311 def aggregate(self, arg, *args, **kwargs): --> 312 result, how = self._aggregate(arg, *args, **kwargs) 313 if result is None: 314 return self.apply(arg, raw=False, args=args, kwargs=kwargs) ~/anaconda3/lib/python3.7/site-packages/pandas/core/base.py in _aggregate(self, arg, *args, **kwargs) 557 return self._aggregate_multiple_funcs(arg, 558 _level=_level, --> 559 _axis=_axis), None 560 else: 561 result = None ~/anaconda3/lib/python3.7/site-packages/pandas/core/base.py in _aggregate_multiple_funcs(self, arg, _level, _axis) 615"
pandas-dev/pandas,https://github.com/pandas-dev/pandas/issues/28155,pandas-dev_pandas_issues_28155,"Inconsistency with ints and floats

#### Problem description 1. pandas statistics sometimes produce numpy data types and sometimes not. This is inconsistent. 2. json_dumps cannot serialize int64 (not a pandas issue but that's how I got here) I created a dictionary containing pandas stats. The input is a pandas series, s, dtype float64. The methods used are  Apparently, pandas returns numpy ints and numpy floats (sometimes).  Note: This discrepancy matters because json.dump cannot currently serialize objects of type int64. Apparently pandas DataFrame.to_json() method has no such issues. Converting the dictionary to a dataframe and then writing it out to JSON works without error. Perhaps pandas could share the to_json code with the Python json library devs, resolving [Python issue 24313](https://bugs.python.org/issue24313) #### Output of pd.show_versions() <details> INSTALLED VERSIONS ------------------ commit : None python : 3.7.3.final.0 python-bits : 64 OS : Darwin OS-release : 15.6.0 machine : x86_64 processor : i386 byteorder : little LC_ALL : None LANG : en_US.UTF-8 LOCALE : en_US.UTF-8 pandas : 0.25.0 numpy : 1.16.4 pytz : 2019.1 dateutil : 2.8.0 pip : 19.1.1 setuptools : 41.0.1 Cython : 0.29.12 pytest : 5.0.1 hypothesis : None sphinx : 2.1.2 blosc : None feather : None xlsxwriter : 1.1.8 lxml.etree : 4.3.4 html5lib : 1.0.1 pymysql : 0.9.3 psycopg2 : None jinja2 : 2.10.1 IPython : 7.7.0 pandas_datareader: None bs4 : 4.7.1 bottleneck : 1.2.1 fastparquet : None gcsfs : None lxml.etree : 4.3.4 matplotlib : 3.1.0 numexpr : 2.6.9 odfpy : None openpyxl : 2.6.2 pandas_gbq : None pyarrow : None pytables : None s3fs : None scipy : 1.3.0 sqlalchemy : 1.3.5 tables : 3.5.2 xarray : None xlrd : 1.2.0 xlwt : 1.3.0 xlsxwriter : 1.1.8 </details>",Can you create a minimal example? http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports and fill out the issue template?,#### Problem description 1. pandas statistics sometimes produce numpy data types and sometimes not. This is inconsistent. 2. json_dumps cannot serialize int64 (not a pandas issue but that's how86.6 <class 'numpy.float64'> std 5.shttps://bugs.python.org/issue24313
pandas-dev/pandas,https://github.com/pandas-dev/pandas/issues/27002,pandas-dev_pandas_issues_27002,"Improve DataFrame.to_string()

Add the following options: - Draw borders: specify border characters and which ones to draw _(similar to [texttable](https://pypi.org/project/texttable/))_ - Allow multi-line cells for long strings instead of truncation (they are visually separated by the borders) - ~~Use [Markdown table format](https://help.github.com/en/articles/organizing-information-with-tables). This could also be implemented in a separate to_markdown() function.~~ _(Duplicate of #11052)_","Can you make your request more explicit? As is, it'll be hard to tell whether this issue is in scope, or when it is closable.",- Allow multi-line cells (they are visually separated by the borders)
pandas-dev/pandas,https://github.com/pandas-dev/pandas/issues/28249,pandas-dev_pandas_issues_28249,"Misbehavior for Dataframe updates on unexisting DateTime index

#### Code Sample, a copy-pastable example if possible  #### Problem description [this should explain **why** the current behaviour is a problem and why the expected output is a better solution.] According to Pandas documentation, the updates of a cell value based on index lookup (df.loc and df.at) should work correctly only when the index is existing within dataframe. The problem I encountered happens when I try to update some cells accessed by DateTime index, in case the index (which is actually a date) does not exist in the dataframe. According to the documentation, an exception should be raised in this case. What actually happens, is that without raising any exception: 1) Pandas transforms the DateTime index into an object index (thus making it unusable for timeseries processing), 2) insert new rows in the dataframe with the specified new object index and set all columns to Nan, except the updated one. I solved the above problem, wrapping the update commands in conditional 'If' rules but according to the documentation it seems to be a misbehavior of Pandas. #### Expected Output #### Output of pd.show_versions() <details> [paste the output of pd.show_versions() here below this line] INSTALLED VERSIONS ------------------ commit: None python: 3.6.8.final.0 python-bits: 64 OS: Linux OS-release: 4.15.0-60-generic machine: x86_64 processor: x86_64 byteorder: little LC_ALL: None LANG: en_US.UTF-8 LOCALE: en_US.UTF-8 pandas: 0.23.4 pytest: 4.5.0 pip: 19.1.1 setuptools: 41.0.1 Cython: 0.29.7 numpy: 1.17.0 scipy: 1.2.1 pyarrow: None xarray: None IPython: 7.5.0 sphinx: 2.0.1 patsy: 0.5.1 dateutil: 2.8.0 pytz: 2019.1 blosc: None bottleneck: 1.2.1 tables: 3.5.1 numexpr: 2.6.8 feather: None matplotlib: 3.1.0 openpyxl: 2.6.1 xlrd: 1.2.0 xlwt: 1.3.0 xlsxwriter: 1.1.8 lxml: 4.3.0 bs4: 4.7.1 html5lib: 0.9999999 sqlalchemy: 1.3.3 pymysql: None psycopg2: None jinja2: 2.10 s3fs: None fastparquet: None pandas_gbq: None pandas_datareader: 0.7.0 </details>","Can you provide a minimal example, and delete the issue template stuff from your post? http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports","testDataset.head(3),testDataset.index[0:3] ( T RollingMean 2017-01-02 -0.062500 21.0 2017-01-03 -0.333333 9.0 2017-01-04 -1.145833 25.0, DatetimeIndex(['2017-01-02', '2017-01-03', '2017-01-04'], dtype='datetime64[ns]', freq='D'))  # The initial dataframe **index is of type Datetime with frequency= D** # The index date '**2017-01-03'** does exists and will be deleted in the subsequent step. # To exemplify, I drop row with the above index:  #After dropping a row, the new index is: testDataset.index[0:3] DatetimeIndex(['2017-01-02', '2017-01-04', '2017-01-05'], dtype='datetime64[ns]', freq=None)  Try to add a new record with index: **2017-01-03**: testDataset.loc['2017-01-03', 'RollingMean']=10 print(testDataset.head(3)) T RollingMean 2017-01-02 00:00:00 -0.062500 28.0 2017-01-04 00:00:00 -1.145833 20.0 2017-01-05 00:00:00 1.645833 17.0 testDataset.tail(1), testDataset.index[0:3] ( T RollingMean 2017-01-03 NaN 10.0, Index([2017-01-02 00:00:00, 2017-01-04 00:00:00, 2017-01-05 00:00:00], dtype='object')) # It can be observed that although there is no error message, the index type is lost, and the new row is appended at the end of the dataframe  INSTALLED VERSIONS ------------------ commit: None python: 3.6.8.final.0 python-bits: 64 OS: Linux OS-release: 4.15.0-60-generic machine: x86_64 processor: x86_64 byteorder: little LC_ALL: None LANG: en_US.UTF-8 LOCALE: en_US.UTF-8 pandas: 0.23.4 pytest: 4.5.0 pip: 19.1.1 setuptools: 41.0.1 Cython: 0.29.7 numpy: 1.17.0 scipy: 1.2.1 pyarrow: None xarray: None IPython: 7.5.0 sphinx: 2.0.1 patsy: 0.5.1 dateutil: 2.8.0 pytz: 2019.1 blosc: None bottleneck: 1.2.1 tables: 3.5.1 numexpr: 2.6.8 feather: None matplotlib: 3.1.0 openpyxl: 2.6.1 xlrd: 1.2.0 xlwt: 1.3.0 xlsxwriter: 1.1.8 lxml: 4.3.0 bs4: 4.7.1 html5lib: 0.9999999 sqlalchemy: 1.3.3 pymysql: None psycopg2: None jinja2: 2.10 s3fs: None fastparquet: None pandas_gbq: None pandas_datareader: 0.7.0"
pandas-dev/pandas,https://github.com/pandas-dev/pandas/issues/28266,pandas-dev_pandas_issues_28266,"set timeoffset as the window parameter doesn't work for rolling corr function

#### Code Sample, a copy-pastable example if possible  #### Problem description Due to some conflicts I'm not able to test it on pandas 0.25, so I'm not sure whether the problem is solved. You may find that the last coorelation value 0.198680 is exactly the same as perform corr() on the total dataframe. No matter how I change the timeoffset, namely the window parameter, the rolling(window='timeoffset string').corr() method returns cummulative correlation instead of correlation inside the window. The reason why the expected output contains so many 'ones' is that rows which not fall in the time window should not be included into the calculation. For example, the 'ones' at the last row is calculated by the fourth row and the fifth row(the last row), because the indices is between 2013-01-03 09:00:06 and 2013-01-06 09:00:06 (the window parameter is 3d) and the correlation of these two point pairs is 1. #### Expected Output  #### Output of pd.show_versions() <details> ------------------ commit: None python: 3.7.3.final.0 python-bits: 64 OS: Windows OS-release: 10 machine: AMD64 processor: Intel64 Family 6 Model 142 Stepping 10, GenuineIntel byteorder: little LC_ALL: None LANG: zh_CN LOCALE: None.None pandas: 0.24.2 pytest: 4.3.1 pip: 19.0.3 setuptools: 40.8.0 Cython: 0.29.6 numpy: 1.16.2 scipy: 1.2.1 pyarrow: None xarray: None IPython: 7.4.0 sphinx: 1.8.5 patsy: 0.5.1 dateutil: 2.8.0 pytz: 2018.9 blosc: None bottleneck: 1.2.1 tables: 3.5.1 numexpr: 2.6.9 feather: None matplotlib: 3.0.3 openpyxl: 2.6.1 xlrd: 1.2.0 xlwt: 1.3.0 xlsxwriter: 1.1.5 lxml.etree: 4.3.2 bs4: 4.7.1 html5lib: 1.0.1 sqlalchemy: 1.3.1 pymysql: None psycopg2: None jinja2: 2.10 s3fs: None fastparquet: None pandas_gbq: None pandas_datareader: None gcsfs: None </details>",What's the expected output here?,"The reason why the expected output contains so many 'ones' is that rows which not fall in the time window should not be included into the calculation. For example, the 'ones' at the last row is calculated by the fourth row and the fifth row(the last row), because the indices is between 2013-01-03 09:00:06 and 2013-01-06 09:00:06 (the window parameter is 3d) and the correlation of these two point pairs is 1. "
pandas-dev/pandas,https://github.com/pandas-dev/pandas/issues/28483,pandas-dev_pandas_issues_28483,"Unexpected results on groupby([]).sum()

#### Code Sample:  #### Problem description [Hi, I think there's a problem dropping/excluding data points with groupby.sum function. I've performed the following code (see above), which at hindsight seemed ok until I compared with the same data using Excel and/or simple plot of the dataset. In addition, removing 'Count' will throw off values on other df columns. Thanks for checking this out.] #### Expected Output Year | 2012 State | **Alabama** Regulator | **SEC** Insurance/Annuity Products | 2 Stocks | 4 Year | 2012 State | **Alabama** Regulator | **FDIC** Debit Card | 1 Residential Mortgage | 3 #### Output of pd.df Year | 2012 State | **Alabama** Regulator | **FDIC** Debit Card | 1 Residential Mortgage | 1",Can you provide a minimal example? http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports You shouldn't need to link out to other sources / read from excel. Just the minimal code needed to reproduce the results.,Thanks for checking this out.
pandas-dev/pandas,https://github.com/pandas-dev/pandas/issues/32409,pandas-dev_pandas_issues_32409,"pandas1.0.1 has trouble with certain column names

#### Code Sample, a copy-pastable example if possible  #### Problem description Pandas1.0.1 with python3.7 hits the above for json containing dicts containing the key ""last_status_change_at"" and possibly others, while pandas0.25.3 with python3.7 does not. I haven't checked yet with python3.6. #### Expected Output no unhashableness #### Output of pd.show_versions() <details> INSTALLED VERSIONS (pandas 1.0.1 which hits error) ------------------ commit : None python : 3.7.6.final.0 python-bits : 64 OS : Darwin OS-release : 18.7.0 machine : x86_64 processor : i386 byteorder : little LC_ALL : en_US.UTF-8 LANG : en_US.UTF-8 LOCALE : en_US.UTF-8 pandas : 1.0.1 numpy : 1.18.1 pytz : 2019.3 dateutil : 2.8.1 pip : 19.3.1 setuptools : 42.0.2 Cython : None pytest : None hypothesis : None sphinx : None blosc : None feather : None xlsxwriter : None lxml.etree : None html5lib : None pymysql : None psycopg2 : None jinja2 : None IPython : 7.12.0 pandas_datareader: None bs4 : None bottleneck : None fastparquet : None gcsfs : None lxml.etree : None matplotlib : 3.1.3 numexpr : None odfpy : None openpyxl : None pandas_gbq : None pyarrow : 0.16.0 pytables : None pytest : None pyxlsb : None s3fs : None scipy : 1.4.1 sqlalchemy : None tables : None tabulate : None xarray : None xlrd : None xlwt : None xlsxwriter : None numba : None INSTALLED VERSIONS (pandas0.25.3 which doesnt hit error) ------------------ commit : None python : 3.7.6.final.0 python-bits : 64 OS : Darwin OS-release : 18.7.0 machine : x86_64 processor : i386 byteorder : little LC_ALL : en_US.UTF-8 LANG : en_US.UTF-8 LOCALE : en_US.UTF-8 pandas : 0.25.3 numpy : 1.18.1 pytz : 2019.3 dateutil : 2.8.1 pip : 19.3.1 setuptools : 42.0.2 Cython : None pytest : None hypothesis : None sphinx : None blosc : None feather : None xlsxwriter : None lxml.etree : None html5lib : None pymysql : None psycopg2 : None jinja2 : None IPython : 7.12.0 pandas_datareader: None bs4 : None bottleneck : None fastparquet : None gcsfs : None lxml.etree : None matplotlib : 3.1.3 numexpr : None odfpy : None openpyxl : None pandas_gbq : None pyarrow : 0.16.0 pytables : None s3fs : None scipy : 1.4.1 sqlalchemy : None tables : None xarray : None xlrd : None xlwt : None xlsxwriter : None </details>",Do you have a reproducible example? http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports,"containing the key ""last_status_change_at"""
pandas-dev/pandas,https://github.com/pandas-dev/pandas/issues/33894,pandas-dev_pandas_issues_33894,"ENH: Construct pandas dataframe from function

It would be great if we could construct a pd.DataFrame from a function. #### Describe the solution you'd like  The output would be equivalent to  Alternatively, we could do the same with something like  args and kwargs are passed to the df constructor. ## EDIT I just realized I can easily obtain what I want with  It's quite compact and easy to read. It's slowish but I don't expect it to be used in high performance tasks.",Could you please provide some more details?,"5)) python col_len = 1 index_len = 15 data = np.array([[my_random() for i in range(col_len)] for j in range(index_len)]) df = pd.DataFrame(data, index=range(index_len), columns=range(columns_len"
pandas-dev/pandas,https://github.com/pandas-dev/pandas/issues/28969,pandas-dev_pandas_issues_28969,"hash_pandas_object fails on tuple

#### Code Sample  <details><summary>Traceback</summary> <p>  </p> </details> #### Problem description Tuples are immutable and hash should work correct? #### Expected Output Hashed dataframe elements. #### Output of pd.show_versions() <details> INSTALLED VERSIONS ------------------ commit : None python : 3.7.3.final.0 python-bits : 64 OS : Linux OS-release : 5.0.0-1018-azure machine : x86_64 processor : x86_64 byteorder : little LC_ALL : None LANG : C.UTF-8 LOCALE : en_US.UTF-8 pandas : 0.25.1 numpy : 1.17.2 pytz : 2019.3 dateutil : 2.8.0 pip : 19.2.3 setuptools : 41.4.0 Cython : None pytest : 5.2.1 hypothesis : None sphinx : None blosc : None feather : None xlsxwriter : None lxml.etree : None html5lib : None pymysql : None psycopg2 : None jinja2 : 2.10.3 IPython : 7.8.0 pandas_datareader: None bs4 : None bottleneck : None fastparquet : None gcsfs : None lxml.etree : None matplotlib : 3.1.1 numexpr : None odfpy : None openpyxl : 3.0.0 pandas_gbq : None pyarrow : 0.15.0 pytables : None s3fs : None scipy : 1.3.1 sqlalchemy : 1.3.10 tables : None xarray : None xlrd : 1.2.0 xlwt : None xlsxwriter : None </details>",Can you include the trace back in the bug report? In particular we need to isolate whether this is a bug in pandas or numpy,<details><summary>Traceback</summary> <p>  </p> </details>
pandas-dev/pandas,https://github.com/pandas-dev/pandas/issues/29316,pandas-dev_pandas_issues_29316,"iloc indexing ~10 times slower than direct column indexing +no documention of it

#### Code Sample, a copy-pastable example if possible Updated without yfinance as it was a lousy choice of example - requires only standard numpy and datetime. A DateTimeIndex exacerbates the time slowness.  ~~Old code example:~~ ~~#requires pip install yfinance for this example - could substitute with any data set~~ ~~timeit.timeit(stmt='for x in range(len(hist)): _ = hist.iloc[x].Close', setup='import yfinance as yf; hist = yf.Ticker(""MSFT"").history(period=""max"")', number=3)~~ ~~#5.178573199999846~~ ~~timeit.timeit(stmt='for x in range(len(hist)): _ = hist.iloc[x].Close', setup='import yfinance as yf; hist = yf.Ticker(""MSFT"").history(period=""max"")', number=3)~~ ~~#0.5886300999998184~~ #### Problem description One would naturally expect iloc to have close to identical efficiency as column indexing. This is a dramatic blunder in a lot of code more than likely if it is not well documented and known. I would expect some strange inefficiency is present as logically it could be slightly slower to return a row rather than a data point. But 10 times slower is so dramatic that this function should be avoided. #### Expected Output Less than double the speed of single column value access. Or at least thorough documentation of this limitation. #### Output of pd.show_versions() <details> INSTALLED VERSIONS ------------------ commit : None python : 3.7.4.final.0 python-bits : 64 OS : Windows OS-release : 10 machine : AMD64 processor : Intel64 Family 6 Model 60 Stepping 3, GenuineIntel byteorder : little LC_ALL : None LANG : en LOCALE : None.None pandas : 0.25.1 numpy : 1.16.5 pytz : 2019.3 dateutil : 2.8.0 pip : 19.2.3 setuptools : 41.4.0 Cython : 0.29.13 pytest : 5.2.1 hypothesis : None sphinx : 2.2.0 blosc : None feather : None xlsxwriter : 1.2.1 lxml.etree : 4.4.1 html5lib : 1.0.1 pymysql : None psycopg2 : None jinja2 : 2.10.3 IPython : 7.8.0 pandas_datareader: None bs4 : 4.8.0 bottleneck : 1.2.1 fastparquet : None gcsfs : None lxml.etree : 4.4.1 matplotlib : 3.1.1 numexpr : 2.7.0 odfpy : None openpyxl : 3.0.0 pandas_gbq : None pyarrow : None pytables : None s3fs : None scipy : 1.3.1 sqlalchemy : 1.3.9 tables : 3.5.2 xarray : None xlrd : 1.2.0 xlwt : 1.3.0 xlsxwriter : 1.2.1 </details>",what does this have to do with pandas? what is yfinance? show a self contained example,"Updated without yfinance as it was a lousy choice of example - requires only standard numpy and datetime. A DateTimeIndex exacerbates the time slowness. timeit.timeit(stmt='for x in range(len(hist)): _ = hist.iloc[x].Col1', setup='import numpy as np; import pandas as pd; col = np.random.uniform(low=-1, high=1, size=(10000)); hist = pd.DataFrame({""Col1"":col,""Col2"":col,""Col3"":col,""Col4"":col,""Col5"":col})', number=3) #3.553613900000073 timeit.timeit(stmt='for x in range(len(hist)): _ = hist.Col1[x]', setup='import numpy as np; import pandas as pd; col = np.random.uniform(low=-1, high=1, size=(10000)); hist = pd.DataFrame({""Col1"":col,""Col2"":col,""Col3"":col,""Col4"":col,""Col5"":col})', number=3) #0.5215671999999358 timeit.timeit(stmt='for x in range(len(hist)): _ = hist.iloc[x].Col1', setup='import datetime; import numpy as np; import pandas as pd; col = np.random.uniform(low=-1, high=1, size=(10000)); hist = pd.DataFrame({""Col1"":col,""Col2"":col,""Col3"":col,""Col4"":col,""Col5"":col}, [datetime.datetime.now() + datetime.timedelta(seconds=x) for x in range(10000)])', number=3) #4.485623600000508 timeit.timeit(stmt='for x in range(len(hist)): _ = hist.Col1[x]', setup='import datetime; import numpy as np; import pandas as pd; col = np.random.uniform(low=-1, high=1, size=(10000)); hist = pd.DataFrame({""Col1"":col,""Col2"":col,""Col3"":col,""Col4"":col,""Col5"":col}, [datetime.datetime.now() + datetime.timedelta(seconds=x) for x in range(10000)])', number=3) #0.7185356999998476 ~~ ~~"
pandas-dev/pandas,https://github.com/pandas-dev/pandas/issues/31805,pandas-dev_pandas_issues_31805,"df.loc[0] = row, datatype changes

#### Code Sample, a copy-pastable example if possible  My problem is, after running  ,the datatypes change #### Output of pd.show_versions() <details> INSTALLED VERSIONS ------------------ commit: None python: 3.7.3.final.0 python-bits: 64 OS: Windows OS-release: 10 machine: AMD64 processor: Intel64 Family 6 Model 142 Stepping 9, GenuineIntel byteorder: little LC_ALL: None LANG: None LOCALE: None.None pandas: 0.23.4 pytest: 3.8.0 pip: 19.1.1 setuptools: 41.0.1 Cython: 0.28.5 numpy: 1.15.4 scipy: 1.1.0 pyarrow: None xarray: None IPython: 6.5.0 sphinx: 1.7.9 patsy: 0.5.0 dateutil: 2.7.3 pytz: 2018.5 blosc: None bottleneck: 1.2.1 tables: 3.4.4 numexpr: 2.6.8 feather: None matplotlib: 2.2.3 openpyxl: 2.5.6 xlrd: 1.1.0 xlwt: 1.3.0 xlsxwriter: 1.1.0 lxml: 4.2.5 bs4: 4.6.3 html5lib: 1.0.1 sqlalchemy: 1.2.11 pymysql: None psycopg2: None jinja2: 2.10 s3fs: None fastparquet: None pandas_gbq: None pandas_datareader: None </details>",what would you expect the result to be? i guess you'd want it to stay object-dtype?,"My problem is, after running  ,the datatypes change"
pandas-dev/pandas,https://github.com/pandas-dev/pandas/issues/31954,pandas-dev_pandas_issues_31954,"BUG: ""cannot reindex from duplicate axis"" thrown using unique indexes, duplicated column names and a specific numpy array values

#### Code Sample  #### Problem description It has a bug that combines numpy specific values and duplicated DataFrame column names when it's used a select operation, such as df[df > 5]. A exception is thrown saying ""cannot reindex from duplicate axis"", however It should not be, because: - The DataFrame has no duplicated indexes ( df.index.is_unique is True) - The DataFrame has duplicated column names, but should not be a problem when we apply the selection operation, such as df_new[df_new > 5] - The DataFrame uses float or int numpy values, so it should not change the behavior of the code **However** the values in the numpy array DO changes the behavior of the DataFrame selection, if the DataFrame has duplicated column names. #### Expected Output  #### Current Output  #### Output of pd.show_versions() <details> INSTALLED VERSIONS ------------------ commit : None python : 3.6.9.final.0 python-bits : 64 OS : Linux OS-release : 5.3.0-28-generic machine : x86_64 processor : x86_64 byteorder : little LC_ALL : None LANG : en_US.UTF-8 LOCALE : pt_BR.UTF-8 pandas : 1.0.1 numpy : 1.18.1 pytz : 2019.3 dateutil : 2.8.1 pip : 20.0.2 setuptools : 45.2.0 Cython : None pytest : None hypothesis : None sphinx : 2.3.1 blosc : None feather : None xlsxwriter : None lxml.etree : None html5lib : None pymysql : None psycopg2 : None jinja2 : 2.11.1 IPython : 7.12.0 pandas_datareader: None bs4 : None bottleneck : None fastparquet : None gcsfs : None lxml.etree : None matplotlib : 3.1.3 numexpr : 2.7.1 odfpy : None openpyxl : None pandas_gbq : None pyarrow : None pytables : None pytest : None pyxlsb : None s3fs : None scipy : 1.4.1 sqlalchemy : None tables : 3.6.1 tabulate : None xarray : None xlrd : None xlwt : None xlsxwriter : None numba : None </details>","Do you still get the error if you only consider its head, or if you only use (say) its first 5 columns?",",2],[3,4]]) # DO NOT WORKS b = np.array([[0.,6],[7,8]]) # b = np.array([[.5,6],[7,8]]) # The same problem # This one works fine: # b = np.array([[5,6],[7,8]]) dfA = pandas.DataFrame(a) # This works fine EVEN using .5, because the columns name is different # dfA = pandas.DataFrame(a, columns=['a','b']) dfB = pandas.DataFrame(b) df_new = pandas.concat([dfA, dfB], axis = 1) print(df_new[df_new > 5])  ValueError: cannot reindex from a duplicate axis"
pandas-dev/pandas,https://github.com/pandas-dev/pandas/issues/32072,pandas-dev_pandas_issues_32072,"Feature Request: ""Skiprows"" by a condition or set of conditions

#### Code Sample, a copy-pastable example if possible  #### Problem description Pandas read methods currently support skipping rows by index with the parameter skiprows. It'd be a good feature if usage of skiprows is extended in way that it can take conditional statements just like many pandas objects. I anticipate that this feature may not be useful to all, it is certain that it will ease the people's pain who are dealing with many (large) files. Memory usage would drastically be downed in some situations I think. I am opening this issue with hoping a welcome on dev side, not opening this because it first glanced as a fancy request, but because I think it may affect many people's work in a good way. It may be later used as a schema reference as well. Similar to schema validation of a data set. **NOTE:** I am uncertain how can this be achieved or even if it can be done at all. It may not apply to pandas module. Consider this also as a brain storming. #### Expected Output This requires passing column names & dtype (schema) before reading the file, but I am not sure how to convey the mask for skiprows it should be similar to pandas.DataFrame condition & masks, but we may only pass column names since there is no pandas.DataFrame object.  #### Output of pd.show_versions() <details> INSTALLED VERSIONS ------------------ commit : None python : 3.7.1.final.0 python-bits : 64 OS : Darwin OS-release : 18.7.0 machine : x86_64 processor : i386 byteorder : little LC_ALL : None LANG : en_US.UTF-8 LOCALE : en_US.UTF-8 pandas : 0.25.3 numpy : 1.18.0 pytz : 2019.2 dateutil : 2.8.1 pip : 20.0.2 setuptools : 45.2.0 Cython : 0.29.12 pytest : 5.3.4 hypothesis : None sphinx : None blosc : None feather : None xlsxwriter : 1.2.0 lxml.etree : 4.3.4 html5lib : None pymysql : 0.9.3 psycopg2 : None jinja2 : 2.10.1 IPython : 7.6.1 pandas_datareader: None bs4 : 4.8.0 bottleneck : None fastparquet : None gcsfs : None lxml.etree : 4.3.4 matplotlib : 3.1.1 numexpr : 2.6.9 odfpy : None openpyxl : 2.6.2 pandas_gbq : None pyarrow : None pytables : None s3fs : 0.4.0 scipy : 1.3.1 sqlalchemy : None tables : 3.5.2 xarray : None xlrd : 1.2.0 xlwt : None xlsxwriter : 1.2.0 </details>","What do @WillAyd and @gfyoung think? @devrimcavusoglu read_parquet has some support for this with the ""filters"" argument.","#### Code Sample, a copy-pastable example if possible  n 0"
pandas-dev/pandas,https://github.com/pandas-dev/pandas/issues/33518,pandas-dev_pandas_issues_33518,"BUG: Illegal instruction: 4 on mid2010 Mac

#### Code Sample, a copy-pastable example  #### Problem description With a mid 2010 Mac, OSX 10.13.6 (High Sierra)  import pandas as pd, crashes with Illegal instruction: 4 Old Mac doesn't support VFX, as it is not displayed in sysctl -a | grep machdep.cpu.features output. This problem is also mentioned here : https://github.com/conda/conda/issues/9678 https://github.com/das-developers/condaCDF/issues/1 and as it was detected when trying to run inside Anaconda/jupyter, the work in progress to solve is described here : https://discourse.jupyter.org/t/osx-10-13-6-kernelrestarter-restarting-kernel/3965/8",Do you see the same issue with just import numpy?,"Old Mac doesn't support VFX, as it is not displayed in sysctl -a | grep machdep.cpu.features output."
pandas-dev/pandas,https://github.com/pandas-dev/pandas/issues/33672,pandas-dev_pandas_issues_33672,"BUG: Assertion error : Gaps in blk ref_locs in pandas profiling

Hi Everyone, I think i am facing a similar kind of issue. I have created a django utility that is using pandas profiling. The steps that i am doing is python are- 1-Uploading a file df = pd.read_csv(path) 2- Changing the datatypes of columns from objects to category df.select_dtypes(['object']).apply(pd.Series.astype, dtype='category')], axis=1).reindex(df.columns, axis=1) 3- Separating continuous and categorical variables from the dataframe and sending this information to multiselect dropdown in HTML 4- As per user selection from continuous and categorical variables dropdown, storing the selections in form of list. context['selectedContinousValues'] = request.POST.getlist('continous') context['selectedCategoricalValues'] = request.POST.getlist('categorical') 5- Combining both list and keeping the list items from df. Profiling_variables=context['selectedContinousValues']+context['selectedCategoricalValues'] print(Profiling_variables) profile=df[Profiling_variables] 6- Using this selected df in pandas profiling profile_final = ProfileReport(profile) Error: Error is coming when profile_final is having all categorical variables. I am using pandas profiling version 2.6. Please note that error is not hitting in following cases: 1- Selecting categorical variables only. 2- if we are not changing the datatypes of columns from objects to category. I have added this steps because i wanted to include a functionality where user could convert selected continuous variable to categorical variables. i tried to change them from continuous to object but as soon as this object variable(say eg: variable having values like 1,2,3,4,1,2,3,4,1,2,3,4 where i want to treat this variable as object) is being used by pandas profiling, it is considering this object variable as numeric in output. but i want to treat it as categories. Request Method: | POST -- | -- http://localhost:8000/selected-data/ | 3.0.5 | AssertionError Gaps in blk ref_locs | C:\Users\myname\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\managers.py in _rebuild_blknos_and_blklocs, line 231 | C:\Users\myname\AppData\Local\Continuum\anaconda3\python.exe | 3.7.4 | ['C:\Users\myname\Desktop\EDA Pandas Profiling V4\file-upload', 'C:\Users\myname\AppData\Local\Continuum\anaconda3\python37.zip', 'C:\Users\myname\AppData\Local\Continuum\anaconda3\DLLs', 'C:\Users\myname\AppData\Local\Continuum\anaconda3\lib', 'C:\Users\myname\AppData\Local\Continuum\anaconda3', 'C:\Users\myname\AppData\Local\Continuum\anaconda3\lib\site-packages', 'C:\Users\myname\AppData\Local\Continuum\anaconda3\lib\site-packages\django_crispy_forms-1.9.0-py3.7.egg', 'C:\Users\myname\AppData\Local\Continuum\anaconda3\lib\site-packages\win32', 'C:\Users\myname\AppData\Local\Continuum\anaconda3\lib\site-packages\win32\lib', 'C:\Users\myname\AppData\Local\Continuum\anaconda3\lib\site-packages\Pythonwin', 'C:\Users\myname\AppData\Local\Continuum\anaconda3\lib\site-packages\IPython\extensions']",Can you provide a minimal example? http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports,"myname\AppData\Local\Continuum\anaconda3\lib', 'C:\Users\myname\AppData\Local\Continuum\anaconda3', 'C:\Users\myname\AppData\Local\Continuum\anaconda3\lib\site-packages', 'C:\Users\myname\AppData\Local\Continuum\anaconda3\lib\site-packages\django_crispy_forms-1.9.0-py3.7.egg', 'C:\Users\\site-packages\win32\Pythonwin"
symfony/symfony,https://github.com/symfony/symfony/issues/28823,symfony_symfony_issues_28823,"Modifying bootstrap 4 label for checkbox removes the actual input element

**Symfony version(s) affected**: 4.1.4 **Description** If you try to override *only a label* of a checkbox, the input element disappears. The custom label is shown once if using ‘form_widget’ and twice if using ‘form_row’, the actual checkbox/input never appears, just the label. **How to reproduce** Use in a twig view:  Render via {{ form_widget(form.termsAccepted) }} - shows the label once, with an indent (form-check div wrap) Render via {{ form_row(form.termsAccepted) }} - shows the label twice, one without an indent and second with an indent same as with form_widget **Possible Solution** <!--- Optional: only if you have suggestions on a fix/reason for the bug --> **Additional context** <!-- Optional: any other context about the problem: log messages, screenshots, etc. -->",Can you check if #26540 is the same?,The label is shown once if using ‘form_widget’ and twice if using ‘form_row’
symfony/symfony,https://github.com/symfony/symfony/issues/29203,symfony_symfony_issues_29203,"[RFC] Simplify wiring of service locators

<!-- Allow service locator services to be autowired or made simpler to inject tagged services --> @nicolas-grekas on [Twitter](https://twitter.com/nicolasgrekas/status/1061724799536414720) asked me to create RFC. And when Symfony core member makes a call, you answer that call :smile: Is it possible to have autowiring for Service Locator? Example:  Because components need custom name instead of FQCN, programmer has to implement interface (otherwise, it works as usual FQCN as name):  so when I make class that implements TagCollectionInterface:  $factories is populated as  instead of  Right now I have to write a compiler pass: ![image](https://user-images.githubusercontent.com/1964158/48431873-1981ea80-e773-11e8-84d0-7248158990b7.png) It is pretty messy code, but the thing is that I make array of components where key is read from ComponentInterface::getName() static method. The rest (like $map variable) is irrelevant to Symfony. Anyway, tell me what you think. This idea will be scraped, I have better idea now (using annotations on controller + kernel.view event), define route tree just like NG and it will be good to go. It will not change the speed but will clean the code.",What kind of feedback are you asking for in this RFC?,"Is it possible to have autowiring for Anyway, tell me what you think. This idea will be scraped,"
symfony/symfony,https://github.com/symfony/symfony/issues/31727,symfony_symfony_issues_31727,"Error: Method ReflectionMethod::__toString() must not throw an exception, caught ErrorException: Warning: Use of undefined constant STDERR - assumed 'STDERR' (this will throw an Error in a future version of PHP)

**Symfony version(s) affected**: 4.2.8, 4.3 ## Description I have a controller calling an action and showing some HTML using Twig. When changing something in src i get 2 errors, each after a reload, before the site works again. ## How to reproduce **Steps to reproduce:** * i change something in my src folder and reload the page * it shows me the first error, as pictured in the *Additional Context* section * After a reload, it shows me the second error, as pictured in the *Additional Context* section * After the third reload the site works fine **Further notes about my setup:** * no changes in Kernel.php or public/index.php * files are minimal, no further external references i can think of * i used major files from an old Symfony 4.0 project, maybe i forgot important adaptions? * PHP 7.2.12, with xdebug **Controller**  **update.html.twig Template**  **services.yaml**  **composer.json**:  ## Possible Solution / Workaround When i add the following code:  **before** [ReflectionClassResource->generateSignature](https://github.com/symfony/symfony/blob/master/src/Symfony/Component/Config/Resource/ReflectionClassResource.php#L144) gets called, like in index.php, no error occurs! Idea from: https://stackoverflow.com/questions/17769041/notice-use-of-undefined-constant-stdout-assumed-stdout ## Additional context First time i get this error page: ![first-time](https://user-images.githubusercontent.com/381727/58623245-daaa2100-82cd-11e9-9604-debefe3f76fa.png) --- After the reload i get: ![second-time](https://user-images.githubusercontent.com/381727/58623273-e5fd4c80-82cd-11e9-8825-8423ab955f8d.png) --- Thanks for your time!",Could you create a small example application that allows us to reproduce this issue?,"* PHP 7.2.12, with xdebug"
symfony/symfony,https://github.com/symfony/symfony/issues/34857,symfony_symfony_issues_34857,"[httpFoundation] Request::getPreferredFormat() - Submit form, force to download render page with IE7 or IE 11 (with strategy GPO to emulate IE7)

**Symfony version(s) affected**: 4.4.1 php 7.2.25 or 7.2.9 **Description** If you try to submit form with bad value, like a empty value with the Assert\NotBlank when you submit the form, **only** on IE 11 (with strategy GPO to emulate IE7) or IE 7 the render page is forced to download. On chrome , EDGE, Firefox the render is correct with the message ""This value should not be blank."" **The same code and apache config with symfony 4.3.x work correctly** **How to reproduce** You can try https://github.com/thewalkingcoder/symf44.git Is a fresh symfony 4.4.1 --full install, with very very simple Entity, form and template. **Additional context** ex IE 11 ![image](https://user-images.githubusercontent.com/10476280/70325715-e8b79c00-1832-11ea-84e9-db644a39ff40.png) ex : chrome ![image](https://user-images.githubusercontent.com/10476280/70325790-1dc3ee80-1833-11ea-9f29-5d3f79ce9816.png) I try multiple tests **linux apache/php 7.2.25** with alias http://symf44.local chrome, firefox, edge -> ok IE 7 -> fail no alias like http://localhost/myapp/public chrome, firefox, edge -> ok IE 7 -> ok **symfony server** with alias http://symf44.local with reverse proxy chrome, firefox, edge -> ok IE 7 -> fail no alias like http://localhost:8080 chrome, firefox, edge -> ok IE 7 -> ok **windows xp with Uwamp beurk !! ** with alias chrome, firefox -> ok IE 7 -> fail no alias like http://localhost/myapp/public chrome, firefox -> ok IE 7 -> fail I try to change or remove .htaccess (provide to apache-pack) same result I try to change Constraint NotBlank annotation to php style or YAML same result",Can you please check if different headers are sent when the page is accessed using Internet Explorer?,(with strategy GPO to emulate IE7)
symfony/symfony,https://github.com/symfony/symfony/issues/28304,symfony_symfony_issues_28304,"Error ""It's a requirement to specify a Metadata Driver..."" after Upgrade to 4.1.4

**Symfony version(s) affected**: 4.1.4 **Description** After Upgrade from 4.1.3 to 4.1.4 i get the following error: It's a requirement to specify a Metadata Driver and pass it to Doctrine\ORM\Configuration::setMetadataDriverImpl(). **EDIT: if you still have this issue, running composer require symfony/proxy-manager-bridge should fix it.** <details> Trace:  To be updated:  Update:  My Doctrine Config:  </details>",Can one of you create a small example application that allows to reproduce?,"<details> </details> [EDIT: if you still have this issue, running composer require symfony/proxy-manager-bridge should fix it."
symfony/symfony,https://github.com/symfony/symfony/issues/30459,symfony_symfony_issues_30459,"Trait autwired as class

**Symfony version(s) affected**: 4.1.0 (other not tested) **Description** <!-- A clear and concise description of the problem. --> services configs  Some services which use traits, when autowiring and configuring used traits parsed as class.  When clearing cache ERROR  $data comes from trait method createResponse(string $message = '', array $data = []...). One more ERROR  clearing cache  Trait treated as class. **How to reproduce** <!-- Code and/or config needed to reproduce the problem. If it's a complex bug, create a ""bug reproducer"" as explained in: https://symfony.com/doc/current/contributing/code/reproducer.html --> **Possible Solution** <!--- Optional: only if you have suggestions on a fix/reason for the bug --> AccessDeniedHandler issue resolved by removing custom handler everywhere in security configs.  access_denied_handler: App\Infrastructure\Security\Authentication\Handler\AccessDeniedHandler  while traits autowired. Or next solution works for both. To resolve AdminController issue and above, Traits must be excluded from autowiring.  Also not using traits another one solutions :) **Additional context** <!-- Optional: any other context about the problem: log messages, screenshots, etc. --> I tried to remove cache folder.",Can you please check if this still happens with 4.2.4 please? 4.1 doesn't get bugfixes anymore.,"while traits autowired. Or next solution works for both. and above, Also not using traits another one solutions :)"
symfony/symfony,https://github.com/symfony/symfony/issues/27677,symfony_symfony_issues_27677,"Request.getScheme() return http with X_FORWARDED_PROTO https

**Symfony version(s) affected**: 4.0.10 **Description** Request.getScheme() return http with X-Forwarded-Proto header on https **How to reproduce** make sure the php $_SERVER array has:  make sure the HTTP Headers has:  in src/public/index.php put:  carete a new /src/Controller/TestController.php  the output is  but the expected output is  **Additional context** ~~Request ->getClientIp() works and return the correct X-Forwarded-For ipaddress~~ i'm on Apache with [mod_remoteip](https://httpd.apache.org/docs/2.4/mod/mod_remoteip.html) enabled and this settings:",Does TRUSTED_PROXIES support *.0.0/12 IPs or only fixed IPs? Can you check?, **Additional context** Request ->getClientIp() works
symfony/symfony,https://github.com/symfony/symfony/issues/28209,symfony_symfony_issues_28209,"Uncaught PHP Exception Doctrine\DBAL\Exception\ConnectionException

symfony 4,1 mysql connectd faild equest.CRITICAL: Uncaught PHP Exception Doctrine\DBAL\Exception\ConnectionException: ""An exception occurred in driver: SQLSTATE[HY000] [2002] No route to host"" at /var/www/blog/vendor/doctrine/dbal/lib/Doctrine/DBAL/Driver/AbstractMySQLDriver.php line 112 {""exception"":""[object] (Doctrine\\DBAL\\Exception\\ConnectionException(code: 0): An exception occurred in driver: SQLSTATE[HY000] [2002] No route to host at /var/www/blog/vendor/doctrine/dbal/lib/Doctrine/DBAL/Driver/AbstractMySQLDriver.php:112, Doctrine\\DBAL\\Driver\\PDOException(code: 2002): SQLSTATE[HY000] [2002] No route to host at /var/www/blog/vendor/doctrine/dbal/lib/Doctrine/DBAL/Driver/PDOConnection.php:50, PDOException(code: 2002): SQLSTATE[HY000] [2002] No route to host at /var/www/blog/vendor/doctrine/dbal/lib/Doctrine/DBAL/Driver/PDOConnection.php:46)""}",Why would anyone every want to allow a title this long GitHub? 🤪,"equest.CRITICAL: Uncaught PHP Exception Doctrine\DBAL\Exception\ConnectionException: ""An exception occurred in driver: SQLSTATE[HY000] [2002] No route to host"" at /var/www/blog/vendor/doctrine/dbal/lib/Doctrine/DBAL/Driver/AbstractMySQLDriver.php line 112 {""exception"":""[object] (Doctrine\\DBAL\\Exception\\ConnectionException(code: 0): An exception occurred in driver: SQLSTATE[HY000] [2002] No route to host at /var/www/blog/vendor/doctrine/dbal/lib/Doctrine/DBAL/Driver/AbstractMySQLDriver.php:112, Doctrine\\DBAL\\Driver\\PDOException(code: 2002): SQLSTATE[HY000] [2002] No route to host at /var/www/blog/vendor/doctrine/dbal/lib/Doctrine/DBAL/Driver/PDOConnection.php:50, PDOException(code: 2002): SQLSTATE[HY000] [2002] No route to host at /var/www/blog/vendor/doctrine/dbal/lib/Doctrine/DBAL/Driver/PDOConnection.php:46)""}"
symfony/symfony,https://github.com/symfony/symfony/issues/29870,symfony_symfony_issues_29870,"ClassNotFoundException with UniqueEntityValidator

**Symfony version(s) affected**: 4.1 **Description** I'm getting a ClassNotFoundException when using the UniqueEntityValidator: Attempted to load class ""doctrine.orm.validator.unique"" from the global namespace. Did you forget a ""use"" statement? **How to reproduce** My Entity:  composer.json  config/bundles.php",Do you have the DoctrineBundle installed and enabled?,My Entity: composer.json  config/bundles.php 
symfony/symfony,https://github.com/symfony/symfony/issues/30263,symfony_symfony_issues_30263,"404 sometimes ignored

**Symfony version(s) affected**: 3.4 **Description** Some url are correctly handled to 404 error, some (having a dot at the end) doesn't. **How to reproduce** 1. extend fosuser. The login path is /login 2. handle error page on production: https://symfony.com/doc/3.4/controller/error_pages.html Now the cases: - app_dev.php/login: HTTP200, page displayed : as expected - app_dev.php/login1: HTTP404, profiler details : as expected - app_dev.php/login.: HTTP404, profiler details : as expected - app.php/login: HTTP200, page displayed : as expected - app.php/login1: HTTP404, custom error page is displayed : as expected - app.php/login.: HTTP404, default error page is displayed : expecting the custom error page Let me know if any additional stuff may help you?",Can you please create a small example application that allows to reproduce the issue you experience?,(having a dot at the end) 
symfony/symfony,https://github.com/symfony/symfony/issues/32512,symfony_symfony_issues_32512,"[Workflow] No hard dependency to expression language component

**Symfony version(s) affected**: 4.3.2 **Description** Running composer require workflow on a project which doesnt have expression language component throws exception The ""Symfony\Component\Security\Core\Authorization\ExpressionLanguage"" class requires the ""ExpressionLanguage"" component. Try running ""composer require symfony/expression-language"". **How to reproduce** https://github.com/rmikalkenas/workflow-issue **Possible Solution** Move symfony/expression-language dependency in workflow component from require-dev to require **Additional context** PHP 7.3.2 ![Screenshot from 2019-07-17 09-02-08](https://user-images.githubusercontent.com/14221532/61350962-fbc0d400-a871-11e9-84a8-6f2c6b11e851.png)",Can you create a small example application that allows us to do so?,How to reproduce** https://github.com/rmikalkenas/workflow-issue **githubusercontent.com/14221532/61350962-fbc0d400-a871-11e9-84a8-6f2c6b11e851.png)
symfony/symfony,https://github.com/symfony/symfony/issues/33730,symfony_symfony_issues_33730,"Compilation failed: lookbehind assertion is not fixed length at offset 0

**Symfony version(s) affected**: 4.4 Compilation failed: lookbehind assertion is not fixed length at offset 0 in /home/[...]/shopware6/vendor/symfony/routing/Matcher/Dumper/StaticPrefixCollection.php on line 177 I'm using it on FreeBSD with PHP7.3 and the PCRE2 Library. **How to reproduce** Installing a Symfony based application (Shopware6 in our example) and you will get an error when it tries to build all the routes & caches I printed all subPattern and tested the resulting regex: https://github.com/symfony/symfony/commit/465b15caa8e7023c4d18f3869c63c8e9d5d30796#diff-b24d3469ad085b21701584ed8f679d2dR177  test.php  **Possible Solution** Don't know - may this helps: https://www.pcre.org/original/doc/html/pcrepattern.html#lookbehind","Could you please provide a reproducer, in the form of a repository we could clone and run locally to hit the warning?",Compilation failed: lookbehind assertion is not fixed length at offset 0 in /home/[...]/shopware6/vendor/symfony/routing/Matcher/Dumper/StaticPrefixCollection.php on line 177 I'm using it on FreeBSD with PHP7.3 and the PCRE2 Library. olution**
symfony/symfony,https://github.com/symfony/symfony/issues/33822,symfony_symfony_issues_33822,"NativeSessionStorage invalidate function return ini_set error

Issue on Symfony **v4.3.4 Http-Foundation** package. I'm working in my free time in a web with practice purpose, and in the part of the login, I add an option to ""prolongate lifetime of session"", the idea of this is, when it'll be selected I change the original session parameters to prolongate it, in this part I used $session->invalidate(<new lifetime>) ($session is the called Session object previously), so this givme an Warning with this: **How to reproduce** Only that u need is define a Session object and start this, then call to $session->invalidate(<some new lifetime>), in my case, in next small example:  **Note:** using invalidate() method supposed to regenerate the session_id and with new, set a new cookie_lifetime (usable to ""Prevent close my session"" to a login form). I don't know how solve it, or if it'll have a solution.","Do you use the HttpFoundation component outside of the full-stack framework? If so, can you create a small example application that allows to reproduce the issue you experience?",", in next small exampleuse Symfony\Component\HttpFoundation\Session\Attribute\AttributeBag; use Symfony\Component\HttpFoundation\Session\Session; use Symfony\Component\HttpFoundation\Session\Storage\NativeSessionStorage; prolongated_**Note:** using invalidate() method supposed to regenerate the session_id and with new, set a new cookie_lifetime (usable to ""Prevent close my session"" to a login form)."
symfony/symfony,https://github.com/symfony/symfony/issues/34324,symfony_symfony_issues_34324,"Message ID header malformed

**Symfony version(s) affected**: 4.3.5 **Description** When sending email with symfony/mailer, API as transport, and adding header id by:  I end up with a message id in Mailgun that looks like this: ""message-id"": ""Message-ID: <ff54379f02c55b9dec34e967e227c3e6@... **Possible Solution** Remove ""Message-ID: <"" from ""Message-ID: <ff54379f02c55b9dec34e967e227c3e6@....","Which transport are you using? SMTP, HTTP or API?",", API as transport,"
symfony/symfony,https://github.com/symfony/symfony/issues/35585,symfony_symfony_issues_35585,"[Router or HttpFoundation] allow request scheme to be overwritten in parameters

**Description** I make use of the router to generate URL's to be displayed in emails, and also to create redirect URL's to be displayed in a IFrame. This, however, causes issues when running behind a reverse proxy to add a HTTPS certificate. The application thinks it is running on HTTP as all requests come in as :80. It would be a nice addition to be able to overwrite the result of request::getScheme **Example** ![image](https://user-images.githubusercontent.com/3371705/73726883-a1fdf980-4730-11ea-9091-a06957efe1f2.png) Used in RequestContext::fromRequest ![image](https://user-images.githubusercontent.com/3371705/73726907-af1ae880-4730-11ea-861b-97b2abd11df4.png) Used in URLGenerator ![image](https://user-images.githubusercontent.com/3371705/73726964-d2459800-4730-11ea-9bb1-c20e714980e6.png)","Did you tried to [configure symfony to work with proxies](https://symfony.com/doc/current/deployment/proxies.html) ? And if you are sending email with CLI, you can read this [doc](https://symfony.com/doc/2.6/cookbook/console/sending_emails.html)",S certificate. The application thinks it is running on HTTP
symfony/symfony,https://github.com/symfony/symfony/issues/34470,symfony_symfony_issues_34470,"Error on listing environment variables

**Symfony version(s) affected**: 4.4.0-RC1 **Description** When I try to list environment variables with bin/console debug:container --env-vars in DEV environment, I have this error:  **How to reproduce** Type bin/console debug:container --env-vars --env=dev with Symfony 4.4.0-RC1 and PHP 7.3.11 and these required packages. I tried with a fresh installation, the error occurs. Example for reproduction: https://github.com/versgui/sf44-bug  **Possible Solution** <!--- Optional: only if you have suggestions on a fix/reason for the bug --> **Additional context** <!-- Optional: any other context about the problem: log messages, screenshots, etc. -->",Could you please provide a reproducer in the form of a repository we could clone locally?,Example for reproduction: https://github.com/versgui/sf44-bug
symfony/symfony,https://github.com/symfony/symfony/issues/34863,symfony_symfony_issues_34863,"Lazy services in root namespace cannot be found

**Symfony version(s) affected**: 4.4.0, 4.4.1 **Description** After upgrading to 4.4.0 (and trying 4.4.1) when using a single container file, proxy classes for lazy services that have classes in the root namespace can't be found since the container file is in a generated namespace. **How to reproduce** Create a class without a namespace and use it as a lazy service in the container. Must be using the proxy-manager-bridge and container.dumper.inline_factories must be true. Reproducer app: https://github.com/bfeaver/lazy-service-bug All you'll need to do is:  When it does the Symfony cache warmup, it'll fail to find the TestService class. **Possible Solution** I traced it down to \Zend\Code\Generator\ClassGenerator::generateShortOrCompleteClassname() stripping off the leading backslash. If that method instead left it, there wouldn't be a problem. Maybe it would be possible to include the container's namespace as the currentNamespace in the ClassGenerator. Then it wouldn't match the final if condition in that method and it would add back the backslash to the class name.",Could you please provide a small reproducer app? That'd help work on the issue.,"Reproducer app: https://github.com/bfeaver/lazy-service-bub All you'll need to do is:  When it does the Symfony cache warmup, it'll fail to find the TestService class."
highcharts/highcharts,https://github.com/highcharts/highcharts/issues/8651,highcharts_highcharts_issues_8651,"Pie chart connectors drawn on Pie chart

I am wrapping this code in a react library #### Expected behaviour The connectors must not be drawn on pie chart but on the side #### Actual behaviour <img width=""606"" alt=""weird_pie"" src=""https://user-images.githubusercontent.com/1889875/42951546-0c246f06-8b94-11e8-90d8-820bf9536f7f.png""> As you can see, the connector crosses over even though there is space on the right side. I am essentially wrapping the code into a React component as outlined in this blog https://www.highcharts.com/blog/frameworks/react/192-use-highcharts-to-create-charts-in-react/ Following is my code import React, {Component} from 'react' import Highcharts from 'highcharts' import HighchartsMore from 'highcharts-more' HighchartsMore(Highcharts) export default class AlarmPieChart extends Component{ componentDidMount(){ if(this.props.data&&this.props.data.length){ this.loadPieChart(this.props.data,this.props.height,this.props.from,this.props.size,this.props.durationOrFrqType,this.props.dispatch,this.props.filterOnSliceClick) } } loadPieChart(data,height,from,size,durationOrFrqType,dispatch,filterOnSliceClick){ var pieColors = (function () { var colors = [] var base = ""#ea5e35"" var i for (i = 255; i > 128; i -= 25) { // Start out with a darkened base color (negative brighten), and end // up with a much brighter color colors.push(base + i.toString(16)); } return colors; }()); Highcharts.chart(from, { chart: { plotBackgroundColor: null, plotBorderWidth: null, plotShadow: false, type: 'pie', height : height }, credits: { enabled: false }, title: { text: '' }, tooltip: { pointFormat: '{series.name}: <b>{point.percentage:.1f}%</b>' }, plotOptions: { pie: { size: size, allowPointSelect: true, allowOverlap: true, cursor: 'pointer', colors: pieColors, dataLabels: { enabled: true, padding: 0, formatter : function(){ var descriptionArray= this.key ? this.key.replace(/.{15}\S*\s+/g, ""$&@"").split(/\s+@/) : [] var text = """" descriptionArray.map((data)=>{ text = text+""<span style={{'fontWeight':'normal'}}>""+data+""<br/></span>"" }) if((from==""alarms""&&durationOrFrqType!=""frequency"")||(from==""turbine""&&durationOrFrqType!=""frequency"")){ return '<span style={{fontWeight"":""normal""}}>' + this.key + '<br></span><span style={{""fontWeight"":""normal""}}> ' + getFormatedTime(this.y,durationOrFrqType) + '</span>' }else if(from==""windfarm""&&durationOrFrqType==""frequency""){ return '<span style={{fontWeight"":""normal""}}>' + text + '<br></span><span style={{""fontWeight"":""normal""}}> ' + this.y + '</span>' }else if(from==""maintenanceRepairByCost""){ return '<span style={{fontWeight"":""normal""}}>' + text + '<br></span><span style={{""fontWeight"":""normal""}}> </span>' }else if(from==""alarms""&&durationOrFrqType==""frequency""){ return '<span style={{fontWeight"":""normal""}}>' + this.key + '<br></span><span style={{""fontWeight"":""normal""}}> ' + this.y + '</span>' }else{ return '<span style={{fontWeight"":""normal""}}>' + text + '<br></span><span style={{""fontWeight"":""normal""}}> ' + getFormatedTime(this.y,durationOrFrqType) + '</span>' } }, filter: { property: 'percentage', operator: '>', value: (from==""maintenanceRepairByCost""||from==""maintenanceRepairByTime"")?3:4 } } }, line:{ fontWeight:""normal"" } }, series: [{ name: 'Share', data: data, dataLabels: { style: { fontWeight: 'normal' } }, point: { events: { select: function() { //some thing }, mouseOver: function (e) { } } } }] }); } render(){ var id = this.props.from return ( <div id={id}> </div> ) } }  #### Product version version 6.0.7 [Updated on July 20, 2018] #### Affected browser(s) Chrome, Safari",Could you update my code with your hardcoded data and values to introduce the issue?,"0.7 [Updated on July 20, 20"
highcharts/highcharts,https://github.com/highcharts/highcharts/issues/11485,highcharts_highcharts_issues_11485,"I find a bug in highcharts-gantt

Expected behaviour In a grouptask highchart gantt, when hide some parent item, and drag some other point, It will be fine. Actual behaviour Many points disappeared. And I can not interact with gantt. Uncaught TypeError: Cannot read property 'push' of undefined at highcharts-gantt.src.js:43547 at Array.forEach () at highcharts-gantt.src.js:43546 at Array.forEach () at l (highcharts-gantt.src.js:43542) at Object.getTree (highcharts-gantt.src.js:43636) at B (highcharts-gantt.src.js:44888) at highcharts-gantt.src.js:44962 at Array.forEach () Live demo with steps to reproduce https://jsfiddle.net/fav2Lwkb/ hide ""Product launch"" parent item drag some item in ""test drag"" Product version Highchart-gantt 7.1.2 Affected browser(s) all","How does it break the chart? What is the expected behaviour? And the most important, please share a live demo (for example in jsFiddle). Thanks!","Expected behaviour In a grouptask highchart gantt, when hide some parent item, and drag some other point, It will be fine. 7 at Array.forEach () at highcharts-gantt.src.js:43"
sympy/sympy,https://github.com/sympy/sympy/issues/6410,sympy_sympy_issues_6410,"SymPy is lacking a expressiontree facility / creator

To create an Integral <-> Integrand Table for all achievable Integral expressions one needs to create all expression-trees including all functions and functionlike expression quantities (==fnq), like:  where every argument is also an expressiontree. Every fnq can be numbered, so every expression is equivalent to a numbertree. Now: To construct all expressions up to N fnq:s in it one can start from simplest (one fnq, no arguments), list all fnq:s like that and then proceed to two, three etc. up to N fnq:s. The result will always become a number tree. I think SymPy should include a procedure call that would return all these fnq-expressions (as number trees or as expressions) in a Table: Allcombinations_fnq(N) Then differentiating this Table of all expressions would give an Integral Table as such. Then all linear combinations would also be Integral Tables. Applying all available transformation rules would then give alternative versions of same integrals. To solve an integral it is then enough to create the number tree of an Integrand and use the Integral Table, which is fast. Original issue for #6410: http://code.google.com/p/sympy/issues/detail?id=3311 Original author: https://code.google.com/u/107630612048340541426/",Do you know of any other computer algebra systems that do something like this so we can see how they have it implemented? Original comment: http://code.google.com/p/sympy/issues/detail?id=3311#c1 Original author: https://code.google.com/u/asmeurer@gmail.com/,"To create an Integral <-> Integrand Table for all achievable Integral expressions one needs to create all expression-trees including all functions and functionlike expression quantities (==fnq), like:"
sympy/sympy,https://github.com/sympy/sympy/issues/14892,sympy_sympy_issues_14892,"Why does this integral hang Sympy?

Edited to correct an error: Mathematica actually can't solve it either.  After 10 minutes, Sympy still appears to be working on it, though I'm not sure whether I may have sent it into an infinite loop through some error. Mathematica, Maxima and Axiom rapidly return it unevaluated (I have never used Axiom before this, so may well have made an error with it). Is this integral not supported yet in Sympy, or have I made some error? Thank you. (note: I originally said Mathematica could do it; but then I fixed an error in my Mathematica code, and Mathematica is unable to evaluate the corrected expression.)","What type of object is ""seconds""?","2*sympy.cos(sympy.pi*s) + 3)*sympy.tan(sympy.pi*sympy.cos(sympy.pi*s)/After 10 minutes, Sympy still appears to be working on it, though I'm not sure whether I may have sent it into an infinite loop through some error. The same integral can be solved by Mathematica instantly: "
sympy/sympy,https://github.com/sympy/sympy/issues/15135,sympy_sympy_issues_15135,"Units in abbreviated form when using Latex printing

Hi there, I'm using *Mathjax* to print my equation in Latex, however I like to print my equations in **abbreviated form**. Is this possible to add this functionality? I was digging through the source code and I think this functionality can be added somewhere here. https://github.com/sympy/sympy/blob/af7b5b23c8ecffeefa1e3452f5aec87a2d863e10/sympy/printing/latex.py#L1344 **EDIT** Here's an example: This is how it looks right now --> Units are displayed as newton/millimeter2 <img width=""250"" alt=""schermafbeelding 2018-08-23 om 23 19 22"" src=""https://user-images.githubusercontent.com/621054/44552579-0b4f9f80-a72b-11e8-8b6e-c0503f87a636.png""> This is how I want it to look like (but then in latex form) --> Units are displayed as N/mm2 <img width=""336"" alt=""schermafbeelding 2018-08-23 om 23 20 39"" src=""https://user-images.githubusercontent.com/621054/44552648-35a15d00-a72b-11e8-8614-911e95366dda.png""> Kind regards, Gilles",What is abbreviated form? Can you give an example?,"**EDIT** Here's an example: This is how it looks right now <img width=""250"" alt=""schermafbeelding 2018-08-23 om 23 19 22"" src=""https://user-images.githubusercontent.com/621054/44552579-0b4f9f80-a72b-11e8-8b6e-c0503f87a636.png""> This is how I want it to look like (but then in latex form) <img width=""336"" alt=""schermafbeelding 2018-08-23 om 23 20 39"" src=""https://user-images.githubusercontent.com/621054/44552648-35a15d00-a72b-11e8-8614-911e95366dda.png"">"
sympy/sympy,https://github.com/sympy/sympy/issues/15553,sympy_sympy_issues_15553,"rsolve can not solve this kind of recurrences.

Is there any algorithm which can solve recurrences with the form of f(n+1)=2f(n)+n^2+1 f(1)=0 correctly in sympy? Note: rsolve cannot",What kind of recurrences? Please add an example.,Is there any algorithm which can solve recurrences with the form of f(n+1)=2f(n)+n^2+1 f(1)=0 correctly in sympy? Note: rsolve cannot
sympy/sympy,https://github.com/sympy/sympy/issues/16387,sympy_sympy_issues_16387,"dsolve unable to solve differential equation

 ode is classified as 'lie_group' but dsolve is not able to solve. It is going into a infinte loop in the line-https://github.com/sympy/sympy/blob/d3c1050bae5b6caeeacd03efbe25d3a57ff371a3/sympy/solvers/ode.py#L5826 from there I find that this partial differential equation is hanging-  I don't know much about pde so if you can help.",What are the equations that solve has failed on? Do you know whether the Lie group solver should be able to solve this?,33 7 d 3 2 ⎛ 2 ⎞ 3 x ⋅──(f(x)) + 5⋅x ⋅f (x) - ⎝2⋅x + 2⎠⋅f (x) = 0 dx In [14]: classify_ode(eq) Out[14]:
sympy/sympy,https://github.com/sympy/sympy/issues/18850,sympy_sympy_issues_18850,"Fix documentation in Parsing

For sympy.parsing.mathematica.mathematica(), formatting of example is improper. [Link to the doc]( https://docs.sympy.org/latest/modules/parsing.html#sympy.parsing.mathematica.mathematica) In [1]: mathematica(‘Log3[9]’, {‘Log3[x]’:’log(x,3)’}) Out[1]: 2 is written instead of usual >>> mathematica(‘Log3[9]’, {‘Log3[x]’:’log(x,3)’} 2 which results in flawed formatting!",Can you give a link to code/docs and explain which part is broken?,[Link to the doc]( https://docs.sympy.org/latest/modules/parsing.html#sympy.parsing.mathematica.mathematica)
mrdoob/three.js,https://github.com/mrdoob/three.js/issues/8046,mrdoob_three.js_issues_8046,"Any interest for a PR for clipping functionality inside THREE.Shape?

Would there be interest in a pull request for shape clipping using a [Sutherland–Hodgman algorithm](https://en.wikipedia.org/wiki/Sutherland%E2%80%93Hodgman_algorithm). I made a fiddle [here](http://jsfiddle.net/wilt/jk9tm3cy/) to demonstrate the working. If there is interest I could make a pull request for this.",does it work to perform boolean operations? in 3D too? {Wishful thinking!},jk9tm3cy/) to demonstrate the
mrdoob/three.js,https://github.com/mrdoob/three.js/issues/17312,mrdoob_three.js_issues_17312,"The pattern of parameter object / fake keyword arguments

##### Description of the problem (The Python code in this post is untested and may contain slight errors.) <details> <summary>Background: Python keyword arguments</summary> Python functions can be defined with keyword arguments that have default values:  and called with f(x=3, y=2). They can also be defined with a varargs pattern:  Here, kwargs is a Python dictionary, roughly resembling a JavaScript literal object. The second function can be called just like the first one. Both can also be called with f(**{""x"": 3, ""x"": 2}), which is very similar to the pattern used in three.js. The following is a robust version with default values and error on invalid arguments:  </details> Many classes take an object as constructor parameter, which gives similar benefits to Python's keyword arguments: 1. Order is flexible (no need to remember or check docs every time). 2. Optional parameters can be omitted without passing undefined. 3. The calling code is more informative of the purposes, because it must include the names. It likewise shares some disadvantages with Python's kwargs: 1. The calling code is longer than with positional args. 2. Instead of remembering/checking parameter order, one may need to remember/check parameter names (which arguably is easier). On top of this, it has some disadvantages that Python's kwargs don't have: 1. Inline default is for the whole object. This can be alleviated by using deconstruction on the left-hand side of the default value assignment, but that does not make very readable code. The style in three.js seems to be to handle the parameters very carefully in the constructor body, adding defaults on the way (which is a decent solution). 2. Python's kwargs supports the following pattern for ""eating"" kwargs, forwarding the remaining ones and finally ""spitting"" out the remaining:  JS, on the other hand, has no pop on objects. It may be polyfilled (get and delete, or else set to default or throw missing), but that would be kind of a hack. Or one can have a global util for it, to be able to use the same kind of pattern. One (minor) obstacle to replciating the above Python pattern in JS is that JS object don't keep track of their number of properties. https://stackoverflow.com/questions/126100/how-to-efficiently-count-the-number-of-keys-properties-of-an-object-in-javascrip I am sure there exist other advantages and disadvantages than the ones I have mentioned. @mrdoob at least once expressed a wish to move away from the parameter object solution, or at least reduce the use of it: https://github.com/mrdoob/three.js/pull/14726#issuecomment-414086592 He argues that it is somehow not resilient to API changes. I am not sure I understand why. Maybe the kwargs ""eating and spitting"" approach will be regarded as sufficiently robust, in the sense that it quickly tells the user that something is wrong in the case of an invalid (outdated) parameter name. What are the good alternatives to the parameter object, really? I don't exactly favor the setters that mrdoob outlined, when setting many properties at once. Object.assign will obviously work in many cases, though.",Do you mind writing a TL;DR? 😇,One (minor) obstacle to replciating the above Python pattern in JS is that JS object don't keep track of their number of properties. https://stackoverflow.com/questions/126100/how-to-efficiently-count-the-number-of-keys-properties-of-an-object-in-javascrip
mrdoob/three.js,https://github.com/mrdoob/three.js/issues/15639,mrdoob_three.js_issues_15639,"THREE.PointerLockControls rotating bug

##### Description of the problem While testing official example of THREE.PointerLockControls I have experienced the bug. When you're moving mosue around at some point the camera ""teleports"" to another rotation. This bug is persistent. I was only one person among my friends that encounters this bug, everyone else has it OK. https://threejs.org/examples/misc_controls_pointerlock.html * [youtube](https://www.youtube.com/watch?v=3dJk9ap0YT8) (Youtube video that shows the bug) ##### Three.js version Latest, used in official examples. ##### Browser - [x] Chrome - [ ] Firefox - [x] Yandex Browser - [x] Opera ##### OS - [x] Microsoft Windows [Version 10.0.17763.253]",Does this also happen with Firefox (I've seen in your video that the browser is already installed on your system^^)?,- [ ] Firefox - [x] Yandex Browser - [x] Opera
mrdoob/three.js,https://github.com/mrdoob/three.js/issues/17955,mrdoob_three.js_issues_17955,"how about that add InstancdMesh.raycast ?

##### how about that add InstancdMesh.raycast ? At work, I used instancedmesh to improve program performance. Sometimes you need to know the ID of each instance by picking, such as this problem #17906 . I made a demo about getting instance ID by raycast not gpu picking. In this demo, clicking the instance will rotate to indicate that it is selected. And I have done more about the raycast of multi instance rendering of morphTargets , which seems to work well. This is an online demo of Demo: * [webgl_instancing_raycast](https://webglzhang.github.io/three.js/examples/webgl_instancing_raycast.html) * [webgl_instancing_morphtargets_raycast](https://webglzhang.github.io/three.js/examples/webgl_instancing_morphtargets_raycast.html) This is the demo code: https://github.com/webglzhang/three.js/blob/dev/src/objects/Mesh.js https://github.com/webglzhang/three.js/blob/dev/src/objects/InstancedMesh.js https://github.com/webglzhang/three.js/blob/dev/examples/webgl_instancing_raycast.html How about this case? We can discuss whether three.js needs this function. If so, I can make this pr. thanks.",Do you think this is possible?,"And I have done more about the raycast of multi instance rendering of morphTargets , which seems to work well.* [webgl_instancing_morphtargets_raycast](https://webglzhang.github.io/three.js/examples/webgl_instancing_morphtargets_raycast.html) https://github.com/webglzhang/three.js/blob/dev/src/objects/Mesh.js"
sequelize/sequelize,https://github.com/sequelize/sequelize/issues/11571,sequelize_sequelize_issues_11571,"mariadb transaction deadlock does not release connection

## Issue Description ### What are you doing? Link to SSCCE PR: https://github.com/papb/sequelize-sscce/pull/15  ### What do you expect to happen? The second query in t1 will wait for t2, and the second query of t2 will result in a deadlock. After this, t2 should be rolled back, so we should have one available connection to start t3. ### What is actually happening?  After one second of waiting, the timeout error is thrown for t3. ### Additional context The problem seems to be these lines in mariadb/query.js:  These mark the transaction as rolled back, but do not call cleanup to release the connection. (Adding t2.cleanup() in the catch seems to solve the problem.) ### Environment - Sequelize version: sequelize@5.19.8 - Node.js version: v12.11.1 - Operating System: Ubuntu ## Issue Template Checklist ### How does this problem relate to dialects? - [ ] I think this problem happens regardless of the dialect. - [x] I think this problem happens only for the following dialect(s): mariadb and mysql - [ ] I don't know, I was using PUT-YOUR-DIALECT-HERE, with connector library version XXX and database version XXX Tested with mariadb, but the same lines appear in mysql/query.js. ### Would you be willing to resolve this issue by submitting a Pull Request? - [ ] Yes, I have the time and I know how to start. - [ ] Yes, I have the time but I don't know how to start, I would need guidance. - [x] No, I don't have the time, although I believe I could do it if I had the time... - [ ] No, I don't have the time and I wouldn't even know how to start.",Can you wrap your code into a [sequelize-sscce](https://github.com/papb/sequelize-sscce) please?,Link to SSCCE PR: https://github.com/papb/sequelize-sscce/pull/15
sequelize/sequelize,https://github.com/sequelize/sequelize/issues/9919,sequelize_sequelize_issues_9919,"_warningRow is not iterable

## What you are doing? when i config sequelize log with winston logger with config below:  I had set options with showWarnings: true to show sql errors, ## What do you expect to happen? when i exe a warning sql,it crashed with error: _warningRow is not iterable,and can't show any warning msg from mysql,how to work around this? i hava exe this sql in terminal manual:  this is because column name is too long. ## What is actually happening? my environment: **sequelize version** 4.38.0; **mysql version** 5.7.12; **winston version** 3.1.0;",Can you show query that throws warning or reproduce this issue?,i hava exe this sql in terminal manual:  this is because column name is too long.
sequelize/sequelize,https://github.com/sequelize/sequelize/issues/10351,sequelize_sequelize_issues_10351,"sequelize escape the punctuation mark () from schemaDelimiter

## What are you doing? I'm Working on Multi-tenant Application (SAAS) with Shared Database Isolated Schema principle. I've tried solution from https://github.com/renatoargh/data-isolation-example from this article https://renatoargh.wordpress.com/2018/01/10/logical-data-isolation-for-multi-tenant-architecture-using-node-express-and-sequelize/ This is My Sequelize Model using schema Option  ## What do you expect to happen? as you see, FROM \tenant_1.Tasks\ in mysql is a wrong syntax. it must be FROM \tenant_1\.\Tasks\ ## What is actually happening? it return  __Dialect:__ mysql __Dialect version:__1.6.4 __Database version:__ 10.2.17-MariaDB-1:10.2.17+maria~bionic __Sequelize version:__4.42.0 **Note :** _Your issue may be ignored OR closed by maintainers if it's not tested against latest version OR does not follow issue template._ I've create a repo to reproduce this issue https://github.com/Wind213/sequelize-multi-tenant.",What happens when you remove this whole code block?,I've create a repo to reproduce this issue https://github.com/Wind213/sequelize-multi-tenant.
sequelize/sequelize,https://github.com/sequelize/sequelize/issues/11162,sequelize_sequelize_issues_11162,"Count with associations

<!-- Please note this is an issue tracker, not a support forum. For general questions, please use StackOverflow or Slack. --> ## What are you doing? <!-- Post a minimal, self-contained code sample that reproduces the issue, including models and associations --> **To Reproduce** Steps to reproduce the behavior: Let's imagine we have two models User and Image. And Image has two fields:  Somewhere in User model:  We try to call count method  *Update*: I didn't mention I have underscored: true option in models ## What do you expect to happen? We want to get a number! ## What is actually happening? But we get error: column images.imageableType does not exist SQL: SELECT count(""User"".""id"") AS ""count"" FROM ""users"" AS ""User"" LEFT OUTER JOIN ""images"" AS ""images"" ON ""User"".""id"" = ""images"".""imageable_id"" AND (""images"".""imageableType"" = 'user'); Actually, there should be ""images"".""imageable_type"" = 'user' And we get same error if we call findAndCountAll ## Environment Dialect: - [ ] mysql - [x] postgres - [ ] sqlite - [ ] mssql - [ ] any Dialect **library** version: XXX Database version: 11.2 Sequelize version: 5.6.1 Node Version: 11.7.0 OS: macOS Mojave Tested with latest release: - [x] No - [ ] Yes, specify that version:","Could you please test with another dialect, such as SQLite? You can connect to an in-memory sqlite db by using new Sequelize('sqlite::memory:').",*Update*: I didn't mention I have underscored: true option in models
sequelize/sequelize,https://github.com/sequelize/sequelize/issues/11183,sequelize_sequelize_issues_11183,"Updating only updatedAt on a model does not cause an update, even with silent: true

<!-- Please note this is an issue tracker, not a support forum. For general questions, please use StackOverflow or Slack. --> ## What are you doing?  **To Reproduce** Steps to reproduce the behavior: 1. Create a model Person and an instance person 2. Use Person.update to update only the updatedAt field on person 3. Reload person to see that updatedAt has not been updated ## What do you expect to happen? I expected updatedAt to be updated, ## What is actually happening? updatedAt was not updated ## Environment Dialect: - [ ] mysql - [ ] postgres - [ ] sqlite - [ ] mssql - [x] any Dialect **library** version: XXX Database version: PostgreSQL 9.6 Sequelize version: 5.10.0 Node Version: 10.15.3 OS: Ubuntu 18.04.1 on Windows 10 Tested with latest release: - [ ] No - [x] Yes, specify that version: 5.10.0",What is the use case for updating only the updatedAt field?,// // // //
sequelize/sequelize,https://github.com/sequelize/sequelize/issues/11201,sequelize_sequelize_issues_11201,"MSSQL Update from Model with trigger OUTPUT clause without INTO clause

<!-- Please note this is an issue tracker, not a support forum. For general questions, please use StackOverflow or Slack. --> ## I'm doing this? The problem occurs when I update a record in a table with triggers **1. Define model InfoClientes**  **2. Use model**  **See error**  helpme please :) The generated update strings should have been. UPDATE [InfoClientes] SET [Nombre]=@0,[ManejaCopago]=@1 WHERE [IdClientes] = @2 This because clearly specified that nothing returns (returning: false) ## Environment Dialect: - [ ] mssql Dialect **tedious** version: 6.1.2 Database version: Microsoft SQL Server 2016 Sequelize version: 5.8.11 Node Version: v10.15.0",Can you post what SQL query sequelize should have produced instead?,"The generated update strings should have been. This because clearly specified that nothing returns (returning: false) UPDATE [InfoClientes] SET [Nombre]=@0,[ManejaCopago]=@1 WHERE [IdClientes] = @2"
numpy/numpy,https://github.com/numpy/numpy/issues/14968,numpy_numpy_issues_14968,"Importing the numpy c-extensions failed

While trying to run tensorflow, I keep getting this message: ""ImportError: IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy c-extensions failed."" This was the video I used as a guide to set it up: https://www.youtube.com/watch?v=tPq6NIboLSc","Did you see any advice on how to solve the issue, as the error message promises? What did you try?",Importing the numpy ce
numpy/numpy,https://github.com/numpy/numpy/issues/12069,numpy_numpy_issues_12069,"BUG: Floor division returns messed up dtype on windows (python 3)

When using floor division on an array of integers, the resulting dtype appears to be incompatible with itself (see code example). Using an in-place floor division seems to not have the same problem. I could only reproduce this on windows (system information below). ### Reproducing code example:  ### Error message: See the output of the example script. <!-- If you are reporting a segfault please include a GDB traceback, which you can generate by following https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging --> <!-- Full error message, if any (starting from line Traceback: ...) --> ### Numpy/Python version information: 1.15.2 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:06:47) [MSC v.1914 32 bit (Intel)] I used a clean windows 10 (enterprise edition, v1709, OS build 16299.19) vm (https://developer.microsoft.com/en-us/windows/downloads/virtual-machines), installed the latest python 3 from python.org, then pip installed numpy.",What is the actual type of the result?,"print(type(foo), foo.dtype, repr(foo)) # <class 'numpy.ndarray'> int32 array([4, 5, 5, 6, 6, 7, 7, 8, 8]) print(type(foo), foo.dtype, repr(foo)) # <class 'numpy.ndarray'> int32 array([4, 5, 5, 6, 6, 7, 7, 8, 8], dtype=int32)"
numpy/numpy,https://github.com/numpy/numpy/issues/12343,numpy_numpy_issues_12343,"Integer overflow in dtype offset causes segfault

<!-- Please describe the issue in detail here, and fill in the fields below --> ### Reproducing code example:  ### Error message: GDB traceback: ~~~ #0 0x00007ffff7ef4f74 in __memmove_avx_unaligned_erms () from /usr/lib/libc.so.6 #1 0x00007ffff71f7014 in ?? () from /usr/lib/python3.7/site-packages/numpy/core/multiarray.cpython-37m-x86_64-linux-gnu.so #2 0x00007ffff71f77b4 in ?? () from /usr/lib/python3.7/site-packages/numpy/core/multiarray.cpython-37m-x86_64-linux-gnu.so #3 0x00007ffff71d1642 in ?? () from /usr/lib/python3.7/site-packages/numpy/core/multiarray.cpython-37m-x86_64-linux-gnu.so #4 0x00007ffff71ed737 in ?? () from /usr/lib/python3.7/site-packages/numpy/core/multiarray.cpython-37m-x86_64-linux-gnu.so #5 0x00007ffff71ed89e in ?? () from /usr/lib/python3.7/site-packages/numpy/core/multiarray.cpython-37m-x86_64-linux-gnu.so #6 0x00007ffff720c30c in ?? () from /usr/lib/python3.7/site-packages/numpy/core/multiarray.cpython-37m-x86_64-linux-gnu.so #7 0x00007ffff720c49b in ?? () from /usr/lib/python3.7/site-packages/numpy/core/multiarray.cpython-37m-x86_64-linux-gnu.so #8 0x00007ffff72920ab in ?? () from /usr/lib/python3.7/site-packages/numpy/core/multiarray.cpython-37m-x86_64-linux-gnu.so #9 0x00007ffff7b713ff in _PyMethodDef_RawFastCallKeywords () from /usr/lib/libpython3.7m.so.1.0 #10 0x00007ffff7b71461 in _PyCFunction_FastCallKeywords () from /usr/lib/libpython3.7m.so.1.0 #11 0x00007ffff7be170b in _PyEval_EvalFrameDefault () from /usr/lib/libpython3.7m.so.1.0 #12 0x00007ffff7b2a879 in _PyEval_EvalCodeWithName () from /usr/lib/libpython3.7m.so.1.0 #13 0x00007ffff7b2b7a4 in PyEval_EvalCodeEx () from /usr/lib/libpython3.7m.so.1.0 #14 0x00007ffff7b2b7cc in PyEval_EvalCode () from /usr/lib/libpython3.7m.so.1.0 #15 0x00007ffff7c541e4 in ?? () from /usr/lib/libpython3.7m.so.1.0 #16 0x00007ffff7c556ee in PyRun_FileExFlags () from /usr/lib/libpython3.7m.so.1.0 #17 0x00007ffff7c58c45 in PyRun_SimpleFileExFlags () from /usr/lib/libpython3.7m.so.1.0 #18 0x00007ffff7c5af43 in ?? () from /usr/lib/libpython3.7m.so.1.0 #19 0x00007ffff7c5b14c in _Py_UnixMain () from /usr/lib/libpython3.7m.so.1.0 #20 0x00007ffff7db9223 in __libc_start_main () from /usr/lib/libc.so.6 #21 0x000055555555505e in _start () ~~~ ### Numpy/Python version information: Happens on both following systems: ~~~ Arch Linux: 4.18.12-arch1-1-ARCH #1 SMP PREEMPT Thu Oct 4 01:01:27 UTC 2018 x86_64 GNU/Linux 1.15.3 3.7.1 (default, Oct 22 2018, 10:41:28) [GCC 8.2.1 20180831] ~~~ ~~~ Debian 9.5: 4.9.0-7-amd64 #1 SMP Debian 4.9.110-1 (2018-07-05) x86_64 GNU/Linux 1.15.1 3.5.3 (default, Jan 19 2017, 14:11:04) [GCC 6.3.0 20170118] ~~~",What platform?,Arch Linux: 4.18.12-arch1-1-ARCH #1 SMP PREEMPT Thu Oct 4 01:01:27 UTC 2018 x86_64 GNU/Linux
numpy/numpy,https://github.com/numpy/numpy/issues/14329,numpy_numpy_issues_14329,"feature request: get most frequent number in an np array.

On a number of occasions, I would have found it very convenient to have something like: np.most_frequent(np_array, axis=1) This would return the most frequent number. You can do that with hitograms as a workaround but I think it would be very nice as a feature in numpy.",Do you mean you want the [mode](https://en.wikipedia.org/wiki/Mode_(statistics))? Take a look at [scipy.stats.mode](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mode.html).,This would return the most frequent number. You can do that with hitograms as a workaround but I think it would be very nice as a feature in numpy.
numpy/numpy,https://github.com/numpy/numpy/issues/12426,numpy_numpy_issues_12426,"np.fromfile() raises non-helpful error when reading non-numerical text

When I try to read strings from file to numpy array Python crashes. ### Reproducing code example:   The file contains:  <!-- Remove these sections for a feature request --> ### Error message: There is no error message just the Python has stopped working window: ![image](https://user-images.githubusercontent.com/35496930/48771242-247fd200-ecc1-11e8-88ba-78a4969a1697.png) --- Summary by @seberg: As noted below, this is now working, but gives a strange ValueError(for 'S' dtype without a size).","Where did you get numpy? Are you running python inside an IDE or jupyter, or directly from the command line?","--- Summary by @seberg: As noted below, this is now working, but gives a strange ValueError(for 'S' dtype without a size)."
numpy/numpy,https://github.com/numpy/numpy/issues/12638,numpy_numpy_issues_12638,"BUG: several test errors on SPARC

Running the tests of 1.14.5 on my Sparc T5120 with Gentoo results in several test failures:  This has been reported as [Gentoo bug 672730](https://bugs.gentoo.org/672730) and seems to affect also ppc64. EDIT: Failing tests on 1.16.0rc2 - [x] TestKind.test_all - [x] TestRecFunctions.test_structured_to_unstructured - [ ] TestArrayEqual.test_recarrays - [ ] TestEqual.test_recarrays",Can you test against 1.16.rc1?,EDIT: Failing tests on 1.16.0rc2 - [ ] TestKind.test_all - [ ] TestRecFunctions.test_structured_to_unstructured - [ ] TestArrayEqual.test_recarrays - [ ] TestEqual.test_recarrays
numpy/numpy,https://github.com/numpy/numpy/issues/12796,numpy_numpy_issues_12796,"f2py fails to generate Fortran modules on Numpy 1.16.0

<!-- Please describe the issue in detail here, and fill in the fields below --> On Numpy 1.16.0, f2py fails to generate Fortran modules:  Tested on Ubuntu 1.16 and macOS 1.14.2 both with Python 3.6. This appears to be a new bug as there are no issue with Numpy 1.15.4 or lower. ### Reproducing code example: Create test_f2py.f90 and test_f2py.py and add the following lines: - test_f2py.f90  - test_f2py.py  Run test_f2py.py.",Where are you getting numpy from? What is your shell on Ubuntu/Mac OS?,py  import numpy.f2py as f2py with open('test_f2py.
numpy/numpy,https://github.com/numpy/numpy/issues/13229,numpy_numpy_issues_13229,"Optimization for complex matmul (2x speedup)

<!-- Please describe the issue in detail here, and fill in the fields below --> I've been doing some benchmarks on complex matrix multiplication in NumPy and noticed that some quite simple optimizations haven't been implemented. Is there a design-related reason for that or I can try to contribute it? ### Reproducing code example: <!-- A short code example that reproduces the problem/missing feature. It should be self-contained, i.e., possible to run as-is via 'python myproblem.py' -->  (Execution time is measured by Jupyter's %%time magic command.) ### Numpy/Python version information: 1.13.1 3.6.1 (default, Apr 4 2017, 09:40:51) [GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)]",Does this replicate on a more recent numpy? matmul was rewritten as a gufunc.,# Check that relative error is fine np.linalg.norm(C - A@B) / np.linalg.norm(A@B) # 3.9738720532213243e-16
numpy/numpy,https://github.com/numpy/numpy/issues/13393,numpy_numpy_issues_13393,"[$] Optimize NumPy SIMD algorithms for Power VSX

NumPy contains SIMD vectorized code for x86 SSE and AVX. This issue is a feature request to implement native, equivalent enablement for Power VSX, achieving equivalent speedup appropriate for the SIMD vector width of VSX (128 bits). EDIT (by @rgommers): link to bounty: https://www.bountysource.com/issues/73221262-optimize-numpy-simd-algorithms-for-power-vsx The focus is PPC64LE Linux. If the optimization can be portable to AIX (big endian) that's great, but not a strict requirement. In other words, if AIX continues to use the scalar code for now, that's okay.",Do you know anyone who could help?,EDIT (by @rgommers): link to bounty: https://www.bountysource.com/issues/73221262-optimize-numpy-simd-algorithms-for-power-vsx
numpy/numpy,https://github.com/numpy/numpy/issues/13808,numpy_numpy_issues_13808,"ndarray.ctypes creates a reference cycle

Reading attribute ""ctypes.data"" from an ndarray causes a leak. version <= 1.14.5 doesn't leak version >= 1.16.2 leaks latest 1.16.4 leaks ### Reproducing code example:  ### Error message: File ""/mnt/d/code/python/test_numpy_leak.py"", line 17, in <module> assert before==after AssertionError ### Numpy/Python version information: 1.16.4 3.6.8 (default, Jan 14 2019, 11:02:34) [GCC 8.0.1 20180414 (experimental) [trunk revision 259383] ---- Summary (@seberg) 2019-12-03 This has been partially fixed by gh-14469; There are still issues here, although they are mainly issues with upstream ctypes creating deep reference cycles.",What happens if you call gc.collect() before getting the refcount for a second time?,"---- Summary (@seberg) 2019-12-03 This has been partially fixed by gh-14469; There are still issues here, although they are mainly issues with upstream ctypes creating deep reference cycles."
numpy/numpy,https://github.com/numpy/numpy/issues/14413,numpy_numpy_issues_14413,"Regression: lexsort gives wrong results on short integer types

Changing the dtype of a lexsort argument like this should not change the result, but it does with numpy 1.17.0 and 1.17.1:  The problem does not affect 1.16.4 so this is a regression.  (Can also reproduce on 3.7.1 (default, Dec 14 2018, 19:28:38) \n[GCC 7.3.0] so it's not a Windows thing)",What was the 1.16 output?,"(Can also reproduce on 3.7.1 (default, Dec 14 2018, 19:28:38) \n[GCC 7.3.0] so it's not a Windows thing)"
numpy/numpy,https://github.com/numpy/numpy/issues/14516,numpy_numpy_issues_14516,undefined reference to PyCObject_Type',Where did you get NumPy?,__multiarray_api.h:1488: ud to PyCO
numpy/numpy,https://github.com/numpy/numpy/issues/14685,numpy_numpy_issues_14685,"BUG: numpy.percentile output is not sorted

The output of numpy.percentile is not always sorted ### Reproducing code example:  ### Error message: [ True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True False False True True True True False True True True False] AssertionError Traceback (most recent call last) <ipython-input-63-2850fe4a4ce3> in <module> 1 q = np.percentile(np.array([0, 1, 1, 2, 2, 3, 3 , 4, 5, 5, 1, 1, 9, 9 ,9, 8, 8, 7]) * 0.1, np.arange(0, 1, 0.01) * 100) 2 equals_sorted = np.sort(q) == q ----> 3 assert equals_sorted.all() AssertionError: ### Numpy/Python version information: 1.17.2 3.6.8 (v3.6.8:3c6b436a57, Dec 24 2018, 02:04:31) [GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]",Why would you expect it to be sorted? Percentile is elementwise - the outputs are in the order of the inputs.,"arange(0, 1, 0.01) * 100 percentile = np. equals_sorted = np.sort(percentile)"
numpy/numpy,https://github.com/numpy/numpy/issues/14787,numpy_numpy_issues_14787,"Compiling numpy 1.17 on cygwin fails with Error: invalid register for .seh_savexmm

<!-- Please describe the issue in detail here, and fill in the fields below --> Attempting to install numpy 1.17.3 on cygwin fails, with the only reported error message being invalid register for .seh_savexmm. I can't figure out where this would be getting called, since the reported file is standard input and grepping the source for savexmm does not produce any obvious clues. ### Reproducing code example: <!-- A short code example that reproduces the problem/missing feature. It should be self-contained, i.e., possible to run as-is via 'python myproblem.py' -->  <!-- Remove these sections for a feature request --> ### Error message: <!-- If you are reporting a segfault please include a GDB traceback, which you can generate by following https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging --> <!-- Full error message, if any (starting from line Traceback: ...) --> <!-- #### Error message on second compilation  --> #### Error message on third and subsequent compilations  The first and second compilations produce different output before the block of errors, but are otherwise identical. ### Numpy/Python version information: Compiling numpy 1.17.3 on python 3.5.7 fails as described above. <!-- Compilation also fails on python 3.7.4. Compiling numpy 1.17.2 on python 3.7.4 fails in the manner described above. --> Compiling numpy 1.17.0 on python 3.7.4 fails in the manner described above. Numpy 1.16.5 built just fine and passed most tests. <!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' --> How would I go about debugging this?",What GCC version do you have? Are you running 32 or 64 bits?,"numpy/core/src/umath/loops.c.src: In function ‘OBJECT_sign’: numpy/core/src/umath/loops.c.src:2919:1: warning: visibility attribute not supported in this configuration; ignored [-Wattributes] } ^ error: Command ""gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ggdb -O2 -pipe -Wall -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fstack-protector-strong --param=ssp-buffer-size=4 -fdebug-prefix-map=/usr/src/ports/python36/python36-3.6.9-1.x86_64/build=/usr/src/debug/python36-3.6.9-1 -fdebug-prefix-map=/usr/src/ports/python36/python36-3.6.9-1.x86_64/src/Python-3.6.9=/usr/src/debug/python36-3.6.9-1 -ggdb -O2 -pipe -Wall -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fstack-protector-strong --param=ssp-buffer-size=4 -fdebug-prefix-map=/usr/src/ports/python36/python36-3.6.9-1.x86_64/build=/usr/src/debug/python36-3.6.9-1 -fdebug-prefix-map=/usr/src/ports/python36/python36-3.6.9-1.x86_64/src/Python-3.6.9=/usr/src/debug/python36-3.6.9-1 -DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -DNO_ATLAS_INFO=1 -DHAVE_CBLAS -I/usr/local/include -I/usr/include -I/usr/include/suitesparse -Ibuild/src.cygwin-3.0.7-x86_64-3.6/numpy/core/src/umath -Ibuild/src.cygwin-3.0.7-x86_64-3.6/numpy/core/src/npymath -Ibuild/src.cygwin-3.0.7-x86_64-3.6/numpy/core/src/common -Inumpy/core/include -Ibuild/src.cygwin-3.0.7-x86_64-3.6/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/include/python3.6m -Ibuild/src.cygwin-3.0.7-x86_64-3.6/numpy/core/src/common -Ibuild/src.cygwin-3.0.7-x86_64-3.6/numpy/core/src/npymath -Ibuild/src.cygwin-3.0.7-x86_64-3.6/numpy/core/src/common -Ibuild/src.cygwin-3.0.7-x86_64-3.6/numpy/core/src/npymath -c build/src.cygwin-3.0.7-x86_64-3.6/numpy/core/src/umath/loops.c -o build/temp.cygwin-3.0.7-x86_64-3.6/build/src.cygwin-3.0.7-x86_64-3.6/numpy/core/src/umath/loops.o -MMD -MF build/temp.cygwin-3.0.7-x86_64-3.6/build/src.cygwin-3.0.7-x86_64-3.6/numpy/core/src/umath/loops.o.d"" failed with exit status 1 ..."
numpy/numpy,https://github.com/numpy/numpy/issues/14848,numpy_numpy_issues_14848,"Error import Numpy

Unable to import numpy, similar issue for matplotlib as well. ' Installed python 3.7 with Anaconda navigator create function. Also tried to install with anaconda prompt and 'conda create --name n' with similar result. Uninstalled python between each try. Numpy installed with 'conda install -c conda-forge numpy' after first running 'activate n' Running Windows 10 version 1903 There are two pythons that I can find with 'where python' C:\Users\username\AppData\Local\Continuum\anaconda3\envs\n\python.exe C:\Users\username\AppData\Local\Microsoft\WindowsApps\python.exe Last one is a 0 kb file that links to microsoft store installation for python that I am unable to remove. ### Reproducing code example:  ### Error message: Traceback (most recent call last): File ""C:\Users\username\AppData\Local\Continuum\anaconda3\envs\n\lib\site-packages\numpy\core\__init__.py"", line 17, in <module> from . import multiarray File ""C:\Users\username\AppData\Local\Continuum\anaconda3\envs\n\lib\site-packages\numpy\core\multiarray.py"", line 14, in <module> from . import overrides File ""C:\Users\username\AppData\Local\Continuum\anaconda3\envs\n\lib\site-packages\numpy\core\overrides.py"", line 7, in <module> from numpy.core._multiarray_umath import ( ImportError: DLL load failed: The specified module could not be found. During handling of the above exception, another exception occurred: Traceback (most recent call last): File ""C:/Users/username/PycharmProjects/Projectname/test.py"", line 1, in <module> import numpy File ""C:\Users\username\AppData\Local\Continuum\anaconda3\envs\n\lib\site-packages\numpy\__init__.py"", line 142, in <module> from . import core File ""C:\Users\username\AppData\Local\Continuum\anaconda3\envs\n\lib\site-packages\numpy\core\__init__.py"", line 47, in <module> raise ImportError(msg) ImportError: IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy c-extensions failed. - Try uninstalling and reinstalling numpy. - If you have already done that, then: 1. Check that you expected to use Python3.7 from ""C:\Users\username\AppData\Local\Continuum\anaconda3\envs\n\python.exe"", and that you have no directories in your PATH or PYTHONPATH that can interfere with the Python and numpy version ""1.17.3"" you're trying to use. 2. If (1) looks fine, you can open a new issue at https://github.com/numpy/numpy/issues. Please include details on: - how you installed Python - how you installed numpy - your operating system - whether or not you have multiple versions of Python installed - if you built from source, your compiler versions and ideally a build log - If you're working with a numpy git repository, try git clean -xdf (removes all files not under version control) and rebuild numpy. Note: this error has many possible causes, so please don't comment on an existing issue about this - open a new one instead. Original error was: DLL load failed: The specified module could not be found. ### Numpy/Python version information: Running  ###Gives Error message Traceback (most recent call last): File ""C:\Users\username\AppData\Local\Continuum\anaconda3\envs\n\lib\site-packages\numpy\core\__init__.py"", line 17, in <module> from . import multiarray File ""C:\Users\username\AppData\Local\Continuum\anaconda3\envs\n\lib\site-packages\numpy\core\multiarray.py"", line 14, in <module> from . import overrides File ""C:\Users\username\AppData\Local\Continuum\anaconda3\envs\n\lib\site-packages\numpy\core\overrides.py"", line 7, in <module> from numpy.core._multiarray_umath import ( ImportError: DLL load failed: The specified module could not be found. During handling of the above exception, another exception occurred: Traceback (most recent call last): File ""C:/Users/username/PycharmProjects/Projectname/test.py"", line 1, in <module> import sys, numpy File ""C:\Users\username\AppData\Local\Continuum\anaconda3\envs\n\lib\site-packages\numpy\__init__.py"", line 142, in <module> from . import core File ""C:\Users\username\AppData\Local\Continuum\anaconda3\envs\n\lib\site-packages\numpy\core\__init__.py"", line 47, in <module> raise ImportError(msg) ImportError: IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy c-extensions failed. - Try uninstalling and reinstalling numpy. - If you have already done that, then: 1. Check that you expected to use Python3.7 from ""C:\Users\joeyf\AppData\Local\Continuum\anaconda3\envs\Photon\python.exe"", and that you have no directories in your PATH or PYTHONPATH that can interfere with the Python and numpy version ""1.17.3"" you're trying to use. 2. If (1) looks fine, you can open a new issue at https://github.com/numpy/numpy/issues. Please include details on: - how you installed Python - how you installed numpy - your operating system - whether or not you have multiple versions of Python installed - if you built from source, your compiler versions and ideally a build log - If you're working with a numpy git repository, try git clean -xdf (removes all files not under version control) and rebuild numpy. Note: this error has many possible causes, so please don't comment on an existing issue about this - open a new one instead. Original error was: DLL load failed: The specified module could not be found.",Did something recently change? xref gh-14846,Running  ###Gives Error message
numpy/numpy,https://github.com/numpy/numpy/issues/15183,numpy_numpy_issues_15183,"DLL load failed: The specific module could not be found. (VSCODE)

Error below occurs upon import numpy as np; command works fine when typed directly in terminal, but fails when ran via [Code Runner](https://marketplace.visualstudio.com/items?itemName=formulahendry.code-runner). My steps to reproduce:  Output of import sys; print(sys.version) is 3.7.5 (default, Oct 31 2019, 15:18:51) [MSC v.1916 64 bit (AMD64)]. VSCode shows it's running the expected Python interpreter: Python 3.7.5 64-bit ('vsc': conda) at bottom-left pane (see clip) Brief [video demo](https://www.dropbox.com/s/cu4vzyp8ybdq6qo/np_demo.mp4?dl=0) [Relevant SO](https://stackoverflow.com/questions/59519338/dll-load-failed-the-specific-module-could-not-be-found-vscode-numpy) <hr> **Env info**: Windows 10 x64, Anaconda 10/19 (virtual env), VSCode 1.41.1 <hr> **Full traceback**:  <hr> **All enabled extensions**, sorted alphabetically: - Anaconda Extension Pack (1.0.1) - AREPL for python (1.0.20) - Blade Runner - Run task When Open (1.0.0) - Bootstrap 4, Font awesome 4, Font Awesome 5 Free & Pro snippets for Visual studio code (6.1.0) - Bracket Light Pro (0.4.3) - C/C++ (0.26.3-insiders2) - C++ Algorithm Mnemonics (1.0.3) - Code Runner (0.9.15) - Django (0.19.0) - Font Switcher (3.1.0) - Glasslt-VSC (0.1.6) - HTML CSS Support (0.2.3) - kite (0.102.0) - MagicPython (1.1.0) - One Dark Pro (3.2.1) - Python (2019.11.50794) - Python Extension Pack (1.6.0) - Theme by language (1.1.1) - Visual Studio IntelliCode (1.2.2) - YAML (0.6.1) <hr> **settings.json**:","How do you configure Code Runner? If via VSCode settings, those are a bit of a zoo so a screenshot would be helpful.",[Relevant SO](https://stackoverflow.com/questions/59519338/dll-load-failed-the-specific-module-could-not-be-found-vscode-numpy) <hr> **settings.json**: 
numpy/numpy,https://github.com/numpy/numpy/issues/15376,numpy_numpy_issues_15376,"free(): invalid next size (normal)

<!-- Please describe the issue in detail here, and fill in the fields below --> A SIGABRT is raised when running the sample code. ### Reproducing code example: <!-- A short code example that reproduces the problem/missing feature. It should be self-contained, i.e., possible to run as-is via 'python myproblem.py' -->  <!-- Remove these sections for a feature request --> ### Error message: <details> <summary> GDB session output </summary>  ### Numpy/Python version information:",Can you open the issue there referencing this one?,sm = 32 <details> <summary> GDB session output </summary>
numpy/numpy,https://github.com/numpy/numpy/issues/15555,numpy_numpy_issues_15555,"argwhere does not work with pandas Series

np.argwhere() does not work on a pandas series in v1.18.1, whereas it works in an older version v1.17.3. Also, np.where() works on a pandas series but np.argwhere() does not. ### Reproducing code example:  <!-- Remove these sections for a feature request --> ### Error message:  ### Numpy/Python version information: <!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' --> 1.18.1 3.7.4 (default, Aug 13 2019, 20:35:49)","Can you give the _""Full error message, if any (starting from line Traceback: ...) ""_, rather than just the last line?","Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""<__array_function__ internals>"", line 6, in argwhere File ""/Users/kramanathan/anaconda3/envs/py3.7/lib/python3.7/site-packages/numpy/core/numeric.py"", line 584, in argwhere return transpose(nonzero(a)) File ""<__array_function__ internals>"", line 6, in nonzero File ""/Users/kramanathan/anaconda3/envs/py3.7/lib/python3.7/site-packages/numpy/core/fromnumeric.py"", line 1896, in nonzero return _wrapfunc(a, 'nonzero') File ""/Users/kramanathan/anaconda3/envs/py3.7/lib/python3.7/site-packages/numpy/core/fromnumeric.py"", line 58, in _wrapfunc return _wrapit(obj, method, *args, kwds) File ""/Users/kramanathan/anaconda3/envs/py3.7/lib/python3.7/site-packages/numpy/core/fromnumeric.py"", line 51, in _wrapit result = wrap(result) File ""/Users/kramanathan/anaconda3/envs/py3.7/lib/python3.7/site-packages/pandas/core/generic.py"", line 1918, in __array_wrap__ return self._constructor(result,"
numpy/numpy,https://github.com/numpy/numpy/issues/15587,numpy_numpy_issues_15587,"Some functions raise an unhelpful error when applied to object array

Consider the following examples:  This raises error messages along the lines of: 'float' object has no attribute 'shape' The tracebacks don’t give much more information for an inexperienced end user:  This raises two points: * I know that you probably do not want to use object arrays for such purposes, but shouldn’t they work anyway? For example, np.mean works with them. * If object arrays will not be supported, can the error message be more informative? #### Version information: NumPy: 1.18.1 Python: 3.7.5",Can you update the issue description with the output of that?,The tracebacks don’t give much more information for an inexperienced end user: 
nim-lang/Nim,https://github.com/nim-lang/Nim/issues/8139,nim-lang_Nim_issues_8139,"Outdated example to invoke Nim from C

The documentation of [Nim invocation example from C](https://nim-lang.org/docs/backends.html#interfacing-backend-code-calling-nim) is completely outdated. ### original issue Nim compiler generates a statically linked library which does not link.",Did you compile with --app:staticLib?,The documentation of [Nim invocation example from C](https://nim-lang.org/docs/backends.html#interfacing-backend-code-calling-nim) is completely outdated. ### original issue
nim-lang/Nim,https://github.com/nim-lang/Nim/issues/13998,nim-lang_Nim_issues_13998,"compiler error with inline async proc and pragma

try/except in a async inline proc results in compiler error. Repro'ed on 1.2.0, 1.3.1 Linux and MacOS. On 1.0.4 this worked fine. **Only manifests on the cpp backend, not the c backend.** ### Example  Remove the {.async.} marked with REM to make the code compile on 1.2.0, but then it won't run. ### Current Output  ### Expected Output  ### Additional Information * It was working in version 1.0.4",Maybe it's macOS-specific (or specific to Clang on macOS)?,", 1.3.1 **Only manifests on the cpp backend, not the c backend.**"
nim-lang/Nim,https://github.com/nim-lang/Nim/issues/14033,nim-lang_Nim_issues_14033,"Nim In Action SFML test fails on OpenBSD

The Nim In Action SFML test tests/niminaction/Chapter8/sfml/sfml_test.nim fails on OpenBSD with the following error:  SFML is installed with doas pkg_add sfml:",Do you have header files of SFML? I think they are installed in /usr/include or /usr/local/include.,----------- It also fails on FreeBSD with the same error.
nim-lang/Nim,https://github.com/nim-lang/Nim/issues/7963,nim-lang_Nim_issues_7963,"strutils/isUpperAscii and unicode/isUpper consider space, punctuations, numbers as ""lowercase""

Test:  Just for precedent, Python, Emacs Lisp return a ""true"" for upper-case checking of ""A B"", ""AB?!"" and ""1, 2, 3 GO!"". --- [Python isupper vs Nim isUpper][1] [1]: https://scripter.co/notes/string-fns-nim-vs-python/#is-upper","Why?  Your tests contains space, it's non alpha. And this the right logic.",--- [Python isupper vs Nim isUpper][1] [1]: https://scripter.co/notes/string-fns-nim-vs-python/#is-upper
nim-lang/Nim,https://github.com/nim-lang/Nim/issues/14060,nim-lang_Nim_issues_14060,"Depending on the gcc version, doing echo .. | nim c -r - gives ""can't open nm output"" error

Hello, At work, we are using gcc 6.1.0 for stability reasons. With that gcc, passing nim code to nim via the stdin results in an error. That issue does not happen on gcc 9.1.0. ### Example  ### Current Output I get **collect2: fatal error: can't open nm output: Success** when I have gcc 6.1.0 loaded in my env.  Here's the same output with more debug info:   ### Expected Output I get this when using gcc 9.1.0 (but I cannot have that gcc version loaded in my work env).  ### Additional Information - While echo echo NimVersion | nim c -r - gives this error, if I run the autogenerated /tmp/kmodi/.cache/nim/stdinfile_d/stdinfile executable, I get the expected echo output. - I have seen this issue before too on older nim versions, but I did not figure out then that that issue had to do with gcc version: https://github.com/nim-lang/Nim/issues/11294#issuecomment-495716406 - The same issue is seen with a bit newer gcc 7.2.0. The next gcc version I have after that is 9.1.0 (and I don't see that issue with gcc 9.1.0) - I get the same error on gcc 6.1.0 when running echo echo NimVersion | nim c - (without -r)  - OS : RHEL 6.8",Can you try --genScript then use the script to compile? If that doesn't work as well I think we'll have to attribute this to gcc.,- I get the same error on gcc 6.1.0 when running echo echo NimVersion | nim c - (without -r)
nim-lang/Nim,https://github.com/nim-lang/Nim/issues/14019,nim-lang_Nim_issues_14019,"Add pragma for disabling enforced exhaustive case coverage

### Problem Enforcing exhaustive coverage in case statement is an awesome feature, but sometimes you don't know if you have covered all cases or not. This creates the following situation where there is always a possibility to get an error: - If you **don't** add an else: discard statement -> not all cases are covered  - If you **do** add an else: discard statement ----> invalid else, all cases are already covered This of course is no problem during the general development case, because the compiler will just inform you whether you are covering all cases or not, which is great. In case you are generating code though, getting such compile errors when you know your generated code is correct can be bothersome. ### Possible solutions 1. Introduce a pragma for disabling the check of exhaustive coverage. Possible names: - {.dirtyCase.} - {.lenientCase.} 2. Change Error: invalid else, all cases are already covered into a warning.",Why not just generate the else: clause in your bothersome cases?,"2. Change Error: invalid else, all cases are already covered into a warning."
nim-lang/Nim,https://github.com/nim-lang/Nim/issues/13405,nim-lang_Nim_issues_13405,"AST injection macro for identifiers, leveraging lookup table

### Summary Ability to inject AST before or after an identifier such as a variable or proc ### Description Would be super powerful if we could allow AST injection of this form:  where the injectAstFor takes an identifier and an enum value of:  The compiler we need to additionally maintain a lookup table for all identifiers: id -> index in p.body where AST generation for id node is: - on injection, insert a new entry in body at that position. - for each entry in lookup table coming after injection id, increase index by 1",What would this be useful for?,noEmit: proc service(name: string)
scikit-learn/scikit-learn,https://github.com/scikit-learn/scikit-learn/issues/12445,scikit-learn_scikit-learn_issues_12445,"[BUG!] elasticNet and multiElasticNet result in wrong model, while enet_path give the right one

#### Description linear_models.enet_path give the right sparse model while linear_models.ElasticNet and linear_models.MultiTaskElasticNet give wrong models. #### Steps/Code to Reproduce I am using Python2.7. please go to my github git clone https://github.com/pswpswpsw/test_on_sklearn_elasticNet.git python test.py **If you don't have the time to check the code, read this**: Do you see any difference betwen  and this  **Note: Look for the grey line in the picture!!!!** #### Expected Results I expect the enet_path gives the same result as MultiTaskElasticNet #### Actual Results enet_path give the right, coefficient with smallest l1 norm, while the others don't. #### Versions ('Python', '2.7.15 |Anaconda, Inc.| (default, May 1 2018, 18:37:09) [MSC v.1500 64 bit (AMD64)]') ('NumPy', '1.14.3') ('SciPy', '1.1.0') ('Scikit-Learn', '0.19.1')",what do you say is the right model? ElasticNet uses enet_path did you check the parametrization?," **If you don't have the time to check the code, read this**: Do you see any difference betwen  alphas_enet, coefs_enet, _ = enet_path(A, B, max_iter=3000, tol=1e-6, alphas=np.logspace(-16,1,200), l1_ratio=0.5, fit_intercept=False)  clf = Ela alphas_enet, coefs_enet, _ = enet_path(A, B, max_iter=3000, tol=1e-6, alphas=np.logspace(-16,1,200), l1_ratio=0.5, fit_intercept=False)  clf = ElasticNet(alpha=alpha, l1_ratio=0.5, tol=1e-6, max_iter=3000, fit_intercept=False) clf_m = MultiTaskElasticNet(alpha=alpha, l1_ratio=0.5, tol=1e-6, max_iter=3000, fit_intercept=False) "
scikit-learn/scikit-learn,https://github.com/scikit-learn/scikit-learn/issues/12256,scikit-learn_scikit-learn_issues_12256,"Errow thrown when running TF-IDF vectorizer on scikit-learn 0.20

<!-- Instructions For Filing a Bug: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#filing-bugs --> #### Description I got an error when I run the fit function from TFIDFVectorizer. I didn't get this issue before 0.20 and when I uninstall 0.20 and re-install 0.19.1 the bug disappear. #### Steps/Code to Reproduce Example:  #### Expected Results No error is thrown and I got the following results.  #### Actual Results  The full traceback :  #### Versions Windows-10-10.0.14393-SP0 Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)] NumPy 1.15.2 SciPy 1.1.0 Scikit-Learn 0.19.1 Is there something that I'm missing ? Or could it come from scikit-learn update ? Thank you ! :) <!-- Thanks for contributing! -->",Could you also please post the full error traceback?,The full traceback : 
scikit-learn/scikit-learn,https://github.com/scikit-learn/scikit-learn/issues/15036,scikit-learn_scikit-learn_issues_15036,"check_array forces finite values for non-numeric data

#### Description When check_array is given dtype=None, it still tries to force finite values. #### Steps/Code to Reproduce  #### Expected Results No error is thrown. Values are not checked for finiteness. _or_ No error is throw. Values are checked for finiteness. #### Actual Results  #### Versions","Did you mean to do,  instead?","check_array(x, dtype=object) # This fails too."
scikit-learn/scikit-learn,https://github.com/scikit-learn/scikit-learn/issues/14485,scikit-learn_scikit-learn_issues_14485,"ImportError: dlopen: cannot load any more object with static TLS with torch built with gcc 5.5

I am not sure if this is a PyTorch bug, a scikit-learn bug or a numba, but this used to work in scikit-learn 0.20.3 and stopped working in the 0.21.0 series, so for now I am going to venture a guess that it is a regression in scikit learn. When I do the following series of imports (minimized from the original import, which was import librosa), loading the following program fails:  with  Downgrading to scikit-learn 0.20.3 makes the problem go away. #### Versions  Also, you may be interested in:  The build of torch must be done with gcc 5.5.0 to cause this problem; other versions of gcc are known not to cause this problem. For ease of reproduction, you can use the following docker image ezyang/scikit-learn-tls-repro:1 https://cloud.docker.com/repository/registry-1.docker.io/ezyang/scikit-learn-tls-repro Once in, follow the reproduction instructions as described above. (EDIT At time of writing, the Docker image is still uploading. Should be done soon.)",How did you build/install sklearn?,"(EDIT At time of writing, the Docker image is still uploading. Should be done soon.)"
scikit-learn/scikit-learn,https://github.com/scikit-learn/scikit-learn/issues/16279,scikit-learn_scikit-learn_issues_16279,"Multinomial LogisticRegressionCV test with solver newton-cg fails on MacOS with dev version of numpy

#### Describe the bug Multinomial LogisticRegressionCV test with solver newton-cg fails on my MacOS with the current dev version of numpy. #### Steps/Code to Reproduce Run pytest. Might be related #11924, but which doesn't fail. #### Actual Results  #### Versions",Would you mind checking if it's reproducible with the latest stable numpy ?,with the current dev version of numpy
Automattic/mongoose,https://github.com/Automattic/mongoose/issues/7838,Automattic_mongoose_issues_7838,"mongoose.connect() doesn't return a promise in Angular universal, when using 'srv' in uri string

in server.ts (express server):  neither .then() nor .catch() is called, but 'aaa' is called, means that the express path works fine **note1**: running this code outside of angular works fine **note2**: console.log(mongoose.connect(uri)) in Angular returns zone (not promise):  ZoneAwarePromise { __zone_symbol__state: null, __zone_symbol__value: [] } when replacing mongoose.connect(..) with new Promise(r=>r()) it works fine update: when removing +srv from the connection string it works with error i.e: catche() runs, but with 'srv' neither .then() or .catch() runs",Do you have a sample project using angular universal?,"update: when removing +srv from the connection string it works with error i.e: catche() runs, but with 'srv' neither .then() or .catch() runs"
modxcms/revolution,https://github.com/modxcms/revolution/issues/13953,modxcms_revolution_issues_13953,"MODX 3: Manager Sidebar - Sidebar position whilst loading

### Summary I appreciate this is a small detail... As the manager is loading, the avatar, settings and help icon change position from being placed just below the ""Manage"" icon, to being positioned at the bottom of the browser window. This causes a slight jump in content and causes distraction, especially for those with a slow internet connection. ### Step to reproduce 1. Login to the manager and keep an eye on the placement of your avatar, settings and help icon. 2. Browse around other pages and notice the jump/change in position as the page loads. ![screen shot 2018-07-03 at 11 41 37](https://user-images.githubusercontent.com/2373940/42212742-4ad8a274-7eb7-11e8-9cfa-41f4374d7d39.png) ![screen shot 2018-07-03 at 11 50 55](https://user-images.githubusercontent.com/2373940/42212779-66628c80-7eb7-11e8-8cd0-b42e7ef95345.png) ### Observed behavior The jump seems to be caused whilst the browser loads the javascript, which then applies a height to the sidebar. ### Expected behavior Perhaps some CSS should be applied such as 'vh', 'vmin', 'min-height' or even a 'height: 100%;' to the sidebar, whilst the javascript loads, to make it feel less jumpy and give the impression the manager is loading faster than it actually is. ### Environment MODX3 Alpha v 3.0.0-dev Chrome Version 67.0.3396.99",Why are you closing all these issues?,' or even a 'height: 100%;
modxcms/revolution,https://github.com/modxcms/revolution/issues/13957,modxcms_revolution_issues_13957,"MODX 3: Resource/Elements/File Tree - contrast of row stripes

### Summary The row stripes in the resource/element/file tree have such little contrast between the light and dark colours, that they can't be seen on budget or old monitors with less colours. I mention this, because in general, i'm finding the new Resource/Elements/File Tree very grey, and lacks contrast that helps with viewing the general hierarchy of content. ![screen shot 2018-07-03 at 13 11 13](https://user-images.githubusercontent.com/2373940/42216689-fa574772-7ec2-11e8-8716-05f9b765f2cd.png) ### Step to reproduce View resource/element/file tree on a cheap monitor or view on a regular monitor ### Observed behavior Resource/element/file tree have such little contrast between the light and dark colours ### Expected behavior If it can't really be seen, why display it? It just creates extra eye fuzz. Removing the stripes or increasing the contrast of the stripes, might add more overall contrast to the resource/element/file tree in general - as i'm finding it hard to distinguish between: 1. The hierarchy of Templates, Template Variables, Chunks, Snippets, Plugins, Categories compared to MODX 2.x.x 2. Open folders/categories in Templates, Template Variables, Chunks, Snippets, Plugins, Categories compared to MODX 2.x.x 3. Viewing active / selected elements compared to MODX 2.x.x 4. The bin icon being active vs non active 5. Hidden / non hidden resources ![screen shot 2018-07-03 at 13 22 48](https://user-images.githubusercontent.com/2373940/42217124-46b1385c-7ec4-11e8-80a3-86b188ac6630.png) Related: https://github.com/modxcms/revolution/issues/13969 https://github.com/modxcms/revolution/issues/13960 ### Environment MODX3 Alpha v 3.0.0-dev From https://github.com/Sterc/modx3builds/raw/master/latest/modx3-alpha-regular.zip Chrome Version 67.0.3396.99",Why are you closing?,of contentRelated: https://github.com/modxcms/revolution/issues/13969 https://github.com/modxcms/revolution/issues/13960
modxcms/revolution,https://github.com/modxcms/revolution/issues/14139,modxcms_revolution_issues_14139,"Error Update from 2.6.5 to 2.7: Call to a member function deprecated()

### Summary Could not update a 2.6.5 Modx website. During the setup the following error appeared.  Log says:  ### Step to reproduce Updated the files on webspace and run setup. All checks were ok. On install the error appeared: ### Observed behavior After setup the manager page is a white page. Things I've tried: 1. Clear core/cache multiple times. 2. Check that compress_js/css are turned off. 3. Re-ran setup. 4. Cleared browser cache and checked in Incognito Mode (Chrome) or Private Mode (Firefox). 5. Nothing in server error log having to do with MODx. ### Installed Extensions ace-1.6.5-pl batcher-2.0.0-pl ckeditor-1.4.0-pl cmpgenerator-1.1.3-pl formit-4.0.1-pl formit2db-1.1.4-pl getresources-1.6.1-pl login-1.9.5-pl migx-2.12.0-pl orphans-1.1.1-pl pdotools-2.11.0-pl simpleupdater-2.1.2-rc uicmpgenerator-1.0.1-beta upgrademodx-1.5.5-pl versionx-2.1.3-pl wayfinder-2.3.3-pl ### Environment MODX version 2.6.5, apache and PHP version 7.1.23 , mysql version 5.6.42,",Could you get an apache or php error log? Normally php fatal errors are logged there., Log says: [2018-11-15 09:09:41] (ERROR in xPDOManager_mysql::addField @ /page/core/xpdo/om/mysql/xpdomanager.class.php : 302) Error adding field modResource->alias_visible: Array ( [0] => 42S21 [1] => 1060 [2] => Duplicate column name 'alias_visible' )
ipython/ipython,https://github.com/ipython/ipython/issues/11372,ipython_ipython_issues_11372,"test_history failure

I am trying to package ipython 7.0.1 for openSUSE and I am getting the following error in the unit tests:  It looks like the last two list elements have switched places but I don't know why that might be the case. ---- EDIT: We can likely fix this issue in two steps: 1) mark the test as skip (or known fail) for the range of sqlite versions that appear to be affected. 2) actually figure out if this is a change in behavior worth fixing or if the test should be updated accordingly.",Which version of Python are you running that on ? Look at the version of sqlite as well.,---- EDIT: We can likely fix this issue in two steps: 1) mark the test as skip (or known fail) for the range of sqlite versions that appear to be affected. 2) actually figure out if this is a change in behavior worth fixing or if the test should be updated accordingly.
ipython/ipython,https://github.com/ipython/ipython/issues/11336,ipython_ipython_issues_11336,"Weird character after first import

I'm running python 7.0.1 Everything seems to work fine but after the first import I consistently get a weird symbol (see attached screenshot). When I press enter again the symbol goes away and doesn't reappear. I'm using fish shell on an iterm2 terminal on mac. ![screenshot 2018-09-27 at 13 51 00](https://user-images.githubusercontent.com/568775/46174104-fb146e00-c25c-11e8-9c46-2c99b7e578ad.png) [Update] Upgrade prompt_toolkit to 2.0.6 should fix the issue.",Maybe @jonathanslenders has some idea?,[Update] Upgrade prompt_toolkit to 2.0.6 should fix the issue.
cakephp/cakephp,https://github.com/cakephp/cakephp/issues/12672,cakephp_cakephp_issues_12672,"Undefined variable: etagMatches and timeMatches

Hello, I recently updated PHP to the PHP 7.3 version and these two advents have appeared. How do I solve it? Thank you CakePHP version: Is the 3.6  Thank you",What version of CakePHP are you using?,"CakePHP version: Is the 3.6 //////////////////////////////////////////////////////////////////////////////////////////////////// // +--------------------------------------------------------------------------------------------+ // // CakePHP Version // // Holds a static string representing the current version of CakePHP // // CakePHP(tm) : Rapid Development Framework (https://cakephp.org) // Copyright (c) Cake Software Foundation, Inc. (https://cakefoundation.org) // // Licensed under The MIT License // Redistributions of files must retain the above copyright notice. // // @copyright Copyright (c) Cake Software Foundation, Inc. (https://cakefoundation.org) // @link https://cakephp.org // @since CakePHP(tm) v 0.2.9 // @license https://opensource.org/licenses/mit-license.php MIT License // +--------------------------------------------------------------------------------------------+ // ////////////////////////////////////////////////////////////////////////////////////////////////////"
doctrine/orm,https://github.com/doctrine/orm/issues/7897,doctrine_orm_issues_7897,"Commands using DisconnectedClassMetadataFactory fails with embedded type without columnPrefix

### Bug Report Commands using DisconnectedClassMetadataFactory fails with embedded type without columnPrefix. Actual for commands: GenerateEntitiesCommand, ConvertMappingCommand, ImportMappingDoctrineCommand | Q | A |------------ | ------ | BC Break | no | Version | 2.6.3 #### Summary Fails in commands, like as doctrine:mapping:convert, which using DisconnectedClassMetadataFactory (it always returns StaticReflectionService with dummy  public function getClass($class) { return null; } ), when Entity has embedded field without prefixColumn (if you want use fully auto-generated column name), because reflection is not object \Doctrine\ORM\Mapping\ClassMetadataInfo:  #### Current behavior  #### How to reproduce Add:  Run: php bin/console doctrine:mapping:convert yml folder-to-dist #### How to reproduce Command execution without exception",Does this also affect 2.6.x?,#### How to reproduce Command execution without exception
doctrine/orm,https://github.com/doctrine/orm/issues/7906,doctrine_orm_issues_7906,"Using remove in preFlush event results in spl_object_hash conflict

### Bug Report | Q | A |------------ | ------ | BC Break | no | Version | 2.6.4 #### Summary We have had a number of reported bugs related to spl_object_hash conflicts in the past, and it seems the problem is not eliminated yet. For example, EntityManager's (and UnitOfWork's) remove method cannot be safely used in PreFlush. #### Current behavior If the remove() is called on an entity within a PreFlush event, it is garbage collected but its hash is still remembered by the EntityManager. Creating another entity afterwards (with a subsequent flush) can result in hash conflict and ORM error: #### How to reproduce Entity specification:  Core of the unit test:  #### Expected behavior Both flush operations should perform without error, and none of the entities should be saved into the database.",Do you expect that none of the entities in the example will be saved into the database? What is your use case?,", and none of the entities should be saved into the database"
cocos2d/cocos2d-x,https://github.com/cocos2d/cocos2d-x/issues/20444,cocos2d_cocos2d-x_issues_20444,"Issues while building a new cocos2dx-v4 project for iOS and xCode

- cocos2d-x version: **cocos2d-x v4** - cmake version: **3.7.2** Steps to Reproduce: Following the documentation to generate iOS Project:  I get the following output in the console:  If I try to run over the cmake:  I get:  Could you return the good practice when the iOS developers for the framework did not have at least the problems before starting the development when creating xCode project?",what's your CMake version?,** - cmake version: **3.7.2**
cocos2d/cocos2d-x,https://github.com/cocos2d/cocos2d-x/issues/19787,cocos2d_cocos2d-x_issues_19787,"COCOS2D_DEBUG is still set to 1 in release on android

I had a if(COCOS2D_DEBUG == 1) { do something } and on iOS it's not defined in release but on Android it is and even if you build in release mode it still runs the if statement pretty much saying that COCOS2D_DEBUG is equal to 1 more info: https://discuss.cocos2d-x.org/t/how-to-build-an-apk-in-in-release-mode-from-android-studio/25906 https://discuss.cocos2d-x.org/t/cocos2d-debug-in-release-builds/23434",What's your engine version? And how did you build with release mode on Android?,more info: https://discuss.cocos2d-x.org/t/how-to-build-an-apk-in-in-release-mode-from-android-studio/25906 https://discuss.cocos2d-x.org/t/cocos2d-debug-in-release-builds/23434
bigbluebutton/bigbluebutton,https://github.com/bigbluebutton/bigbluebutton/issues/9338,bigbluebutton_bigbluebutton_issues_9338,"Screen sharing not working on Wayland

Sharing the entire screen does not work on Wayland systems. **To Reproduce** Server: BBB 2.2.5 Client: Debian Bullseye (testing) with GNOME on Wayland, AMD64, two monitors Tested in Firefox 68.7.0esr(64-bit) 1. Click on 'Share your Screen' 2. Firefox offers different Options of what to share. ""Your Entire Screen"" is black in the preview. Share this. 3. The screen is shared but it's black except for the mouse pointer. The same problem appears in Chromium 81 **Additional context** This problem seems to be know, as it was [mentioned](https://groups.google.com/d/msg/bigbluebutton-dev/v81jZjBMOkI/AAJXeFSwCwAJ) on the developer mailing list. **update** I tested on a ""GNOME on X.ORG""-Session and can confirm that it's working as expected there. I also noticed that in a Wayland session not all open windows are available to be shared. Not sure it that should be a separate issue.",Does it work for you using this [WebRTC screen sharing sample](https://www.webrtc-experiment.com/Pluginfree-Screen-Sharing/#9514356012950654)?,"**update** I tested on a ""GNOME on X.ORG""-Session and can confirm that it's working as expected there. I also noticed that in a Wayland session not all open windows are available to be shared. Not sure it that should be a separate issue."
jgm/pandoc,https://github.com/jgm/pandoc/issues/5543,jgm_pandoc_issues_5543,"Markdown to markdown conversion losing backslash escape in definition list

Source:  Command: pandoc --from markdown --to markdown Bad result:  Expected result:  https://pandoc.org/try/?text=1%5C.+item%0A%3A+++description&from=markdown&to=markdown",What's wrong with that? What's the expected result?,. item : description markdown 1\
jgm/pandoc,https://github.com/jgm/pandoc/issues/4736,jgm_pandoc_issues_4736,"Modify warning when included file not found?

Pandoc version: 2.2.1 I have the following setup: personal-hyphenation.tex is residing in my texmf-local tree (FWIW it's actually a symlink to a version controlled file someplace else, but a test confirmed that even if the link is replaced by an actual file it is not found) and consists solely of \hyphenation{word-list}. kpsewhich finds the file, and so does pdflatex. In my .md sources I use \include{personal-hyphenation.tex} to load these patterns. Up until pandoc 2.1.3 (which was the last version I had installed), pandoc used find that file. Now I get a warning:",Does it work if the file is not a symlink?,", but a test confirmed that even if the link is replaced by an actual file it is not found"
jgm/pandoc,https://github.com/jgm/pandoc/issues/4768,jgm_pandoc_issues_4768,"LaTeX Reader \def\Sec#1[#2]#3{

using the versoin 1.19.2.4~dfsg-1 in debian on the file https://github.com/cplusplus/draft/blob/master/source/macros.tex#L85 in this part of the code  Gives the output","Can you try with the latest pandoc version? Also, what exact command are you using?",using the versoin 1.19.2.4~dfsg-1 in debian
jgm/pandoc,https://github.com/jgm/pandoc/issues/5097,jgm_pandoc_issues_5097,"Add support for blank slides with powerpoint output.

### Original (24 November 2018) There is currently no way to specify an intentional blank slide for powerpoint output. For blank slides with reveal.js, I can specify markdown input like this:  .hidden is my own CSS style that applies display: none. This allows headings and content to still display (for information purposes) in non-slide output formats (e.g. pure html or pdf). I would like to have the powerpoint output to also know that the slide should be blank. I have a reference.pptx file with the blank layout in the slide master as I would like to have it (e.g. no text with a completely black background), but I don't have a way to specify that the 'blank' layout should apply to the slide. Blank pages are useful: https://en.wikipedia.org/wiki/Intentionally_blank_page Perhaps YAML frontmatter can be used to specify the slide layout for powerpoint output specific? It may even be made more general to apply to all slide output formats, but a powerpoint-only solution would satisfy my requirements. ### Edit (18 December 2018) Just some clarifications about what I mean. It is possible to create 'blank content' slides for powerpoint output using markdown input. But the powerpoint layout used for the blank slide is the same layout as for the non-blank slides. Powerpoint master slide layouts support a variety of predefined layouts, that can each be styled separately. I think Pandoc supports four of those predefined layouts, and applies them automatically based on the content of the slide. One of the predefined powerpoint layouts, is called 'Blank' or 'None'. I would like this predefined Blank layout to be applied to slides that have no content. If I edit the reference document or template (let's say reference.pptx), and change the properties of the Blank layout (let's say to use a black background instead of the default white), then the blank slides in my pandoc-generated pptx file, should have a black background. If this can't be achieved using the predefined Blank layout, then perhaps it should be possible to style a powerpoint slide (i.e. set its background color) using Pandoc classes or some other method.",What about  Does this not work?,"### Original (24 November 2018) ### Edit (18 December 2018) Just some clarifications about what I mean. It is possible to create 'blank content' slides for powerpoint output using markdown input. But the powerpoint layout used for the blank slide is the same layout as for the non-blank slides. Powerpoint master slide layouts support a variety of predefined layouts, that can each be styled separately. I think Pandoc supports four of those predefined layouts, and applies them automatically based on the content of the slide. One of the predefined powerpoint layouts, is called 'Blank' or 'None'. I would like this predefined Blank layout to be applied to slides that have no content. If I edit the reference document or template (let's say reference.pptx), and change the properties of the Blank layout (let's say to use a black background instead of the default white), then the blank slides in my pandoc-generated pptx file, should have a black background. If this can't be achieved using the predefined Blank layout, then perhaps it should be possible to style a powerpoint slide (i.e. set its background color) using Pandoc classes or some other method."
jgm/pandoc,https://github.com/jgm/pandoc/issues/5521,jgm_pandoc_issues_5521,"Temporary files are created in directory where pandoc is called instead of in /tmp

When I run pandoc -o /tmp/output.pdf input.md there are temporary files created in the directory where I ran the command. Is there a reason that these are not created in /tmp? I think it would be better to create those in /tmp both for increased speed and less unnecessary wear of the hard drive.  The files are deleted afterwards. The directory I run it from is a subdirectory of my homedir, so there is a ~ in that path, but that should not matter edit: more detailed description",What pandoc version and OS are you on? Relevant code: https://github.com/jgm/pandoc/blob/master/src/Text/Pandoc/PDF.hs#L88, The files are deleted afterwards. edit: more detailed description
jgm/pandoc,https://github.com/jgm/pandoc/issues/5867,jgm_pandoc_issues_5867,"Workflow for writing and exporting scientific documents with Pandoc

## Backstory Last two months I spent on trying to develop a workflow for easy (but powerful) scientific writing in some lightweight markup language. Among other things that are implied by almost any markup language (code blocks, images, etc...) my needs were following: * Support for mathematical expressions * Good quality customizable PDF conversion without LaTeX (see more on that below) * HTML export (for deploying it to GitHub Pages) as PDF is printable paged media, but almost nobody these days cares about printing. PDF is good for offline reading, but why pages? Firstly, I tried [Jupyter](https://jupyter.org): both Jupyter Notebooks and JupyterLab. After two weeks of trying to create a nice-looking PDF from notebook, I decided to stop this masachism and look for the alternatives. Here are some of the reasons, why Jupyter Notebooks is not appropriate solution for me: * Jupyter Notebook is buggy. JupyterLab is very buggy. * It is written in python, so automatically comes mistrust to this product. * Diffs are unreadable (I am aware of [nbdime](https://github.com/jupyter/nbdime), but another tool for diffs, really?) So, I discovered [Pandoc](https://github.com/jgm/pandoc). It is awesome and HTML support is excellent and highly customizable, but when it comes to PDF, it uses LaTeX. In my opinion, LaTeX engines is 40 years old ancient bulky piece of software and the only good part in it is its syntax for mathematical expressions. Of course, there are some ""modern"" LaTeX engines, but they suck, too. All LaTeX infrastructure seems weird for me and using it leads to committing a suicide (joke, do not worry :)). Firstly, I wanted to read specification of PDF format and try to write custom pandoc Lua writer for pdf. Then I realized that pandoc supports only text writers, not binary, isn't it? Then I discovered [KaTeX](http://katex.org/) and it's awesome. Then I discovered [CSS Paged Media Draft](https://www.w3.org/TR/css-page-3). It makes it possible to write PDF layout in CSS! And I thought it is a solution: it makes it possible to forget about LaTeX and write in indeed modern markup languages! But then I found out that there is only proprietary software that fully supports this standard and it costs a lot. (e.g. [PrinceXML](http://princexml.com)). Moreover, it does not support latest HTML and CSS features, so it sucks, too. There are two open-source solutions: [wkhtmltopdf](http://wkhtmltopdf.org) and [weasyprint](http://weasyprint.org), but they are outdated in terms of latest standards, too. I looked a bit more about different lightweight markup languages and found out about [Asciidoctor](https://asciidoctor.org). It is great for some reasons. For example, I was bribed by their cool [VS Code Extension](https://github.com/asciidoctor/asciidoctor-vscode) that being developed actively. But at the same time not as great as pandoc (pandoc flavored markdown gives you almost the same amount of control over the document as asciidoctor does). And there are some disadvantages: * AsciiDoc syntax is harder than Markdown * Their publishing stack seems a bit buggy. A good thing is that PDF files are made with native converter written in Ruby (bypassing LaTeX), but LaTeX math formulas are converted to SVG and pasted as images in PDF. It makes it impossible to search the document for text in formulas and sometimes SVGs are placed with wrong vertical alignment. Anyway, Ruby as any other interpreted and dynamically typed language isn't good choice for making stable and reliable software :) * Community is too small in [comparison](https://star-history.t9t.io/#jgm/pandoc&asciidoctor/asciidoctor) to pandoc's one. Anyway, despite having ~3k stars on GitHub, which is a lot, the only places that contain information on Asciidoctor is their [website](https://asciidoctor) and [GitHub Repo](https://github.com/asciidoctor/asciidoctor)! So, I switched back to pandoc and remembered why I went away from it. After two months of looking for the solution, I was so disappointed and desperate that I was going to switch to MS Word and do my homework there. But then... I discovered [Paged.js](https://www.pagedmedia.org/paged-js/) and now it seems like this is final solution! It brings CSS3 Paged Media into CSS2 and JS and makes it possible to use headless browser to produce PDF. ## Final workflow So, the final pipeline for me looks as the following:  All these things are going to be tested in [this](http://github.com/mitinarsenyHSE/statisticsHW) repo. ## Any ideas? If there is alternative way of doing it simpler and more straight-forward, please, join the discussion.",Why not use ConTeXt?,"Firstly, I tried [Jupyter](https://jupyter.org): both Jupyter Notebooks and JupyterLab. After two weeks of trying to create a nice-looking PDF from notebook, I decided to stop this masachism and look for the alternatives. Here are some of the reasons, why Jupyter Notebooks is not appropriate solution for me: * Jupyter Notebook is buggy. JupyterLab is very buggy. * It is written in python, so automatically comes mistrust to this product. * Diffs are unreadable (I am aware of [nbdime](https://github.com/jupyter/nbdime), but another tool for diffs, really?)"
jgm/pandoc,https://github.com/jgm/pandoc/issues/5976,jgm_pandoc_issues_5976,"2.8.0.1 latex -> markdown (all flavors) - blockquotes inconsistent rendering

Sorry if duplicate. ## 1. The following is from the Try Pandoc page: <img width=""716"" alt=""Screen Shot 2019-12-09 at 10 44 43 AM"" src=""https://user-images.githubusercontent.com/15252830/70463305-86001380-1a71-11ea-800c-d30cacb0b047.png""> The following is the command + version information from pandoc installed on macOS Catalina: pandoc -s HomelessNotHopeless.tex --wrap=none --reference-links -t markdown -o HomelessNotHopeless.md <img width=""599"" alt=""Screen Shot 2019-12-09 at 10 45 08 AM"" src=""https://user-images.githubusercontent.com/15252830/70463377-a4fea580-1a71-11ea-8a37-5c32f7c3ecb8.png""> ## 2. This is the result of that same text: <img width=""949"" alt=""Screen Shot 2019-12-09 at 10 50 31 AM"" src=""https://user-images.githubusercontent.com/15252830/70463427-bb0c6600-1a71-11ea-9d75-571ba2944e0e.png""> ## 2.1 Oddly enough, the next quote in that same document rendered like this (more like a table): <img width=""986"" alt=""Screen Shot 2019-12-09 at 10 51 47 AM"" src=""https://user-images.githubusercontent.com/15252830/70463523-e7c07d80-1a71-11ea-8de5-c472e6f01784.png""> All escaped right angle brackets (""\>"") can be ignored as I'm experimenting with the conversions to make sure they will work with for my purposes and were put there are on purpose.",Do you get something different?,## 1. ## 2. ## 2.1
jgm/pandoc,https://github.com/jgm/pandoc/issues/6018,jgm_pandoc_issues_6018,"{.unnumbered} tag cannot work at level 4 or lower?

Do you have any idea to solve this? I want to disappear the section number on the level4 of headline. The un-numbering command {-} or {.unnumberd} can work expectedly as follows below figures. However the number on the level4- of headline did not disappear unexpectedly. - input(test.md)  - output(test.pdf) ![image](https://user-images.githubusercontent.com/34906909/71551697-76506b00-2a30-11ea-9e3a-3fa4499310d3.png) - cmd: pandoc test.md -s -o test.pdf -N -F pandoc-crossref --pdf-engine=lualatex -V documentclass=ltjsarticle -V luatexjapresetoption=ms - pandoc: v2.9.1 P.S This pdf was produced by vscode-pandec addon @ VSCode",What command did you try with this input? What output did you get? What did you expect? What version of pandoc?,- input or higher#or lower\
diaspora/diaspora,https://github.com/diaspora/diaspora/issues/7887,diaspora_diaspora_issues_7887,"if I hashtag dandelion my post disappears down a black hole.

...and that is an exact description of my problem. Some strange things: - this occurs not just with the dandelion app, also posting to diaspora from both the desktop and mobile web client - if I reproduce the #dandelion in the exact format dandelion uses to add tags to the footer of a post then the problem doesn't occur. that is by using **Tags:** #dandelion (THIS TURNED OUT TO BE NOT FULLY ACCURATE - instead it's about the type of i in dandelion - see my next reply below) - reporducible in all instances of posting public, or limited aspects That points to a very strange filter rule operating at some level within the diaspora ecosystem. I am a new member in the pluspora pod. This is no hardship to workaround now I know what the problem is but I wasted quite a bit of time understanding why certain posts would seem to go through and then just fail to appear in my stream.",Can you please describe the exact steps you are following explaining what happens and what is expected **without using the dandelion app**? Thank you.,(THIS TURNED OUT TO BE NOT FULLY ACCURATE - instead it's about the type of i in dandelion - see my next reply below)
libretro/RetroArch,https://github.com/libretro/RetroArch/issues/9778,libretro_RetroArch_issues_9778,"[Feature Request] Having support for Windows Gaming Input API for joypads

## Description In the Windows PC ecosystem we're pretty used to two different API's when it comes to handling game controllers; DirectInput and Xinput. Xinput being the successor to dinput and the one that fully utilizes both the Xbox 360- and Xbox One gamepads still has its limits. One of them is that xinput is as far as I know Windows only, and the other one that it supports a maximum of 4 gamepads connected at once (as stated in [this Wikipedia article](https://en.wikipedia.org/wiki/DirectInput#XInput)). Thereby if you play games on PC with the Xbox Wireless Adapter on a Windows 10 PC, an adapter that supports 8 players simultaneously through its wireless connection, you have to select dinput in the games that supports more than 4 players if more than 4 people are going to play at the same time. I don't know much about the Gamepad API as of now, I suppose [this article](https://hacks.mozilla.org/2013/12/the-gamepad-api/) shines some light to its origins. This API has supposedly made it possible to play games through the web-browser with gamepads, and this API supports most modern web-browsers. I hope this API doesn't have the limit of max 4 gamepads connected at once, that it has all the benefits from xinput and dinput combined & more. **Edit:** Looks like I have mixed the [Windows Gaming Input API](https://docs.microsoft.com/en-us/uwp/api/windows.gaming.input) with the Gamepad API for web-browsers; with the first one I suppose are meant to be a successor to the xinput API in the long run. Maybe support for the Windows Gamepad Input API could have its benefits?",why not just use dinput for everything? it works for any number of pads you may have.,Edit: Looks like I have mixed the Gamepad API for web-browsers with the [Windows Gaming Input API](https://docs.microsoft.com/en-us/uwp/api/windows.gaming.input); with the latter one I suppose are meant to be a successor to the xinput API in the long run.
libretro/RetroArch,https://github.com/libretro/RetroArch/issues/7064,libretro_RetroArch_issues_7064,"[Feature Request] Wayland: Support for xdg-shell

_Update:_ closed by #7607 --- ## Description [wl_shell](https://wayland.freedesktop.org/docs/html/apa.html#protocol-spec-wl_shell) is deprecated in favor of [xdg-shell](https://cgit.freedesktop.org/wayland/wayland-protocols/tree/stable/xdg-shell):  In e.g. sway 1.0 (currently in [beta](https://github.com/swaywm/sway/issues/1735#issuecomment-431621738)) support for wl_shell has already been [removed](https://github.com/swaywm/sway/pull/1871) and therefore running RA in wayland context fails:  To be future-proof for wayland please add support for the successor xdg-shell.",Would this fix the input problems in KMS https://github.com/libretro/RetroArch/issues/7020 What about the GPU permission problems in KMS again?,and therefore running away in wayland context fails
libretro/RetroArch,https://github.com/libretro/RetroArch/issues/8081,libretro_RetroArch_issues_8081,"[Wayland+Vulkan] Core Dump in wl_egl_window_resize

## Description Commit dba9781f26d53548baa8e9552960ec8107db55c5 (PR #7992 [Wayland] Fix toggle fullscreen from @Sunderland93 ) leads to the core dump found below with sway master (compiled with wlroots master) and Vulkan video driver (does not dump with GL). Backtrace with debugging symbols: [file](https://github.com/libretro/RetroArch/files/2797651/gdb.txt)  The core dumps in second call of wl_egl_window_resize:  ### Bisect Results last good: 6ca9afbd577d5d5f4bb8cadf59c94180854a24b6 first bad: dba9781f26d53548baa8e9552960ec8107db55c5 ### Environment information Video driver: vulkan https://github.com/swaywm/sway/commit/1a1133dcc5fb03773dfc3df3af04325245f7d67awlroo https://github.com/swaywm/wlroots/commit/c41d01306de59235256d96902cced49a8eef15e9",Would you mind getting a backtrace with debugging symbols? @Sunderland93 Any ideas?,and Vulkan video driverVideo driver: vulkan
libretro/RetroArch,https://github.com/libretro/RetroArch/issues/8114,libretro_RetroArch_issues_8114,"[RA] build fails: undefined reference to 'gfx_ctx_mali_fbdev'

## Description Build fails for Vim (S905X) Mali target. ### Expected behavior Should build fine. ### Actual behavior  Build log -> https://pastebin.com/drC5qyLb ### Steps to reproduce the bug Update your lokal repo to commit 2bd70abfcdc737bb71b8aed9924a3ee146e58854 or later and try to compile it for Mali. ### Bisect Results This commit builds fine b2e8a7add2b3d7712c0160e5c7c636a2a440be37 <- latest working commit ### Environment information Latest LibreELEC 9.0 build system -> cross compile Package file: https://github.com/5schatten/LibreELEC.tv/blob/libreelec-9.0-rr/packages/5schatten/emulation-frontends/retroarch/package.mk",Can you find the first bad commit so we know when this started? I don't have any mali hardware to test.,Build log -> <- latest working commitPackage file:
libretro/RetroArch,https://github.com/libretro/RetroArch/issues/10381,libretro_RetroArch_issues_10381,"[gl/glcore/vulkan] RA menu uses 100% of GPU resources in fullscreen mode (G-sync related)

**Edit:** This is a G-sync related problem. I was mistaken in the OP when I thought that G-sync had no effect. When G-sync is enabled in Fullscreen mode or in Fullscreen+Windowed mode in the GPU driver settings, then the RetroArch menu will have this problem of 100% GPU usage. If G-sync is disabled at the driver level, then GPU usage by RA is normal. The only RA setting that seems to have some effect is ""Max swapchain images"" under Video -> Synchronization. With that setting at 3 (default) or 4, GPU usage is at 100%. When set at 1 or 2, then the GPU sits between 85-95% instead. Changing RA's v-sync swap interval setting has no effect at all. Even if v-sync is disabled in the RA video settings, the RA menu will still use 100% of the GPU, as long as G-sync is enabled at the video driver level. ### Description While sitting in the RetroArch menu, using either the gl, glcore, or vulkan video drivers, the _retroarch.exe_ process is using 100% of my GPU. This only happens when RA is in either exclusive fullscreen or windowed fullscreen mode. When using normal windowed mode, the GPU usage is greatly reduced (around 5-10% on average, although it will spike upward to a sustained 30-33% after a minute or so, even if RA is only sitting idle in the background). Once any piece of content is loaded, GPU usage drops to normal levels depending on the core in use. Then when the content is closed and the RA menu is reloaded, GPU usage spikes back up to 100% again. Alt-tabbing away from RA causes GPU usage to drop, until the window is refocused. This problem occurs with every RA menu driver (tested ozone, xmb, rgui, glui). It does not occur when using the D3D11 video driver. My GPU is a Nvidia GTX 980 Ti with the latest driver installed (445.75). My monitor is an Acer Predator with hardware G-sync support. ### Expected behavior The RetroArch menu should only need minimal GPU resources to operate. ### Actual behavior The RetroArch menu always uses 100% of GPU resources. ### Steps to reproduce the bug 1. Start RetroArch in fullscreen or windowed fullscreen mode, using the gl, glcore, or vulkan video drivers. 2. Check GPU usage with any hardware monitor (I am using Microsoft's Process Explorer v16.31) ### Bisect Results [Try to bisect and tell us when this started happening] ### Version/Commit You can find this information under Information/System Information - RetroArch: 1.8.5 stable / Mar 19 2020 / 532fd88e2f ### Environment information - OS: Windows 7 Pro 64-bit SP1 (all updates installed) - Compiler: MinGW 9.2.0 (64-bit) - Intel core i7 6700K CPU @4.00 GHz","Can you check and see if RA is set to ""prefer maximum performance"" or if there's some other personalized setting in the nVidia Control Panel that could be affecting this?","**Edit:** This is a G-sync related problem. I was mistaken in the OP when I thought that G-sync had no effect. When G-sync is enabled in Fullscreen mode or in Fullscreen+Windowed mode in the GPU driver settings, then the RetroArch menu will have this problem of 100% GPU usage. If G-sync is disabled at the driver level, then GPU usage by RA is normal. The only RA setting that seems to have some effect is ""Max swapchain images"" under Video -> Synchronization. With that setting at 3 (default) or 4, GPU usage is at 100%. When set at 1 or 2, then the GPU sits between 85-95% instead. Changing RA's v-sync swap interval setting has no effect at all. Even if v-sync is disabled in the RA video settings, the RA menu will still use 100% of the GPU, as long as G-sync is enabled at the video driver level. ****"
libretro/RetroArch,https://github.com/libretro/RetroArch/issues/8352,libretro_RetroArch_issues_8352,"Scrolling issues in glui

# First and foremost consider this: - Only RetroArch bugs should be filed here. Not core bugs or game bugs - This is not a forum or a help section, this is strictly developer oriented ## Description Scrolling bug in glui ### Expected behavior A bug if you scroll a menu the GUI turns black and returns and loops anytime ### Actual behavior But not the game is not lauched ### Steps to reproduce the bug 1. [First step] Lauch the Game 2. [Second step] And Press toggle menu 3. [and so on...] If you scroll anytime and it happened ### Bisect Results Glui/Bug [Try to bisect and tell us when this started happening] The other menu drivers are fine but requires the controller ![screenshot_2019-02-23-19-52-41](https://user-images.githubusercontent.com/37503397/53286134-cf15fe00-37a4-11e9-8690-168f837da542.png) ### Version/Commit You can find this information under Information/System Information - RetroArch: Latest Stable Version (Samsung Galaxy tab A 7.0 mali-400mp2) ### Environment information - OS: Android 5.1.1 ![screenshot_2019-02-23-19-52-38](https://user-images.githubusercontent.com/37503397/53286114-a130b980-37a4-11e9-9c66-c98fd88cdf02.png)",What RetroArch version are you using? Fwiw I was not able to reproduce this on desktop linux.,(Samsung Galaxy tab A 7.0 mali-400mp2)
libretro/RetroArch,https://github.com/libretro/RetroArch/issues/8735,libretro_RetroArch_issues_8735,"3DS cia builds crash after being started (since v1.7.7)

# First and foremost consider this: - Only RetroArch bugs should be filed here. Not core bugs or game bugs - This is not a forum or a help section, this is strictly developer oriented ## Description [Description of the bug] ### Expected behavior [What you expected to happen] ### Actual behavior Retroarch does launch, but crashes right after the message: 'Found display server=NULL' ### Steps to reproduce the bug 1. Installation cia files (from retroarch/cores folder) 2. Installation cia file (from cia folder) 3. Starting retroach - then crash ![20190510_101710](https://user-images.githubusercontent.com/39792666/57513583-df9f3480-730e-11e9-81c9-160e94445cb1.jpg) ### Bisect Results [Try to bisect and tell us when this started happening] ### Version/Commit You can find this information under Information/System Information - RetroArch: 1.7.7 stable and nightly cia builds up to and including 2019-05-10 ### Environment information - OS: Luma3DS v9.1 on N3DS XL","Which model does he use for testing? Also, could he send his config? There might be something set in his config differently from the default that doesn't trigger the crash?",Starting retroach - then crash !githubusercontentcom/39792666/57513477-9d75f300-730e-11e9-9aa1-24b6bcaef88c
libretro/RetroArch,https://github.com/libretro/RetroArch/issues/9445,libretro_RetroArch_issues_9445,"Framerate displays N/A when enabled when threaded video is on

## Description Framerate only displays N/A when enabled with threaded video turned on, it should show the current fps of the menu and game. ### Expected behavior Enabling framerate should display the current fps of the menu and game. ### Actual behavior Framerate only displays N/A when enabled with threaded video turned on. ### Steps to reproduce the bug 0. Enable ""threaded video"" in settings > video > threaded video 1. Go to ""Onscreen Display"" 2. Then ""Onscreen Notification"" 3. Then ""Display Framerate"" ### Bisect Results Did a fresh install of the 2019-09-06 nightly and it wasn't working. ### Version/Commit You can find this information under Information/System Information - RetroArch: b875a69 ### Environment information - OS: Horizon (Nintendo Switch) - Compiler: GCC (8.3.0) 64-bit",Does it start working if you disable threaded video in settings > video?,"0. Enable ""threaded video"" in settings > video > threaded video"
libretro/RetroArch,https://github.com/libretro/RetroArch/issues/9272,libretro_RetroArch_issues_9272,"(Menu) Menu doesn't hide when using Quick Menu > Resume

# First and foremost consider this: - Only RetroArch bugs should be filed here. Not core bugs or game bugs - This is not a forum or a help section, this is strictly developer oriented ## Description After opening retroarch's menu and playing while playing on any core, when I try to resume the game, it resumes in background. ### Expected behavior Menu should hide after resuming the gameplay. ### Actual behavior Menu blocks the view and game plays in the background. ![menu_on_top_while_running](https://user-images.githubusercontent.com/15956550/62964277-aa542800-bdd0-11e9-949f-448203c4bcf1.jpg) ### Steps to reproduce the bug 1. Launch a game 2. Open retroarch's menu 3. Resume the game ### Bisect Results Issue appeared here: 4d860c4d16c09cc6b7e06966089abd92d0bcc1e5 Last commit without the issue: 6482a81ef5e294d282b38eea63de19425b0fe836 ### Version/Commit You can find this information under Information/System Information - RetroArch: 7c9de7ec0f40128f27586167506e2872051633fc The bug must have been introduced today (9th august), can't tell the exact commit. **UPDATE:** I've tested da1ff54ddde8f01181abe03accc3faefd8825f3d from the day before (8th august) which doesn't have this problem. ### Environment information - OS: I'm on a development branch of recalbox, compiling for the rpi3. - Compiler: [In case you are running local builds]","Do you have Settings > User Interface > ""Pauuse when menu activated"" enabled?",(9th august). **UPDATE:** I've tested da1ff54ddde8f01181abe03accc3faefd8825f3d from the day before (8th august) which doesn't have this problem
Graylog2/graylog2-server,https://github.com/Graylog2/graylog2-server/issues/5036,Graylog2_graylog2-server_issues_5036,"Converting standalone Graylog server to cluster setup causes global inputs to fail

I have total three nodes A, B and C. Initially I was running graylog only on node A. Now, I am trying to have a graylog cluster with all three nodes A, B and C. This also works fine I can see the nodes belognign to graylog cluster as shown below: ![image](https://user-images.githubusercontent.com/38486100/44895667-fbc7ec00-ad12-11e8-8cda-0763bf38825c.png) ## Expected Behavior If global inputs were working in standalone mode it should also work in cluster setup. ## Current Behavior But the global inputs fails to start on the new nodes B and C and complains that the address is already in use as shown below: ![image](https://user-images.githubusercontent.com/38486100/44841620-6d476200-ac61-11e8-849d-dc4ec1c85c59.png) ## Your Environment <!--- Include as many relevant details about the environment you experienced the bug in --> * Graylog Version: 2.4.3 * Elasticsearch Version: 5.6.8 * MongoDB Version: 3.6.2 * Operating System: Red Hat Enterprise Linux Server 7.3 * Browser version: Chrome",Can you confirm that the port of that input is not already in use on Nodes B & C?,95667-fbc7ec00-ad12-11e8-8cda-0763bf38825c.png) ## Expected Behavior If global inputs were working in standalone mode it should also work in cluster setup. ## Current Behavior But the global inputs fails to start on the new nodes B and C and complains that the address is already in use as shown below: ![image](https://user-images.githubusercontent.com/38486100/448
Graylog2/graylog2-server,https://github.com/Graylog2/graylog2-server/issues/6161,Graylog2_graylog2-server_issues_6161,"Graylog causes an IllegalArguementException on Elasticsearch

<!--- Provide a general summary of the issue in the Title above --> Graylog causes an unexpected behaviour of Elasticsearch. ## Expected Behavior When I enter Search tab, I would like to be able to view Messages graph, Statistics and Quick values. ## Current Behavior For now, Graylog front-end app says that the back-end returned HTTP/1.1 500 Internal Server Error, but graylog.log does not contain any information about that event. Elasticsearch's log says much more: [2019-07-18T11:27:01,521][DEBUG][o.e.a.s.TransportSearchAction] [bmkjLh9] All shards failed for phase: [query] org.elasticsearch.ElasticsearchException$1: Fielddata is disabled on text fields by default. Set fielddata=true on [message] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead. The thing is: Even though tha Graylog server uses field param in query, elasticsearch throws an exception. Despite adding the properties as the log says to [message] field, it still doesn't work. ## Possible Solution I don't know possible solution for the actual code, however I would start looking for a problem in part where Graylog queries elasticsearch for [message] field. ## Steps to Reproduce (for bugs) <!--- Provide a link to a live example, or an unambiguous set of steps to --> <!--- reproduce this bug. Include code to reproduce, if relevant --> I have: 1. installed Graylog 3.0.2, 2. connected it to Elasticsearch, 3. entered messages on the left side menu, 4. chosen one of suboptions of 'messages': Quick Values, Statistics etc. I've just followed the tutorial and nothing more. It is just not working. ## Context Not much, Graylog works fine, but it is just a lack of the feature I'm expecting it to have. ## Your Environment * Graylog Version: 3.0.2 * Elasticsearch Version: 6.8.1 * MongoDB Version: 4.0.10 * Operating System: CentOS 7 x86_64 kernel: 3.10.0-957 * Browser version: Firefox 68.0, Chrome 75.0.3770.100",Could you please describe the steps you took to get this error message? It's a bit difficult to exactly reproduce the issue otherwise. Thanks!,"I have: entered messages on the left side menu,. chosen one of suboptions of 'messages': Quick Values, Statistics etc. I've just followed the tutorial and nothing more. It is just not working"
fabricjs/fabric.js,https://github.com/fabricjs/fabric.js/issues/2374,fabricjs_fabric.js_issues_2374,"Cursor of iText in RTL mode (originX == 'right')

EDIT (17/05/2020) ------------------------- Hello guyz, i have started to write the RTL version of iText, you can take the dist/favric.rtl.js in my forked repository, after i'll finish i will rewrite it and merge to fabric.js repository. [https://github.com/YaakovHatam/fabric.js](https://github.com/YaakovHatam/fabric.js) --------------------------------------- hello i made quick fix that allow the cursor behave like it should be in Right-to-Left texts. in the function:  i have replaces the code:  with:  and now it works proper! add: and for the selection the change is: in the function  instead  replace with",what language are you using?,"y forked repository, after i'll finish i will rewrite it and merge to fabric.js repository. [](https://github.com/YaakovHatam/fabric.js) hello i m"
Leaflet/Leaflet,https://github.com/Leaflet/Leaflet/issues/6632,Leaflet_Leaflet_issues_6632,"Zooming on map within iframe causes Samsung Galaxy to freeze map

**Situation** We need the map to capture pointer events to interact with 6.000+ canvas shapes, plotted on a Grid layer. We need to check for click events, mouse moves, etc. on the canvases. All works fine when the map is not loaded within an iframe. To capture pointer events, we have set:  .leaflet-tile-container{ pointer-events:visiblePainted; pointer-events:auto }  However, when the map is displayed within an iframe (with pointer-events set to auto) Samsung Galaxy devices cause the map to freeze after zooming in / out after a while (fast zooming causes the map to freeze quickly) On all other touch devices, except for Samsung devices, the map works just fine (also within an iframe). We have tested multiple Android and iOS devices (HTC One, OnePlus, iPhone 5, iPhone7 etc.) **Steps to reproduce** Steps to reproduce the behavior: - Step 1: Test on a Samsung Galaxy - Step 2: Open https://onlinebikelight.com/galaxy.html - Step 3: Start zooming in / out fast - ... Map freezes **Expected behavior** Expected behavior: Map shouldn't freeze. When the map is not loaded in an iframe but directly in the browser, like here: https://onlinebikelight.com/map.html The map won't freeze on a Samsung Galaxy **Current behavior** The map freezes when loaded within an iframe on a Samsung Galaxy, with pointer-events set to auto. **Environment** - Leaflet 1.4.0 - Browser: Chrome (all recent versions), Samsung stock browser - OS: Android (all recent versions) - Devices: Samsung Galaxy (tested on Galaxy S6, S8, S9) **Minimal example reproducing the issue** _IMPORTANT: Test on Samsung Galaxy!_ Works perfectly fine when zooming in / out in browser: https://onlinebikelight.com/map.html However, map freezes when embedded in iframe: https://onlinebikelight.com/galaxy.html - [x] this example is as simple as possible - [x] this example does not rely on any third party code **UPDATE** When the map is displayed in an iframe on a Samsung Galaxy device, but within **FireFox** instead of **Chrome** or **Samsung stock browser**, the map won't freeze. So the issue really has to do with: - Leaflet map with pointer-events: auto; - loaded within an <iframe> - on a **Samsung Galaxy** device - within **Chrome** or the **Samsung stock browser**",Does this happen as well on a non-samsung phone/tablet?,"**Update:** When the map is displayed in an iframe on a Samsung Galaxy device, but within **FireFox** instead of **Chrome**, the map won't freeze. So the issue really has to do with: - Leaflet map with pointer-events: auto; - loaded within an <iframe> - on a Samsung Galaxy device - within a Chrome browser"
mono/mono,https://github.com/mono/mono/issues/8127,mono_mono_issues_8127,"No BTLS in Windows builds

Add support for BTLS on Windows targeting both cygwin/mingw and msvc build runtimes. - [x] Build BTLS as part of cygwin/mingw build. #9306. - [x] Validate pass on basic BTLS tests using cygwin/mingw build (NOTE, by default CI uses msvc build runtime to run tests). #9306. - [x] Build BTLS as part of msvc build. #11100. - [x] Do a better integration between autoconf and msvc build. Implement logic to pickup build parameters from autoconf step (in this case BTLS but should be other configs as well) and add logic to build, use and link external repro's as part of msvc build without additional configuration. #11100. - [x] Add support for assembler BTLS builds on cygwin and msvc builds. #11234. - [x] Update Mono Windows build instructions. #11343, https://github.com/mono/website/pull/411 - [x] Update documentation related to additional build dependencies when build using BTLS. #11343, https://github.com/mono/website/pull/411",What's the status of this? Not having BTLS on Windows is blocking the Legacy TLS removal (https://github.com/mono/mono/issues/8559).,Add support for BTLS on Windows targeting both cygwin/mingw and msvc build runtimes. - [ ] Build BTLS as part of cygwin/mingw build. - [ ] Validate pass on all BTLS tests using cygwin/mingw build (
mono/mono,https://github.com/mono/mono/issues/16759,mono_mono_issues_16759,"Stream.ReadAsync continues on different thread

## Steps to Reproduce 1. Use code: await stream.ReadAsync (buf, 0, count); The above should continue on the same thread that called ReadAsync but seems to continue on a different thread. awaiting a Task.Run works as expected. Sample Xamarin.Mac project: https://github.com/mrward/TestStreamReadAsync Slightly more complicated console app with a custom synchronisation context. https://github.com/mrward/TestStreamReadAsyncConsoleApp Also affects VS Mac. ### Current Behavior After the await the thread has changed. awaiting a Task.Run works as expected. ### Expected Behavior After the await the thread is the same as the one that called ReadAsync. ## On which platforms did you notice this [x] macOS [ ] Linux [ ] Windows **Version Used**: Mono JIT compiler version 6.4.0.188 (2019-06/2c3aeaf3780 Tue Sep 3 15:10:54 EDT 2019) Mono JIT compiler version 6.0.0.327 (2019-02/f8ea05bddcb Sat Aug 10 18:47:12 EDT 2019)",What SynchronizationContext is this with?,Mono JIT compiler version 6.0.0.327 (2019-02/f8ea05bddcb Sat Aug 10 18:47:12 EDT 2019)
mono/mono,https://github.com/mono/mono/issues/10278,mono_mono_issues_10278,"[Mac] Run-Time Error - ""Could not resolve type with token 01000065...""

<!-- If you are new to the project get yourself familiar with https://www.mono-project.com/community/bugs/make-a-good-bug-report/ before filling the issue --> ## Steps to Reproduce 1. Upgrade to latest Stable... === Visual Studio Enterprise 2017 for Mac === Version 7.6 (build 2190) Installation UUID: 60c7c193-36a0-40cb-a1f6-65589da22877 Runtime: Mono 5.12.0.301 (2018-02/4fe3280bba1) (64-bit) GTK+ 2.24.23 (Raleigh theme) Xamarin.Mac 4.4.1.178 (master / eeaeb7e6) Package version: 512000301 === NuGet === Version: 4.3.1.4445 === .NET Core === Runtime: /usr/local/share/dotnet/dotnet Runtime Versions: 2.1.2 2.1.1 2.0.5 2.0.3 2.0.0 SDK: /usr/local/share/dotnet/sdk/2.1.302/Sdks SDK Versions: 2.1.302 2.1.301 2.1.4 2.0.3 2.0.0 MSBuild SDKs: /Library/Frameworks/Mono.framework/Versions/5.12.0/lib/mono/msbuild/15.0/bin/Sdks === Xamarin.Profiler === Version: 1.6.3 Location: /Applications/Xamarin Profiler.app/Contents/MacOS/Xamarin Profiler === Apple Developer Tools === Xcode 9.4.1 (14161) Build 9F2000 === Xamarin.Mac === Version: 4.6.0.13 (Visual Studio Enterprise) Hash: 373c313a Branch: Build date: 2018-07-24 23:47:12-0400 === Xamarin.iOS === Version: 11.14.0.13 (Visual Studio Enterprise) Hash: 373c313a Branch: HEAD Build date: 2018-07-24 23:47:12-0400 === Xamarin.Android === Version: 9.0.0.18 (Visual Studio Enterprise) Android SDK: /Users/dominique/Library/Developer/Xamarin/android-sdk-macosx Supported Android versions: 8.1 (API level 27) SDK Tools Version: 26.1.1 SDK Platform Tools Version: 27.0.1 SDK Build Tools Version: 27.0.3 Java SDK: /usr java version ""1.8.0_131"" Java(TM) SE Runtime Environment (build 1.8.0_131-b11) Java HotSpot(TM) 64-Bit Server VM (build 25.131-b11, mixed mode) Android Designer EPL code available here: https://github.com/xamarin/AndroidDesigner.EPL === Xamarin Inspector === Version: 1.4.3 Hash: db27525 Branch: 1.4-release Build date: Mon, 09 Jul 2018 21:20:18 GMT Client compatibility: 1 === Build Information === Release ID: 706002190 Git revision: 329690594b336b0b810b0399136d55eb54d292ec Build date: 2018-08-20 12:03:11+00 Build branch: release-7.6 Xamarin extensions: 23b59d33e3e5e6b7efa0f6d2d699867ab5082527 === Operating System === Mac OS X 10.13.6 Darwin 17.7.0 Darwin Kernel Version 17.7.0 Thu Jun 21 22:53:14 PDT 2018 root:xnu-4570.71.2~1/RELEASE_X86_64 x86_64 === Enabled user installed extensions === Internet of Things (IoT) development (Preview) 7.5.3 1. Grab latest from master - https://github.com/xamarin/Xamarin.PropertyEditing 2. Build and Run the Xamarin.PropertyEditing.Mac.Standalone default project. 3. Once the App starts, Click on either Mock 1 or Mock 2 buttons at the top and wait. 4. After 2-3 seconds the stand alone app crashes with... ### Current Behavior Crashes at run-time. ### Expected Behavior No run-time crash because previous stable version did NOT crash. ## On which platforms did you notice this [X] macOS [ ] Linux [ ] Windows **Version Used**: Mono JIT compiler version 5.12.0.301 (2018-02/4fe3280bba1 Fri Jul 20 08:25:42 EDT 2018) Copyright (C) 2002-2014 Novell, Inc, Xamarin Inc and Contributors. www.mono-project.com TLS: normal SIGSEGV: altstack Notification: kqueue Architecture: amd64 Disabled: none Misc: softdebug Interpreter: yes LLVM: yes(3.6.0svn-mono-master/8b1520c8aae) GC: sgen (concurrent by default) ## Stacktrace  <!-- You can join us at https://gitter.im/mono/mono to discuss your reported issue --> ## Workaround Disable Inspector (Visual Studio->Extensions->IDE Extensions-Xamarin Inspector Support) as it was loading the .NET Core 2 lib that was causing the crash",Can you help check this out?,## Workaround Disable Inspector (Visual Studio->Extensions->IDE Extensions-Xamarin Inspector Support) as it was loading the .NET Core 2 lib that was causing the crash
mono/mono,https://github.com/mono/mono/issues/10309,mono_mono_issues_10309,"Mono issuing SIGSEGV running C# Semantic Tests

## Steps to Reproduce This was found attempting to get the C# semantic tests [running on Mono](https://github.com/dotnet/roslyn/issues/28011) 1. Clone https://github.com/jaredpar/roslyn 1. Check out the branch repro/mono-semantic-crash2 1. Make sure mono is on your path 1. Run ./build.sh --restore --build 1. Run ./build.sh --test --mono ### Actual Behavior This will run the Semantic unit tests only and eventually Mono will crash with a SIGSEGV in the following function:  Full stack and dump info at the end of the report. Note: xunit runs tests in a random-ish order. Hence it's possible you will hit a different crash than this one. I pretty consistently saw this locally though. ### Expected Behavior The xunit process runs to completion likely with a number of failing tests. ## On which platforms did you notice this This was discovered on Ubuntu 18.04. I'm using the latest stable mono as described by the Download page https://www.mono-project.com/download/stable/#download-lin  ## Stacktrace and full details",Could you make sure to install gdb and mono-runtime-dbg and run again?,dll 7fd539964000-7fd539a3a000 r--p 00000000 08:02 2375327 /home/jaredpar/code/roslyn/Binaries/Debug/UnitTests/Microsoft.CodeAnalysis..CodeAnalysis/net46
mono/mono,https://github.com/mono/mono/issues/11939,mono_mono_issues_11939,"Orphaned mono-sgen64 processes occupy CPU

Starting with **Mono MDK 5.16.0.220**, there are numerous ""orphaned"" *mono-sgen64* processes using the CPU even after the project is built and all IDEs are closed. We have to force-quit these processes (or restart). **NOTE**: The issue was **not present** in Mono 5.12.0.309 ~~Warning: I'm afraid this going to be one of ""those"" issue reports. Try as I might, I cannot seem to isolate this bug in a simple test project, therefore I cannot provide steps to reproduce this outside the context of *our* project (*sigh* I know! I hate issue reports like this too), which is closed-source. That said, I am happy to furnish additional information.~~ ## Steps to Reproduce 1. Clone https://github.com/dbelcher/TestVBCompile 2. Build TestVBCompile. 3. Once the project is built, quit all applications including all IDEs, etc. Quit everything except Finder. 4. Open macOS's *Activity Monitor.app* and sort descending on the CPU column. ### Current Behavior Numerous *mono-sgen32* ~~garbage collector~~ processes are still active and occupying the CPU. Laptop fans are ON full blast. <img width=""637"" alt=""mono-sgen"" src=""https://user-images.githubusercontent.com/3045641/49490638-b6c9bf00-f805-11e8-8f09-9d863a1624f1.png""> ### Expected Behavior Since all builds are finished and all apps closed, no *mono-sgen32* processes are active nor appear in Activity Monitor. ## On which platforms did you notice this [X] macOS [ ] Linux [ ] Windows **Version Used**: *NOTE*: The following info is from Mono 5.16.0.221, which also exhibits this behavior. === Visual Studio Professional 2017 for Mac === Version 7.7 (build 1868) Installation UUID: 3152b549-c736-4969-9316-62a0762b3ffa GTK+ 2.24.23 (Raleigh theme) Xamarin.Mac 4.4.1.178 (master / eeaeb7e6) Package version: 516000221 === Mono Framework MDK === Runtime: Mono 5.16.0.221 (2018-06/b63e5378e38) (64-bit) Package version: 516000221 === NuGet === Version: 4.7.0.5148 === .NET Core === Runtime: /usr/local/share/dotnet/dotnet Runtime Versions: 2.1.2 2.1.1 2.0.5 2.0.3 2.0.0 1.1.1 1.0.4 SDK: /usr/local/share/dotnet/sdk/2.1.302/Sdks SDK Versions: 2.1.302 2.1.301 2.1.4 2.0.3 2.0.0 1.0.3 MSBuild SDKs: /Library/Frameworks/Mono.framework/Versions/5.16.0/lib/mono/msbuild/15.0/bin/Sdks === Xamarin.Profiler === Version: 1.6.4 Location: /Applications/Xamarin Profiler.app/Contents/MacOS/Xamarin Profiler === Apple Developer Tools === Xcode 10.1 (14460.46) Build 10B61 === Xamarin.Mac === Version: 5.2.1.11 (Visual Studio Professional) Hash: 5ef73d34 Branch: Build date: 2018-11-21 00:03:25-0500 === Xamarin.iOS === Version: 12.2.1.11 (Visual Studio Professional) Hash: 5ef73d34 Branch: d15-9 Build date: 2018-11-21 00:03:24-0500 === Xamarin.Android === Version: 9.1.0.38 (Visual Studio Professional) Android SDK: /Users/dan/Library/Developer/Xamarin/android-sdk-macosx Supported Android versions: 7.1 (API level 25) SDK Tools Version: 26.1.1 SDK Platform Tools Version: 27.0.1 SDK Build Tools Version: 26.0.3 Java SDK: /Users/dan/Library/Developer/Xamarin/jdk/microsoft_dist_openjdk_1.8.0.9 openjdk version ""1.8.0-9"" OpenJDK Runtime Environment (build 1.8.0-9-microsoft-b00) OpenJDK 64-Bit Server VM (build 25.71-b00, mixed mode) Android Designer EPL code available here: https://github.com/xamarin/AndroidDesigner.EPL === Android Device Manager === Version: 7.7.1.0 Hash: 06ceaea1 === Xamarin Inspector === Version: 1.4.3 Hash: db27525 Branch: 1.4-release Build date: Mon, 09 Jul 2018 21:20:18 GMT Client compatibility: 1 === Build Information === Release ID: 707001868 Git revision: 8bcdfa96d67c8c88dc45728cc968a2f84b55322f Build date: 2018-11-22 12:19:29+00 Build branch: release-7.7 Xamarin extensions: d66dbedcda237cd9b546abfecfeecafba6f8e3cb === Operating System === Mac OS X 10.14.1 Darwin 18.2.0 Darwin Kernel Version 18.2.0 Fri Oct 5 19:41:49 PDT 2018 root:xnu-4903.221.2~2/RELEASE_X86_64 x86_64 === Enabled user installed extensions === RhinoCommon Plugin Support 7.4.3.1","Can you see the command line of any of these, like with ""ps waux"" or similar? Can you attach a debugger to any of them?",", which is closed-source~~~~"
mono/mono,https://github.com/mono/mono/issues/14650,mono_mono_issues_14650,"Tarball includes Linux object files

(Edited by @lambdageek, original bug report below) The following object files (Linux 64-bit ELF, afaict) are included in our tarball, which causes Homebrew to build a bad libmono-native:  --- Given the popularity of the [Homebrew package manager](https://brew.sh/) on macOS (particularly amongst developers), and the fact that numerous existing Homebrew packages depend on the mono package, it seems obviously desirable that installing mono via Homebrew should provide a fully-functional install. This appears not to be the case at present: when mono is installed using Homebrew, System.IO.CoreFX.FileSystemWatcher seems to be broken. This was reported initially as [KSP-CKAN/CKAN#2630](https://github.com/KSP-CKAN/CKAN/issues/2630), which begat [Homebrew/homebrew-core#35848](https://github.com/Homebrew/homebrew-core/issues/35848), which has unfortunately been closed because of lack of expertise amongst any of the interested parties. It would be great if someone who understands how mono is built could assist in resolving this issue. ## Steps to Reproduce 1. Install the ckan homebrew package, which will cause the mono package to be installed if it is not already: $ brew install ckan 2. Run ckan: $ ckan ### Current Behaviour The program crashes with Unhandled Exception: System.InvalidOperationException: object_op ---> System.EntryPointNotFoundException: SystemNative_RealPath (full stack trace below). ### Expected Behaviour No crash, as is the case using the official .pkg: 1. Remove brewed mono package: $ brew remove mono --ignore-dependencies 2. [Download](https://www.mono-project.com/download/stable/) and install MonoFramework-MDK-5.20.1.19.macos10.xamarin.universal.pkg. 3. Open new Terminal window to pick up new paths. 4. Run ckan again: $ ckan 5. Observe that the error above does not occur (or at any rate, [an unrelated error](https://github.com/KSP-CKAN/CKAN/issues/2769) occurs later). ## On which platforms did you notice this [X] macOS [ ] Linux [ ] Windows **Version Used**: Installed via Homebrew (defective):  Installed manually via downloaded .pkg (works):  ## Stacktrace  ## Additional information Here is [the mono Homebrew formula](https://github.com/Homebrew/homebrew-core/blob/master/Formula/mono.rb). Building the package from source using brew install --build-from-source mono did not make any difference, so it does not appear to be merely a defective bottle.","Could we merge System.Native into mono runtime? That would probably fix this, and make developming mono easier, like it used to be.","(Edited by @lambdageek, original bug report below) The following object files (Linux 64-bit ELF, afaict) are included in our tarball, which causes Homebrew to build a bad libmono-native:  ---"
mono/mono,https://github.com/mono/mono/issues/16248,mono_mono_issues_16248,"mono_thread_set_name_internal needs to be optimized for multi-thread

### Current Behavior I profiled mono-netcore with AOT=LLVM on macOS with the following benchmark. https://github.com/TechEmpower/FrameworkBenchmarks/tree/master/frameworks/CSharp/aspnetcore/PlatformBenchmarks After discussing with the team, we found that mono_thread_set_name_internal needs to be optimized to improve the performance of Mono-netcore. We probably don't need to set a new name for every thread. <img width=""656"" alt=""Screen Shot 2019-08-14 at 3 47 35 PM"" src=""https://user-images.githubusercontent.com/52458914/63051370-202cc200-beab-11e9-9c79-93b85e74a760.png""> **Mono version used**: 36af44e3be4a0f73586e886b96f86ee500f8ab32 ## Steps to Reproduce Reproduce by following **Profiling workflow (Method 1)** from the following link: https://paper.dropbox.com/doc/Profile-Mono-on-macOS--Aijv~7jvs5XNRvKUtb_7UzTYAg-ryHeobv8okzUs1Fa9TfBG","How about just not setting it in a loop, but just once per thread?","### Current Behavior <img width=""656"" alt=""Screen Shot 2019-08-14 at 3 47 35 PM"" src=""https://user-images.githubusercontent.com/52458914/63051370-202cc200-beab-11e9-9c79-93b85e74a760.png""> **Mono version used**: 36af44e3be4a0f73586e886b96f86ee500f8ab32 ## Steps to Reproduce Reproduce by following **Profiling workflow (Method 1)** from the following link: https://paper.dropbox.com/doc/Profile-Mono-on-macOS--Aijv~7jvs5XNRvKUtb_7UzTYAg-ryHeobv8okzUs1Fa9TfBG"
mono/mono,https://github.com/mono/mono/issues/18096,mono_mono_issues_18096,"[metadata] Crash on Ubuntu, assertion at class-init.c:3930

## Steps to Reproduce 1. Clone a Mono local repository 2. Clone the test app repository from https://github.com/aspnet/AspNetCore 3. In the attached script, replace <MONO_GIT_ROOT> with the path to the root of your mono repository, and replace <TE_GIT_ROOT> with the path to the root of your test app repository 4. Need to install wrk, if you don't have it 5. Run the script assertion.sh [assertion.sh.zip](https://github.com/mono/mono/files/3941441/assertion.sh.zip) This is the assemblies of the test app which causing the crash: [PlatformBenchmarks.zip](https://github.com/mono/mono/files/3947256/PlatformBenchmarks.zip) ### Current Behavior * Assertion at class-init.c:3930, condition klass->instance_size == instance_size' not met ### Expected Behavior No crash. ## On which platforms did you notice this [ ] macOS [x ] Linux - Ubuntu 18.04.3 LTS [ ] Windows **Version Used**: 3d83c7429ef9e66697b254c569d78f5da7cb5b42 from Dec 9th, 2019 ## Stacktrace",Can it be reproduced on the last stable? I'm wondering if this is a recent regression or just a new bug.,This is the assemblies of the test app which causing the crash: [PlatformBenchmarks.zip](https://github.com/mono/mono/files/3947256/PlatformBenchmarks.zip)
mono/mono,https://github.com/mono/mono/issues/18371,mono_mono_issues_18371,"Investigate heavy stack from ConcurrentQueueSegment

## Steps to Reproduce Make the following change to mono/netcore/build.sh <img width=""512"" alt=""Screen Shot 2020-01-07 at 11 20 17 AM"" src=""https://user-images.githubusercontent.com/52458914/71911413-a50dc600-3141-11ea-92ae-4921d3ee337b.png""> Then run the following script by following the instructions in it. [profileMonoScript.sh.zip](https://github.com/mono/mono/files/4031516/profileMonoScript.sh.zip) ### Current Behavior Heavy stack from ConcurrentQueueSegment, according to the following flamegraph (the highlighted area) <img width=""1203"" alt=""Screen Shot 2020-01-07 at 10 29 24 AM"" src=""https://user-images.githubusercontent.com/52458914/71911627-0c2b7a80-3142-11ea-93cd-774cc21f90a8.png""> January 8 LLVM AOT flamegraph with ConcurrentQueueSegment<T>.TryDequeue highlighted: ![Screen Shot 2020-01-17 at 12 10 26](https://user-images.githubusercontent.com/480437/72631663-9930b980-3922-11ea-9f70-9280114f9799.png) ### Expected Behavior ConcurrentQueueSegment which does only few Volatile.Reads should take a small portion of time of the whole program. ## On which platforms did you notice this [ ] macOS [x] Linux [ ] Windows **Version Used**: mono master: 61eb2be2e10c228c5915d90a90b52e7d410c0f25 from Dec 30th, 2019",Can you make sure I'm not missing something? @fanyang-mono,January 8 LLVM AOT flamegraph with ConcurrentQueueSegment<T>.TryDequeue highlighted: ![Screen Shot 2020-01-17 at 12 10 26](https://user-images.githubusercontent.com/480437/72631663-9930b980-3922-11ea-9f70-9280114f9799.png)
getsentry/sentry,https://github.com/getsentry/sentry/issues/16761,getsentry_sentry_issues_16761,"Unable to view projects and errors on fresh docker desktop deployment

<!-- Do you want to ask a question? Are you looking for support? The Sentry message board is the best place for getting support: https://forum.sentry.io --> ## Important Details How are you running Sentry? * [x] On-Premise docker [10.1.0.dev0 1e17bb4] * [ ] Saas (sentry.io) * [ ] Other [briefly describe your environment] ## Description Unable to view projects and errors on fresh docker desktop deployment ## Steps to Reproduce 1. Configured settings: sentry.conf.py:  config.yml  2. Run install.sh 3. On docker *_web ran sentry upgrade 4. Login to http://localhost:9000/ ![image](https://user-images.githubusercontent.com/6367998/73600863-c38a9580-4524-11ea-88d3-efa21138ca0e.png) Good items to include here include:  - Include a stacktrace or other logs when relevant - Include a redacted version of your configuration when relevant ### What you expected to happen The project i created and the event that should be posted to appear ### Possible Solution Might be related to #16723 Similar problem when looking at Projects tab: ![image](https://user-images.githubusercontent.com/6367998/73600887-3f84dd80-4525-11ea-8020-942a656aad53.png)",Why are you changing these configuration values? Seems like you are not running Snuba or it is unreachable to Sentry?,* [ ] Other [briefly describe your environment]
rundeck/rundeck,https://github.com/rundeck/rundeck/issues/5368,rundeck_rundeck_issues_5368,"""Run this job again with the same options"" does not run the job again with the same options.

Execution > Run Again: caption text: ""Run this job again with the same options"" does not run the job again with the same options. * Rundeck version: Rundeck 3.1.0-20190731 * install type: docker image rundeck/rundeck:3.1.0 * OS Name/version: ? * DB Type/version: postgres **To Reproduce** Steps to reproduce the behavior: - Create a job with no options - Trigger the job via an api call to rundeck. In the call, set option ""foo"" to value ""bar"". - Open execution details: https://rundeck.acme.com/project/xyz/execution/show/123 - Observe that rundeck displays that the job was executed with option ""foo"" set to value ""bar"". - Observe that rundeck displays as hover text of the Run Again button: ""Run this job again with the same options"". - Click on ""Run Again"" > ""Run Job Now"" **Actual behaviour** The job is run again **without any** options. **Expected behavior** The job is run again with the option ""foo"" set to value ""bar"".",Would you be able to update the reproduction steps in the issue template with the exact steps required to reproduce and attach a sample job if available? Thanks!,"- Create a job with no options - Trigger the job via an api call to rundeck. In the call, set option ""foo"" to value ""bar"". - - Observe that rundeck displays that the job was executed with option ""foo"" set to value ""bar"". - Observe that rundeck displays as hover text of the Run Again button: ""Run this job again with the same options"". -"
kivy/kivy,https://github.com/kivy/kivy/issues/6420,kivy_kivy_issues_6420,"Colors changing in Kivy Scrollview

### Versions * Python: 3.7.4 * OS: Windows 10 * Kivy: latest version (fresh install as of 7/16/2019) * Kivy installation method: pip install ### Description I created some simple lines in between the buttons within my scrollview but the brightness of the lines seem to fluctuate when you drag the scroll bar up and down in various places. This issue occurs when you set the HEIGHT of the scrollview object to anywhere in between 501 and 999 pixels. However, when you change the height of the scrollview object to say, exactly 500 or 1000 pixels, the issue stops and the colors stay consistent. I need the red color lines to be consistent throughout all rows, no matter what the height of the scrollview is. Does anyone know what is causing this issue and how to fix it? I've experienced this issue on 3 separate computers. One with Windows 10 and the other 2 with Windows 7. ### Code and Logs",Can you post the full kivy log? Maybe take some screenshots to show the issue?,3 separate computers. One with Windows 10 and the other
kivy/kivy,https://github.com/kivy/kivy/issues/6708,kivy_kivy_issues_6708,"Exponential window size & memory allocation growth + crash when scaling window on Retina OS X

<!-- The issue tracker is a tool to address bugs. Please use the #support Discord channel at https://chat.kivy.org/ or Stack Overflow for support questions, more information at https://git.io/vM1yQ. Before opening a new issue, make sure you do the following: * check that your issue isn't already filed: https://git.io/vM1iE * prepare a short, runnable example that reproduces the issue * reproduce the problem with the latest development version of Kivy * double-check that the issue is indeed a bug and not a support request --> ### Versions * Python: 3.7.5 * OS: 10.12.6 * Kivy: 1.11.1 * Kivy installation method: pip ### Description **What are you trying to get done?** Lock resizing of a window to an aspect ratio on an OS X desktop **What has happened? / What went wrong?** Resizing causes: 1. a hang 2. exceptions mentioning failed attachment and very large & negative texture sizes 3. (usually) a crash. Items are also placed incorrectly on resize. Listening in on resize events shows exponential growth of the window size with each repeated resize event fired when dragging the window edge:  **What stops it** Dividing window size by two before setting it. My suspicion is that this is a runaway exponentiation version of the same issue as faced [here](https://github.com/ElliotGarbus/KivyWindowSize): Given that element placement is also affected, a number of other OS X specific issues may be related: - #6144 - #6446 There is also a [closed issue](https://github.com/kivy/kivy/issues/6407) by the author of the quote above that _may_ be a reference to this issue, but it's unclear whether the fix was merged for whatever problem was encountered. ### Code and Logs Switchable crasher:  Example errors:",Can you try using Window.system_size (list of system width/height) to see if you replicate it too?,metrics import Metrics from kivy.
videojs/video.js,https://github.com/videojs/video.js/issues/4758,videojs_video.js_issues_4758,"Captions menu doesn't show up in IE when having longer caption labels

## Description When using longer caption labels, the captions/subtitles menu doesn't render correctly in IE/Edge so a user can't select them. Sample test case: https://codepen.io/ttom/pen/rYJBrM ## Steps to reproduce 1. Load the player in IE/Edge 2. Start playback and click on captions/subtitle button in the control bar ## Results ### Expected Captions/subtitles is displayed menu with a list of captions ### Actual An empty menu with scrollbars appear http://take.ms/MGe1W ### versions #### videojs 6.x/7.x #### browsers IE11/Edge #### OSes Windows 7/8.1/10","Can you submit a PR? If not, I can help out - I think this fix may help with my issue with #4599's layout.","When using longer caption labels, the captions/subtitles menu doesn't render correctly in /Edge menu with a list of captions /7.x/Edge"
videojs/video.js,https://github.com/videojs/video.js/issues/6320,videojs_video.js_issues_6320,"Any custom skin has broken time/duration/remaining output

## Description If I use any skin other than the default one I get broken time in control bar. I primarily used the Skin Generator . ## Steps to reproduce To test you can create a skin there and try using it. Explain in detail the exact steps necessary to reproduce the issue. 1. Generate a skin in Skin Generator and use it ## Results ![Screenshot_2019-11-16](https://user-images.githubusercontent.com/16373480/68990977-f5f7f100-0859-11ea-98b4-a5ac6a9eb096.png) ### Expected The timer should be inline ### Actual The timer displays vertically ### Error output No programmatic error ## Additional Information ### versions #### videojs what version of videojs does this occur with? 7.4.1 7.5.5 I checked on these #### browsers All #### OSes All (maybe) ### plugins Seek Buttons Generator https://www.scriptsmashup.com/Video_Skin_Generator/Videojs/videojs-skin-generator.html (Found in wiki)",Which skin generator? I'd suspect it's designed for an older version of Video.js,Generator https://www.scriptsmashup.com/Video_Skin_Generator/Videojs/videojs-skin-generator.html (Found in wiki)
readthedocs/readthedocs.org,https://github.com/readthedocs/readthedocs.org/issues/5304,readthedocs_readthedocs.org_issues_5304,"Incompatible dependency for prospector with pylint-django

When installing requirements/lint.txt I got the error prospector 1.1.6.2 has requirement pylint-django==2.0.2, but you'll have pylint-django 2.0.4 which is incompatible.",Can you please give more details? Where are you seeing this problem? What error is this causing?,"llng requirements/s requirement pylint-django==2.0.2, buve ich is incompible."
rstudio/rstudio,https://github.com/rstudio/rstudio/issues/6594,rstudio_rstudio_issues_6594,"RStudio 1.3 hangs with ""lt_LT_new"" dictionary (Qt QtWebEngineProcess)

### System details RStudio Edition : Desktop RStudio Version : 1.3.938-2 Preview (2020-04-07) OS Version : Windows 10 R Version : 3.6.3 ### Steps to reproduce the problem 1) I had RStudio 1.2.5033 installed. 2) I updated RStudio to 1.3.938. 3) I went to Tools → Global Options → Spelling 4) I downloaded additional dictionaries. 5) I chose the dictionary ""lt_LT_new"", enabled real-time spellchecking (see Fig. 1) and pressed ""OK"" to exit the Global Options pane. 6) RStudio hanged. I had to use Task Manager to kill RStudio. But after RStudio was restarted, the problem still persisted. **Update:** RStudio hangs even if real-time spellchecking is not enabled. ![image](https://user-images.githubusercontent.com/12725868/79246804-57ac9d80-7e82-11ea-95c6-2788844052f3.png) **Fig. 1.** Modified options. ### Describe the problem in detail With these settings, RStudio opens and panes are loaded but the program is frozen. I can do nothing, it is even impossible to write to the console. The Windows _Task Manager_ shows that RStudio uses a lot of computer resources (see the list of processes below). The situation did not change in 30 minutes when RStudio was opened. It seems that the problem may be related to the dictionary ""lt_LT_new"" and ""Qt Qtwebengineprocess"": ![image](https://user-images.githubusercontent.com/12725868/78423860-fc083600-7671-11ea-837a-f006341d444c.png) **Fig. 2.** Processes in Task Manager. **Notes:** 1) When I roll back to 1.2.5033, RStudio works normally again. ""Qt Qtwebengineprocess"" takes only 141.4MB and power usage is ""Very low"". 2) If I install RStudio 1.3 again, the problem persists until I change the settings in %APPDATA%\Roaming\RStudio\rstudio-prefs.json from:  to:  and restart RStudio. lt_LT does not support real-time spell checking. But some other dictionaries (English US, German) do not cause the issue with real-time spellchecking enabled. ### Describe the behavior you expected I expect real-time spellchecking works well with Lithuanian language and RStudio does not hang when this functionality is used. <details> <summary>Issue checklist</summary> <!-- Please keep the below portion in your issue, and check [x] the applicable boxes. --> - [x] I have read the guide to submitting good bug reports at https://github.com/rstudio/rstudio/wiki/Writing-Good-Bug-Reports . - [x] I have installed the latest version of RStudio and confirmed that the issue still persists. - [ ] If I am reporting an RStudio crash, I have included a diagnostics report. https://support.rstudio.com/hc/en-us/articles/200321257-Running-a-Diagnostics-Report - [x] I have done my best to include a minimal, self-contained set of instructions for consistently reproducing the issue. </details>",Does switching to software rendering make a difference? https://support.rstudio.com/hc/en-us/articles/360017886674-Troubleshooting-RStudio-Rendering-Errors,"the dictionary ""lt_LT_new"" ands 1)2) If I install RStudio 1.3 again, the problem persists until I change the settings in %APPDATA%\Roaming\RStudio\rstudio-prefs.json from:  to:  and restart RStudio. lt_LT does not support real-time spell checking. But some other dictionaries (Englush US, German) do not cause the issue.does not hang"
rstudio/rstudio,https://github.com/rstudio/rstudio/issues/4521,rstudio_rstudio_issues_4521,"Figure size in RStudio is different than in rendered output

### System details RStudio Edition : Desktop RStudio Version : 1.3.74 OS Version : macOS 10.13.6 R Version : 3.5.1 ### Steps to reproduce the problem Create a new .Rmd file with this content: {r setup, include=FALSE} knitr::opts_chunk$set(echo = TRUE) {r mtcars, fig.width = 2, fig.height = 1} library(ggplot2) ggplot(mtcars, aes(wt, mpg)) + geom_point()  Run the chunk to see the output inside RStudio. Knit the document to see the output in the rendered HTML. ### Describe the problem in detail The size of the figure in the RStudio interface is different than the size in the rendered output. I spent time trying to adjust the fig.width and fig.height so the figure looks good when I run the chunk in RStudio. It turns out my time is wasted because the rendered output does not use the same figure size as the figured in RStudio (see screenshot below). ### Questions - How are users supposed to size their figures properly? - Are we meant to re-knit the file each time we tweak the fig.width and fig.height parameters? - If so, notice that the user must ignore the output as shown in RStudio while tweaking chunk options, since they will never share RStudio output with others and it is not representative of the knitted output. ![image](https://user-images.githubusercontent.com/209714/55044514-fd3f6580-5010-11e9-969d-25db07b002eb.png)",How else would you expect it to work? you're not setting a pre-defined size in terms of pixels for your plot in there.,"### Questions - - - If so, notice that the user must ignore the output as shown in RStudio while tweaking chunk options, since they will never share RStudio output with others and it is not representative of the knitted output."
rstudio/rstudio,https://github.com/rstudio/rstudio/issues/3617,rstudio_rstudio_issues_3617,"unable to import any theme into Rstudio preview 1.2.1015 in windows 10

System details RStudio Edition : Desktop Preview RStudio Version : 1.2.1015 OS Version : Windows 10 R Version : 3.5.1 Installed as admin user. Develop as unprivileged user Rstudio does not accept any new theme import. ### Steps to reproduce the problem 1- choose any theme from from https://tmtheme-editor.herokuapp.com/ 2. Tools > Global options > appearance > add theme 3. choose tmTheme file and accept 4. Dialog blinks and ""Textmate (default)"" is highlighted. No new teme is available. No error is shown. R version 3.5.1 (2018-07-02) Platform: x86_64-w64-mingw32/x64 (64-bit) Running under: Windows >= 8 x64 (build 9200) Matrix products: default locale: [1] LC_COLLATE=Spanish_Spain.1252 LC_CTYPE=Spanish_Spain.1252 LC_MONETARY=Spanish_Spain.1252 [4] LC_NUMERIC=C LC_TIME=Spanish_Spain.1252 attached base packages: [1] stats graphics grDevices utils datasets methods base other attached packages: [1] webshot_0.5.0 highlight_0.4.7.2 loaded via a namespace (and not attached): [1] Rcpp_0.12.18 packrat_0.4.9-3 digest_0.6.16 rprojroot_1.3-2 jsonlite_1.5 backports_1.1.2 magrittr_1.5 [8] evaluate_0.11 xaringan_0.7.1 stringi_1.1.7 xml2_1.2.0 rmarkdown_1.10 tools_3.5.1 stringr_1.3.1 [15] xfun_0.3 yaml_2.2.0 rsconnect_0.8.8 compiler_3.5.1 htmltools_0.3.6 knitr_1.20","Can you also share the output of normalizePath(""~/"")? Do you have any non-ASCII characters in your user name? I wonder if we might be running afoul of some encoding issue.",Installed as admin user. Develop as unprivileged user
rstudio/rstudio,https://github.com/rstudio/rstudio/issues/4497,rstudio_rstudio_issues_4497,"Cannot view typed text in text field when renaming a file in files pane

<!-- RStudio v1.2 is currently available as a preview release, from https://www.rstudio.com/products/rstudio/download/preview/. If you are reporting a bug with RStudio v1.1, it would be greatly appreciated if you could check if the issue also reproduces in the preview release before reporting an issue. This issue tracker is for bugs and feature requests in the RStudio IDE. If you're having trouble with R itself or an R package, see https://www.r-project.org/help.html, and if you want to ask a question rather than report a bug, go to https://community.rstudio.com/. Finally, if you use RStudio Server Pro, get in touch with our Pro support team at support@rstudio.com. See our guide to writing good bug reports for more details: https://github.com/rstudio/rstudio/wiki/Writing-Good-Bug-Reports --> ### System details RStudio Edition : <!-- Desktop or Server --> RStudio Version : 1.2.1330 OS Version : Mojave 10.14.13 <- typo Mojave 10.14.3 R Version : ### Steps to reproduce the problem 1. Open a project in RStudio 1. In the file viewer pane, check the box to select a file and click ""Rename"" 1. Start typing the free text field that pops up ### Describe the problem in detail When I try to rename a file in the RStudio files pane, I cannot see the text as I type in the free text field. See screenshot, where you can see the cursor is to the right (I was typing ""index"" but no text is visible): <img width=""1312"" alt=""Screen Shot 2019-03-22 at 1 01 48 PM"" src=""https://user-images.githubusercontent.com/12160301/54850011-24113b00-4ca3-11e9-98bf-b52ffbda50f7.png""> ### Describe the behavior you expected In previous versions I could always see the text as I typed in this field and I use this feature a lot so it is a new-ish issue. <!-- Depending on the problem, the following may also be helpful 1. The output of sessionInfo() 2. The R code in question 3. A diagnostics report; see https://support.rstudio.com/hc/en-us/articles/200321257-Running-a-Diagnostics-Report Thank you for taking the time to file an issue! -->",What does sw_vers return for you in the Terminal? I'm on: ProductName: Mac OS X ProductVersion: 10.14.4 BuildVersion: 18E220a Thanks for reporting!,3 <- typo Mojave 10.14.
rstudio/rstudio,https://github.com/rstudio/rstudio/issues/5132,rstudio_rstudio_issues_5132,"Show the interesting source code when click + Ctrl on a function created with purrr::partial()

When clickling on a bare function in the editor pane and hitting Ctrl (or cmd on macOs), RStudio brings me to that function declaration. However, the linking for functions created with purrr::partial() brings me somewhere else. Assume this code in the editor pane: ![Screen Shot 2019-07-21 at 15 46 49](https://user-images.githubusercontent.com/10477073/61592099-c51aee80-abce-11e9-9c1a-dc5731e50ed4.png) Executing the code, pressing Ctrl and clicking on sum_ and at the same time, then releasing Ctrl, RStudio opens a new file and shows me this: ![Screen Shot 2019-07-21 at 15 47 31](https://user-images.githubusercontent.com/10477073/61592104-dd8b0900-abce-11e9-8877-6da438e08a7d.png) It might be technically consistent to show this, but it isn't of any use. It appears to be the same whenever the function of interest is created with purrr::partial(). Can we change the linking to display the call where the function was declared?",How can we retrieve the 'real' body of these functions generated by purrr::partial?,/or cmd on macOs)
pyrocms/pyrocms,https://github.com/pyrocms/pyrocms/issues/4809,pyrocms_pyrocms_issues_4809,"Guest role does not work to restrict the pages for registered users.

### EDIT: There is no problem about the guest role, it was me who could not do it. There is a request down in the page from another user. I wanted to create a register page which is not accessable by any user, but guests. So I created a page from the panel and in the ""Options"" tab, I assigned ""Guest"" role in ""Allowed Roles"" section. But this resulted in me being able to see the page when I am logged in only. In the docs, it says So I went to ServiceProviders of Users Module and added the following, 'register' => [ 'as' => 'anomaly.module.users::register', **'anomaly.module.users::role' => 'guest',** 'uses' => 'Anomaly\UsersModule\Http\Controller\RegisterController@register', ], which also did not work. Could you guide me about this? I also read in github this solution is fixed in another issue but I can't make it work.",What issue was that @mkaanery ?,"### EDIT: There is no problem about the guest role, it was me who could not do it. There is a request down in the page from another user."
Particular/NServiceBus,https://github.com/Particular/NServiceBus/issues/5279,Particular_NServiceBus_issues_5279,"reply message dies at handler (Nothing happens)

I have a well-functioning server/host; version 5.2.21; and i'm trying to upgrade our client's to version 7.1.4. Everything seems to work until the point where the host replies to the clients messages. The message reaches the clients handler, and then nothing more happens. It's pretty straight forward: **Client:**  This yields the following in the debugging: **Send**  **Reply**  As you see; it invokes ""ResponseMessageHandler"" looking like this:  This renders the following in the log (Straight after the upper log entries)  And then nothing else happens; it just stops the thread and wont deliver any result to my ""_task"" variable.",What happens if you remove the handler?,c# txt txt  txt  
socketio/socket.io,https://github.com/socketio/socket.io/issues/3540,socketio_socket.io_issues_3540,"sockjs-node errors

I just created a tiny test app with vue cli, so without adding anything, apart from what the empty vue-cli scaffolding brings. (base) marco@pc:~/vueMatters/testproject$ npm run serve testproject@0.1.0 serve /home/marco/vueMatters/testproject vue-cli-service serve INFO Starting development server... 98% after emitting CopyPlugin DONE Compiled successfully in 1409ms 8:14:46 PM With this /etc/nginx/conf.d/default.conf : server { listen 443 ssl http2 default_server; server_name ggc.world; ssl_certificate /etc/ssl/certs/chained.pem; ssl_certificate_key /etc/ssl/private/domain.key; ssl_session_timeout 5m; ssl_protocols TLSv1 TLSv1.1 TLSv1.2 TLSv1.3; ssl_ciphers EECDH+CHACHA20:EECDH+CHACHA20-draft:EECDH+AES128:RSA+AES128:EECDH+AES256:RSA+AES256:EECDH+3DES:RSA+3DES:!MD5; ssl_prefer_server_ciphers on; ssl_session_cache shared:SSL:50m; ssl_dhparam /etc/ssl/certs/dhparam.pem; #ssl_stapling on; #ssl_stapling_verify on; access_log /var/log/nginx/ggcworld-access.log combined; add_header Strict-Transport-Security ""max-age=31536000""; location = /favicon.ico { access_log off; log_not_found off; } location / { proxy_pass http://127.0.0.1:8080; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection ""upgrade""; } } server { listen 80 default_server; listen [::]:80 default_server; error_page 497 https://$host:$server_port$request_uri; server_name www.ggc.world; return 301 https://$server_name$request_uri; access_log /var/log/nginx/ggcworld-access.log combined; add_header Strict-Transport-Security ""max-age=31536000""; location = /favicon.ico { access_log off; log_not_found off; } location / { proxy_pass http://127.0.0.1:8080; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection ""upgrade""; } } # https://www.nginx.com/blog/nginx-nodejs-websockets-socketio/ # https://gist.github.com/uorat/10b15a32f3ffa3f240662b9b0fefe706 # http://nginx.org/en/docs/stream/ngx_stream_core_module.html upstream websocket { ip_hash; server localhost:3000; } server { listen 81; server_name ggc.world www.ggc.world; #location / { location ~ ^/(websocket|websocket\/socket-io) { proxy_pass http://127.0.0.1:4201; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection ""upgrade""; proxy_set_header X-Forwared-For $remote_addr; proxy_set_header Host $host; proxy_redirect off; proxy_set_header X-Real-IP $remote_addr; } } # https://stackoverflow.com/questions/40516288/webpack-dev-server-with-nginx-proxy-pass with vue.config.js : module.exports = { // options... publicPath: '', devServer: { host: 'localhost', } } and with this webpack.config.js : { ""mode"": ""development"", ""entry"": [ ""src/index.js"", ""webpack-dev-server/client?http://"" + require(""os"").hostname() + "":3000/"" ], ""output"": { ""path"": __dirname+'/static', ""filename"": ""[name].[chunkhash:8].js"" }, ""module"": { ""rules"": [ { ""test"": /\.vue$/, ""exclude"": /node_modules/, ""use"": ""vue-loader"" }, { ""test"": /\.pem$/, ""use"": ""file-loader"" } ] }, plugins: [ new BrowserSyncPlugin( { host: 'localhost', port: 3000, proxy: 'http://localhost:8080' }, { reload: false } ), ], node: { __dirname: false, __filename: false }, resolve: { extension: ['*', '.pem'] }, devServer: { watchOptions: { aggregateTimeout: 300, poll: 1000 } } } Get this error message: GET https://localhost/sockjs-node/info?t=1580397983088 net::ERR_CONNECTION_REFUSED ![sockejsError13](https://user-images.githubusercontent.com/3073209/73527082-baa99f00-4412-11ea-8c81-ebbc8a84fce4.jpg) - Ubuntu 18.04.03 Server Edition - (base) marco@pc:~$ node -v v12.10.0 - npm -v 6.13.6 - webpack-cli@3.3.10 - vue --version @vue/cli 4.1.2 - (base) marco@pc:~$ nginx -v nginx version: nginx/1.14.0 (Ubuntu) - From within ghe PC-Server: (base) marco@pc:~$ curl -Iki https://localhost/sockjs-node/info?t=1580397983088 HTTP/2 405 server: nginx/1.14.0 (Ubuntu) date: Fri, 31 Jan 2020 08:19:02 GMT allow: OPTIONS, GET (base) marco@pc:~/vueMatters/testproject$ curl -vk https://localhost/sockjs- node/info?t=1580397983088 * Trying ::1... * TCP_NODELAY set * connect to ::1 port 443 failed: Connection refused * Trying 127.0.0.1... * TCP_NODELAY set * Connected to localhost (127.0.0.1) port 443 (#0) * ALPN, offering h2 * ALPN, offering http/1.1 * successfully set certificate verify locations: * CAfile: /etc/ssl/certs/ca-certificates.crt CApath: /etc/ssl/certs * TLSv1.3 (OUT), TLS handshake, Client hello (1): * TLSv1.3 (IN), TLS handshake, Server hello (2): * TLSv1.3 (IN), TLS Unknown, Certificate Status (22): * TLSv1.3 (IN), TLS handshake, Unknown (8): * TLSv1.3 (IN), TLS Unknown, Certificate Status (22): * TLSv1.3 (IN), TLS handshake, Certificate (11): * TLSv1.3 (IN), TLS Unknown, Certificate Status (22): * TLSv1.3 (IN), TLS handshake, CERT verify (15): * TLSv1.3 (IN), TLS Unknown, Certificate Status (22): * TLSv1.3 (IN), TLS handshake, Finished (20): * TLSv1.3 (OUT), TLS change cipher, Client hello (1): * TLSv1.3 (OUT), TLS Unknown, Certificate Status (22): * TLSv1.3 (OUT), TLS handshake, Finished (20): * SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384 * ALPN, server accepted to use h2 * Server certificate: * subject: CN=ggc.world * start date: Nov 30 11:22:10 2019 GMT * expire date: Feb 28 11:22:10 2020 GMT * issuer: C=US; O=Let's Encrypt; CN=Let's Encrypt Authority X3 * SSL certificate verify ok. * Using HTTP2, server supports multi-use * Connection state changed (HTTP/2 confirmed) * Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0 * TLSv1.3 (OUT), TLS Unknown, Unknown (23): * TLSv1.3 (OUT), TLS Unknown, Unknown (23): * TLSv1.3 (OUT), TLS Unknown, Unknown (23): * Using Stream ID: 1 (easy handle 0x559bc64c5580) * TLSv1.3 (OUT), TLS Unknown, Unknown (23): * TLSv1.3 (IN), TLS Unknown, Certificate Status (22): * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4): * TLSv1.3 (IN), TLS Unknown, Certificate Status (22): * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4): * TLSv1.3 (IN), TLS Unknown, Unknown (23): * Connection state changed (MAX_CONCURRENT_STREAMS updated)! * TLSv1.3 (OUT), TLS Unknown, Unknown (23): * TLSv1.3 (IN), TLS Unknown, Unknown (23): < HTTP/2 200 < server: nginx/1.14.0 (Ubuntu) < date: Fri, 31 Jan 2020 14:00:47 GMT < content-type: application/json; charset=UTF-8 < access-control-allow-origin: * < vary: Origin < cache-control: no-store, no-cache, no-transform, must-revalidate, max-age=0 < strict-transport-security: max-age=31536000 < * Connection #0 to host localhost left intact - From a laptop: (base) marco@marco-U36SG:~$ curl -Iki https://ggc.world/sockjs-node/info?t=1580397983088 HTTP/1.1 405 Method Not Allowed Server: nginx/1.14.0 (Ubuntu) Date: Fri, 31 Jan 2020 09:34:59 GMT Connection: keep-alive Allow: OPTIONS, GET (base) marco@marco-U36SG:~$ curl -vk https://ggc.world/sockjs-node/info?t=1580397983088 * Trying 2.36.58.214:443... * TCP_NODELAY set * Connected to ggc.world (2.36.58.214) port 443 (#0) * ALPN, offering http/1.1 * successfully set certificate verify locations: * CAfile: /home/marco/anaconda3/ssl/cacert.pem CApath: none * TLSv1.3 (OUT), TLS handshake, Client hello (1): * TLSv1.3 (IN), TLS handshake, Server hello (2): * TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8): * TLSv1.3 (IN), TLS handshake, Certificate (11): * TLSv1.3 (IN), TLS handshake, CERT verify (15): * TLSv1.3 (IN), TLS handshake, Finished (20): * TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1): * TLSv1.3 (OUT), TLS handshake, Finished (20): * SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384 * ALPN, server accepted to use http/1.1 * Server certificate: * subject: CN=ggc.world * start date: Nov 30 11:22:10 2019 GMT * expire date: Feb 28 11:22:10 2020 GMT * issuer: C=US; O=Let's Encrypt; CN=Let's Encrypt Authority X3 * SSL certificate verify ok. * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4): * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4): * old SSL session ID is stale, removing * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK < Server: nginx/1.14.0 (Ubuntu) < Date: Fri, 31 Jan 2020 14:04:11 GMT < Content-Type: application/json; charset=UTF-8 < Transfer-Encoding: chunked < Connection: keep-alive < Access-Control-Allow-Origin: * < Vary: Origin < Cache-Control: no-store, no-cache, no-transform, must-revalidate, max-age=0 < Strict-Transport-Security: max-age=31536000 < * Connection #0 to host ggc.world left intact {""websocket"":true,""origins"":[""*:*""],""cookie_needed"":false,""entropy"":1587194190}(base) Network Tab in the browser: ![sockejsError-NetworkTab-16](https://user-images.githubusercontent.com/3073209/73555251-928b6180-444d-11ea-8c7a-1b0499f3420a.jpg)",Does the same output is generated if you use non-secure protocol (http) ?,"7 https://$host:$server_port$request_uri; server_name www.ggc.world; return 301 https://$server_name$request_uri; access_log /var/log/nginx/ggcworld-access.log combined; add_header Strict-Transport-Security ""max-age=31536000""; location = /favicon.ico { access_log off; log_not_found off; } location / { proxy_pass http://162b9b0fefe706 # http://nginx.org/en/docs/stream/ngx_stream_core_module.html upstream websocket { ip_hash; server localhost:3000; } server { listen 8039ONNECTION_REFUSED ![sockejsError13](https://user-images.githubusercontent.com/3073209/73527082-baa99f00-4412-11ea-8c81-ebbc8a84fce4.jpg) - Ubuntu 18.04.03 Server Edition - (base) marco@pc:~$ node -v v12.10.0 - npm -v 6.13.6 - webpack-cli@3.3.10 - vue --version @vue/cli 4.1.2 - (base) marco@pc:~$ nginx -v nginx version: nginx/1.14.0 (Ubuntu) - From within ghe PC-Server: (base) marco@pc:~$ curl -Iki https://localhost/sockjs-node/info?t=1580397983088 HTTP/2 405 server: nginx/1.14.0 (Ubuntu) date: Fri, 31 Jan 2020 08:19:02 GMT allow: OPTIONS, GET (base) marco@pc:~/vueMatters/testproject$ curl -vk https://localhost/sockjs- node/info?t=1580397983088 * Trying ::1... * TCP_NODELAY set * connect to ::1 port 443 failed: Connection refused * Trying 127.0.0.1... * TCP_NODELAY set * Connected to localhost (127.0.0.1) port 443 (#0) * ALPN, offering h2 * ALPN, offering http/1.1 * successfully set certificate verify locations: * CAfile: /etc/ssl/certs/ca-certificates.crt CApath: /etc/ssl/certs * TLSv1.3 (OUT), TLS handshake, Client hello (1): * TLSv1.3 (IN), TLS handshake, Server hello (2): * TLSv1.3 (IN), TLS Unknown, Certificate Status (22): * TLSv1.3 (IN), TLS handshake, Unknown (8): * TLSv1.3 (IN), TLS Unknown, Certificate Status (22): * TLSv1.3 (IN), TLS handshake, Certificate (11): * TLSv1.3 (IN), TLS Unknown, Certificate Status (22): * TLSv1.3 (IN), TLS handshake, C verify (15): * TLSv1.3 (IN), TLS Unknown, Certificate Status (22): * TLSv1.3 (IN), TLS handshake, Finished (20): * TLSv1.3 (OUT), TLS change cipher, Client hello (1): * TLSv1.3 (OUT), TLS Unknown, Certificate Status (22): * TLSv1.3 (OUT), TLS handshake, Finished (20): * SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384 * ALPN, server accepted to use h2 * Server certificate: * subject: CN=ggc.world * start date: Nov 30 11:22:10 2019 GMT * expire date: Feb 28 11:22:10 2020 GMT * issuer: C=US; O=Let's Encrypt; CN=Let's Encrypt Authority X3 * SSL certificate verify ok. * Using HTTP2, server supports multi-use * Connection state changed (HTTP/2 confirmed) * Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0 * TLSv1.3 (OUT), TLS Unknown, Unknown (23): * TLSv1.3 (OUT), TLS Unknown, Unknown (23): * TLSv1.3 (OUT), TLS Unknown, Unknown (23): * Using Stream ID: 1 (easy handle 0x559bc64c5580) * TLSv1.3 (OUT), TLS Unknown, Unknown (23): * TLSv1.3 (IN), TLS Unknown, Certificate Status (22): * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4): * TLSv1.3 (IN), TLS Unknown, Certificate Status (22): * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4): * TLSv1.3 (IN), TLS Unknown, Unknown (23): * Connection state changed (MAXNCURRENT_STREAS updated)! * TLSv1.3 (OUT), TLS Unknown, Unknown (23): * TLSv1.3 (IN), TLS Unknown, Unknown (23): < HTTP/2 200 < server: nginx/1.14.0 (Ubuntu) < date: Fri, 31 Jan 2020 14:00:47 GT < content-type: application/json; charset=UTF-8 < access-control-allow-origin: * < vary: Origin < cache-control: no-store, no-cache, no-transform, must-revalidate, max-age=0 < strict-transport-security: max-age=31536000 < * Connection #0 to host localhost left intact - From a laptop: (base) marco@marco-U36SG:~$ curl -Iki https://ggc.world/sockjs-node/info?t=1580397983088 HTTP/1.1 405 Method Not Allowed Server: nginx/1.14.0 (Ubuntu) Date: Fri, 31 Jan 2020 09:34:59 GMT Connection: keep-alive Allow: OPTIS, GET (base) marco@marco-U36SG:~$ curl -vk https://ggc.world/sockjs-node/info?t=1580397983088 * Trying 2.36.58.214:443... * TCPODELPN, server accepted to use http/1.1 * Server certificate: * subject: CN=ggc.world * start date: Nov 30 11:22:10 2019 GMT * expire date: Feb 28 11:22:10 2020 GMT * issuer: C=US; O=Let's Encrypt; CN=Let's Encrypt Authority X3 * SSL certificate verify ok. * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4): * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4): * old SSL session 3555c7a-1b0499f34"
processone/ejabberd,https://github.com/processone/ejabberd/issues/2742,processone_ejabberd_issues_2742,"[SCRAM] RFC supports: SCRAM-SHA-1 and SCRAM-SHA-1-PLUS + SCRAM-SHA-256 and SCRAM-SHA-256-PLUS

""When using the SASL SCRAM mechanism, the SCRAM-SHA-256-PLUS variant SHOULD be preferred over the SCRAM-SHA-256 variant, and SHA-256 variants [RFC7677] SHOULD be preferred over SHA-1 variants [RFC5802]"". After SCRAM-SHA-1(-PLUS): - https://tools.ietf.org/html/rfc5802 - https://tools.ietf.org/html/rfc6120 Note that in https://github.com/processone/ejabberd/blob/master/src/ejabberd_c2s.erl, there is only SCRAM-SHA-1 and not SCRAM-SHA-1-PLUS. Now there is SCRAM-SHA-256(-PLUS): - https://tools.ietf.org/html/rfc7677 since 2015-11-02 - https://tools.ietf.org/html/rfc8600 since 2019-06-21: https://mailarchive.ietf.org/arch/msg/ietf-announce/suJMmeMhuAOmGn_PJYgX5Vm8lNA I add SCRAM-SHA-512(-PLUS): https://xmpp.org/extensions/inbox/hash-recommendations.html. Linked to: - https://github.com/scram-xmpp/info/issues/1",Why?,)) I add SCRAM-SHA-512(-PLUS): https://xmpp.org/extensions/inbox/hash-recommendations.html
processone/ejabberd,https://github.com/processone/ejabberd/issues/2583,processone_ejabberd_issues_2583,"Unable to get subscription request from other users.

18.03 CentOS Docker I am using external authentication with stanza.io client. I also tried with pidgin but unable to get subscription request. I have installed ejabberd on my server and also registered few domains and their sub-domains. Below is my config file and error log. Almost all the functionalities are working fine except user subscription. I am unable to subscribe with any user. Whenever I am sending subscription request to any user he never receives my requests. that's why we are unable to subscribe. I am unable to understand whats the error in ejabberd log file. Can someone point me right direction.  ---------------------------------EJABBERD LOG FILE------------------------------------------------",Can you try with other clients like Conversations or Gajim ?,"per: fast access_rules: local: - allow: local c2s: - deny: blocked - allow announce: - allow: admin configure: - allow: admin muc_create: - allow: local pubsub_createnode: - allow: local register: - allow trusted_network: - allow: loopback api_permissions: ""console commands"": from: - ejabberd_ctl who: all what: ""*"" ""admin access"": who: - access: - allow: - acl: loopback - acl: admin - oauth: - scope: ""ejabberd:admin"" - access: - allow: - acl: loopback - acl: admin what: - ""*"" - ""!stop"" - ""!start"" ""public commands"": who: - ip: ""127.0.0.1/8"" what: - ""status"" - ""connected_users_number"" modules: mod_adhoc: {} mod_admin_extra: {} mod_announce: # recommends mod_adhoc access: announce mod_blocking: {} # requires mod_pri: access_max_usere:bookmarks"": access_model:"
AutoMapper/AutoMapper,https://github.com/AutoMapper/AutoMapper/issues/2723,AutoMapper_AutoMapper_issues_2723,"ProjectTo() seems to fail with enums?

### Source/destination types  ### Mapping configuration  ### Version: 7.0.1 ### Expected behavior I expect a bunch of foo's to be mapped to bars and vice versa using the ProjectTo() method. Instead, AutoMapper throws an exception both ways. ### Actual behavior Exceptions thrown. ### Steps to reproduce A gist to reproduce the issue is provided here: https://gist.github.com/RobThree/170022cd422936da400fa9605b439e67 When I map an object with an enum property to an object with a string property (and vice versa) this works just fine. However, when I do this on an IQueryable using the ProjectTo method this fails. The exception thrown (when mapping from object with enum to object with string property) is **System.ArgumentException: 'Argument types do not match'** and is thrown on [this line](https://github.com/AutoMapper/AutoMapper/blob/8a4e0bcc3f0cbb2337fd5b99e44432b2f415cfae/src/AutoMapper/QueryableExtensions/Impl/MappedTypeExpressionBinder.cs#L27) in the MappedTypeExpressionBinder. The exception thrown (when mapping from object with string to object with enum property) is **System.ArgumentException: 'Type 'System.String' does not have a default constructor'** and is thrown on [this line](https://github.com/AutoMapper/AutoMapper/blob/8a4e0bcc3f0cbb2337fd5b99e44432b2f415cfae/src/AutoMapper/QueryableExtensions/ExpressionBuilder.cs#L240) in the ExpressionBuilder. It does work, however, when mapping a single item also demonstrated in the provided program. I'm sure I'm doing something wrong but maybe someone can tell me what? (Or did I actually find a bug)?",How would you do this with an EF query?,"When I map an object with an enum property to an object with a string property (and vice versa) this works just fine. However, when I do this on an IQueryable using the ProjectTo method this fails. A gist to reproduce the issue is provided here: https://gist.github.com/RobThree/170022cd422936da400fa9605b439e67"
cython/cython,https://github.com/cython/cython/issues/3342,cython_cython_issues_3342,"Explicitly offloading nogil code to device/GPU

Allow marking nogil-code to be run on a device/GPU. There is also a discussion on the mailing-list: https://mail.python.org/pipermail/cython-devel/2020-January/005262.html. Older CEPs suggest doing similar things by * either automatically determining regions for offload using OpenCL (https://github.com/cython/cython/wiki/enhancements-opencl) which is relatively tricky and intriduces a new tool * or pretty extensively extend the Cython language (https://github.com/cython/cython/wiki/enchancements-metadefintions) A first and already very powerful step would be to explicitly mark code that should be offloaded, minimize language extensions and not require extra tools. As a start, we could consider only parallel devices, such as GPUs and use OpenMP target since Cython already uses OpenMP for parallelism Let's consider a simple example for computing pairwise distances between vectors in parallel:  The parallel region is implicitly defined by the first, outermost prange. For offloading we could demand that the parallel region needs to be defined explicitly:  Now all we need is a marker that the parallel region should be offloaded to a device:  Cython should take care of a safe way to define data mappings: transferring the necessary data to the device and from device to the host: - by default arrays are sent from host to device when entering the parallel region and from the device to host when exiting - read-only data is only sent from host to device but not from device to host - ideally, write-only data will not be sent from host to device but only from device to host - ideally, we can also detect that a variable is not used outside the parallel region so that we do not need to transfer any data (only allocate and deallocate) Because complex indexing can make it impossible to correctly determine the best mapping and because data-movement is often the biggest performance bottleneck, we also need a way for experts to optimize the data movement. For that, device should accept a dictionary mapping variable names to a map-type (as borrowed from OpenMP target map clauses) * ""to"" means host to device * ""from"" means device to host * ""tofrom"" means host to device and device to host * ""alloc"" means not data-transfer at all In the above example, the input array/memview is read-only on the device, so we could indicate it like this:  Map-values provided in device overwrite whatever Cython would automatically infer. Another common challenge in offloading is that computation might go back and forth between host and GPU. In such cases it is often required to keep data on the GPU between different GPU regions even if a host-section is in between. As an example, let's look at the above code and block the computation by only computing a single row of the output array at once. Note that this will be needed anyway if the input array becomes large since the output vector size increases with quadratically and might simply not fit on the GPU.  Even though we only transfer slices of D in Dslice from device to host, the entire input array X will be send from host to device in every iteration of the outermost loop. The suggested solution adds a data-context (to be used with with) defining the lifetime of variables on the device. Let's simply use the same keyword device and let it accept the same mappings:  Since this is an expert tool we might not want or need to infer any map-type and leave it to the programmer. ### Calling functions in a device block The OpenMP compiler will try to inline a function that appears in a target/device section/block and will usually complain if that's not possible. For such cases Cython provides the decorate @cython.device to explicitly make functions available on the device::  A first prototype is implemented here: https://github.com/fschlimb/cython/tree/offload ### Limitations, open questions etc * In most cases the generated code does not work if @boundscheck and @wraparound are not set to False. * OpenMP does not allow mapping overlapping memory regions. We need to at least check that 2 memviews do not overlap * C-pointers are not properly checked * Only C-contiguous memviews are supported * need support for setup.py/distutils * need tests * need docu (syntax, semantics, and how to setup offload compiler) * error reporting back to host is disabled, properly mapping related variables could allow a useful error reporting * string support has not been looked at yet @ogrisel @GaelVaroquaux @oleksandr-pavlyk @DrTodd13",Could you please point us to some build instructions to get it to work with CUDA-enabled in Intel GEN Graphics GPUs for testing?,"### Calling functions in a device block The OpenMP compiler will try to inline a function that appears in a target/device section/block and will usually complain if that's not possible. For such cases Cython provides the decorate @cython.device to explicitly make functions available on the device::  ### * error reporting back to host is disabled, properly mapping related variables could allow a useful error reporting * string support has not been looked at yet"
robolectric/robolectric,https://github.com/robolectric/robolectric/issues/4568,robolectric_robolectric_issues_4568,"java.lang.NoSuchMethodError: com.google.common.base.CharMatcher.whitespace()Lcom/google/common/base/CharMatcher

### Description Hi, I'm writing unit tests with Robolectric. I get the above error when I try to run my tests. I have tried several solutions suggested from StackOverflow and other forums, none of which have resolved the issue for me. So far I have looked at the dependency tree and I can see two versions of guava being used: +--- com.google.guava:guava-jdk5:13.0@jar +--- com.google.guava:guava:27.0.1-android@jar After noticing this, I have then tried to: 1. exclude the first dependency  2. upgrade the guava version to the latest  This is what the error looks like:  And this is my Test class currently:  Can someone help me see what I'm missing? Thanks. ### Robolectric & Android Version Robolectrive version 4.1 Android Studio version 3.3",Can you please post a complete github project that demonstrates the problem? You can fork and use https://github.com/robolectric/deckard as a baseline if needed,groovy groovy java groovy java 
beetbox/beets,https://github.com/beetbox/beets/issues/3128,beetbox_beets_issues_3128,"Convert: calling arg_encoding leads to windows path failures

### Problem Arg_encoding leads to a non-Unicode encoding being used. The paths need to be unicode to make Japanese characters, so they fail. https://github.com/beetbox/beets/blob/master/beetsplug/convert.py#L222  https://github.com/beetbox/beets/blob/master/beets/util/__init__.py#L319 When I hardcode the  try: block to ""utf-8"", the problem is fixed. 2019-10-28 edit: Hardcode arg_encoding to always return 'utf-8'. https://github.com/beetbox/beets/blob/1b187fbf5345727e0dfdaea958a714f19e917a4e/beets/util/__init__.py#L328 I haven't set my default codepage to UTF8, but the process has [side-effects](https://superuser.com/questions/269818/change-default-code-page-of-windows-console-to-utf-8) on Windows so I don't know if that's what users should be expected to do. My terminal can print special characters just fine, and beets has been able to write paths with special characters as well. This leads me to conclude that calling arg_encoding() isn't the right move here. Running this command in verbose (-vv) mode:  ### Setup * OS: Win10 x64 * Python version: 3.7.1 * beets version: 1.4.8 (dev) * Terminal: Cmder, cmd, powershell",Do you know what encoding it's try to use and---while this is a long shot---why it's doing that?,2019-10-28 edit: Hardcode arg_encoding to always return 'utf-8'. https://github.com/beetbox/beets/blob/1b187fbf5345727e0dfdaea958a714f19e917a4e/beets/util/__init__.py#L328
beetbox/beets,https://github.com/beetbox/beets/issues/3031,beetbox_beets_issues_3031,"need help with discogs plugin

### Problem It seems like discogs can't be connected. Running this command in verbose (-vv) mode: $ docker exec -it beets /bin/bash -c 'beet -vv import -q /downloads' export of a part of the log (it is gigantic ;): Sending event: import_task_created Sending event: import_task_start Looking up: /downloads/036/Sunlounger-The_Downtempo_Edition-2CD-2010-TGX Tagging Sunlounger - The Downtempo Edition No album ID found. Search terms: Sunlounger - The Downtempo Edition Album might be VA: False Searching for MusicBrainz releases with: {'release': u'the downtempo edition', 'tracks': u'2', 'artist': u'sunlounger'} Requesting MusicBrainz release 28f465dd-df5b-4eac-905b-4d0fb879dfe8 Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created primary MB release type: album secondary MB release type(s): compilation Sending event: albuminfo_received Candidate: Sunlounger - The Downtempo Edition (28f465dd-df5b-4eac-905b-4d0fb879dfe8) Computing track assignment... ...done. Success. Distance: 0.70 Requesting MusicBrainz release 5629ec6e-179e-4b2c-903c-e79eac2166ce Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created primary MB release type: album secondary MB release type(s): compilation Sending event: albuminfo_received Candidate: Sunlounger - Beautiful Voices 036 (Sunlounger a.k.a. DJ Shah Special Edition 3) (5629ec6e-179e-4b2c-903c-e79eac2166ce) Computing track assignment... ...done. Success. Distance: 0.77 Requesting MusicBrainz release 3624d4be-2607-442c-b670-cb6d8df6dac2 Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created primary MB release type: album Sending event: albuminfo_received Candidate: Sunlounger - Another Day on the Terrace (3624d4be-2607-442c-b670-cb6d8df6dac2) Computing track assignment... ...done. Success. Distance: 0.77 Requesting MusicBrainz release 6a9a1b75-9003-444f-890e-33575a05a5b3 Sending event: import_task_created primary MB release type: album Sending event: albuminfo_received Candidate: Sunlounger - Another Day on the Terrace: Chill Versions (6a9a1b75-9003-444f-890e-33575a05a5b3) Computing track assignment... ...done. Success. Distance: 0.72 Requesting MusicBrainz release cfe68c8c-70c4-4dcb-a516-9e3ae764bcca Sending event: import_task_created Sending event: import_task_created primary MB release type: album Sending event: albuminfo_received Candidate: Sunlounger - Another Day on the Terrace: Dance Versions (cfe68c8c-70c4-4dcb-a516-9e3ae764bcca) Computing track assignment... ...done. Success. Distance: 0.70 Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created discogs: Communication error while searching for u'Sunlounger The Downtempo Edition' Traceback (most recent call last): File ""/usr/lib/python2.7/site-packages/beetsplug/discogs.py"", line 211, in get_albums type='release').page(1) File ""/usr/lib/python2.7/site-packages/discogs_client/models.py"", line 347, in page data = self.client._get(self._url_for_page(index)) File ""/usr/lib/python2.7/site-packages/discogs_client/client.py"", line 123, in _get return self._request('GET', url) File ""/usr/lib/python2.7/site-packages/discogs_client/client.py"", line 115, in _request body = json.loads(content.decode('utf8')) File ""/usr/lib/python2.7/json/__init__.py"", line 339, in loads return _default_decoder.decode(s) File ""/usr/lib/python2.7/json/decoder.py"", line 364, in decode obj, end = self.raw_decode(s, idx=_w(s, 0).end()) File ""/usr/lib/python2.7/json/decoder.py"", line 382, in raw_decode raise ValueError(""No JSON object could be decoded"") ValueError: No JSON object could be decoded Evaluating 5 candidates. Sending event: import_task_start /downloads/036/Sunlounger-The_Downtempo_Edition-2CD-2010-TGX (2 items) Skipping. Looking up: /downloads/036/Sunn_0)))-Black_One-2005-BUTT Tagging Sunn 0))) - Black One No album ID found. Search terms: Sunn 0))) - Black One Album might be VA: False Searching for MusicBrainz releases with: {'release': u'black one', 'tracks': u'7', 'artist': u'sunn 0)))'} Requesting MusicBrainz release 2cd90e18-0b4b-32e9-b07d-ea5d13cd0c2e Sending event: import_task_choice Sending event: import_task_created Sending event: import_task_created primary MB release type: album Sending event: albuminfo_received Candidate: Sunn O))) - Black One (2cd90e18-0b4b-32e9-b07d-ea5d13cd0c2e) Computing track assignment... ...done. Success. Distance: 0.04 Requesting MusicBrainz release fc4ab79b-3622-4221-8883-8da2e9d2b5ce open failed: can't sync to MPEG frame unreadable file: /downloads/036/Super_Flu_and_Andhim--Reeves-(MONABERRY013-6)-WEB-2012-SiBERiA/05-super_flu_and_andhim--scuzzlebutt_(gn)-sibrsion)-siberia.mp3 Sending event: import_task_created Sending event: import_task_created primary MB release type: album Sending event: albuminfo_received Candidate: Sunn O))) - Black One (fc4ab79b-3622-4221-8883-8da2e9d2b5ce) Computing track assignment... ...done. Success. Distance: 0.04 Requesting MusicBrainz release a87bb818-1584-4792-a4ea-ede2a752670f Sending event: import_task_created primary MB release type: album Sending event: albuminfo_received Candidate: Sunn O))) - Black One (a87bb818-1584-4792-a4ea-ede2a752670f) Computing track assignment... ...done. Success. Distance: 0.04 Requesting MusicBrainz release 6cb486f7-ae6d-3205-869a-e99dd5c410f4 Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created primary MB release type: album Sending event: albuminfo_received Candidate: Sunn O))) - Black One (6cb486f7-ae6d-3205-869a-e99dd5c410f4) Computing track assignment... ...done. Success. Distance: 0.18 Requesting MusicBrainz release 2bf3e883-7b9d-4a49-a16b-70a0f043bb31 Sending event: import_task_created Sending event: import_task_created primary MB release type: album Sending event: albuminfo_received Candidate: Sunn O))) - Black One (2bf3e883-7b9d-4a49-a16b-70a0f043bb31) Computing track assignment... ...done. Success. Distance: 0.11 Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created discogs: Connection error in album search Traceback (most recent call last): File ""/usr/lib/python2.7/site-packages/beetsplug/discogs.py"", line 151, in candidates return self.get_albums(query) File ""/usr/lib/python2.7/site-packages/beetsplug/discogs.py"", line 216, in get_albums return [album for album in map(self.get_album_info, releases[:5]) File ""/usr/lib/python2.7/site-packages/beetsplug/discogs.py"", line 246, in get_album_info result.refresh() File ""/usr/lib/python2.7/site-packages/discogs_client/models.py"", line 211, in refresh data = self.client._get(self.data['resource_url']) File ""/usr/lib/python2.7/site-packages/discogs_client/client.py"", line 123, in _get return self._request('GET', url) File ""/usr/lib/python2.7/site-packages/discogs_client/client.py"", line 115, in _request body = json.loads(content.decode('utf8')) File ""/usr/lib/python2.7/json/__init__.py"", line 339, in loads return _default_decoder.decode(s) File ""/usr/lib/python2.7/json/decoder.py"", line 364, in decode obj, end = self.raw_decode(s, idx=_w(s, 0).end()) File ""/usr/lib/python2.7/json/decoder.py"", line 382, in raw_decode raise ValueError(""No JSON object could be decoded"") ValueError: No JSON object could be decoded Evaluating 5 candidates. Sending event: import_task_start /downloads/036/Sunn_0)))-Black_One-2005-BUTT (7 items) Looking up: /downloads/036/Sunn_0)))-Black_One_(Bonus_CD)-CD-2005-COR Correcting tags from: Tagging Sunn 0))) - Black One (Bonus CD) Sunn [31;01m0[39;49;00m))) - Black One To: No album ID found. Sunn [31;01mO[39;49;00m))) - Black One Search terms: Sunn 0))) - Black One (Bonus CD) URL: https://musicbrainz.org/release/2cd90e18-0b4b-32e9-b07d-ea5d13cd0c2e (Similarity: [32;01m96.3%[39;49;00m) [33;01m(artist, tracks)[39;49;00m [37m(2x12"" Vinyl, 2005, US, Southern Lord, picture disc)[39;49;00m Album might be VA: False Searching for MusicBrainz releases with: {'release': u'black one (bonus cd)', 'tracks': u'2', 'artist': u'sunn 0)))'} 12"" Vinyl 1 * Cursed Realms -> Cursed Realms[31;01m (of the Winterdemons)[39;49;00m [33;01m(title)[39;49;00m * Candle[37mg[39;49;00moat -> Candle[37mG[39;49;00moat 12"" Vinyl 2 Sending event: import_task_choice Requesting MusicBrainz release 6cb486f7-ae6d-3205-869a-e99dd5c410f4 Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created primary MB release type: album Sending event: albuminfo_received Candidate: Sunn O))) - Black One (6cb486f7-ae6d-3205-869a-e99dd5c410f4) Computing track assignment... ...done. Success. Distance: 0.63 Requesting MusicBrainz release 2bf3e883-7b9d-4a49-a16b-70a0f043bb31 Sending event: import_task_created Sending event: import_task_created primary MB release type: album Sending event: albuminfo_received Candidate: Sunn O))) - Black One (2bf3e883-7b9d-4a49-a16b-70a0f043bb31) Computing track assignment... ...done. Success. Distance: 0.42 Requesting MusicBrainz release a87bb818-1584-4792-a4ea-ede2a752670f primary MB release type: album Sending event: albuminfo_received Candidate: Sunn O))) - Black One (a87bb818-1584-4792-a4ea-ede2a752670f) Computing track assignment... Sending event: import_task_created ...done. Success. Distance: 0.56 Requesting MusicBrainz release 2cd90e18-0b4b-32e9-b07d-ea5d13cd0c2e Sending event: import_task_apply primary MB release type: album Sending event: albuminfo_received Candidate: Sunn O))) - Black One (2cd90e18-0b4b-32e9-b07d-ea5d13cd0c2e) Computing track assignment... ...done. Success. Distance: 0.56 Requesting MusicBrainz release fc4ab79b-3622-4221-8883-8da2e9d2b5ce Sending event: import_task_created 0 of 7 items replaced Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change primary MB release type: album Sending event: albuminfo_received Candidate: Sunn O))) - Black One (fc4ab79b-3622-4221-8883-8da2e9d2b5ce) Computing track assignment... ...done. Success. Distance: 0.56 Sending event: import_task_created Evaluating 5 candidates. /downloads/036/Sunn_0)))-Black_One_(Bonus_CD)-CD-2005-COR (2 items) Skipping. Sending event: import_task_start Sending event: import_task_choice Looking up: /downloads/036/Sunn_0)))_-_Boris-Altar_(Bonus)-CD-2006-COR Tagging Sunn 0)))_-_Boris - Altar (Bonus) No album ID found. Search terms: Sunn 0)))_-_Boris - Altar (Bonus) Album might be VA: False Searching for MusicBrainz releases with: {'release': u'altar (bonus)', 'tracks': u'1', 'artist': u'sunn 0)))_-_boris'} lastgenre: added last.fm album genre (album): Drone Metal Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Requesting MusicBrainz release eacb8678-70e8-45a0-88c3-06bf42f26a19 Sending event: import_task_created fetchart: trying source filesystem for album Sunn O))) - Black One fetchart: trying source coverart for album Sunn O))) - Black One fetchart: downloading image: https://coverartarchive.org/release/2cd90e18-0b4b-32e9-b07d-ea5d13cd0c2e/front fetchart: not a supported image: image/x-None fetchart: trying source coverart for album Sunn O))) - Black One fetchart: downloading image: https://coverartarchive.org/release-group/5cf7cf40-d44c-3393-923e-ad399170d76d/front Sending event: import_task_created primary MB release type: album Sending event: albuminfo_received Candidate: Sunn O))) & Boris - Altar (eacb8678-70e8-45a0-88c3-06bf42f26a19) Computing track assignment... ...done. Success. Distance: 0.59 Requesting MusicBrainz release 441e27af-1bab-4295-9a07-76b5d534766c Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created primary MB release type: album Sending event: albuminfo_received Candidate: Sunn O))) & Boris - Altar (441e27af-1bab-4295-9a07-76b5d534766c) Computing track assignment... ...done. Success. Distance: 0.59 Requesting MusicBrainz release 0611646d-bd7f-491a-bfd0-d7a7ae7bda55 primary MB release type: album Sending event: albuminfo_received Candidate: Sunn O))) & Boris - Altar (0611646d-bd7f-491a-bfd0-d7a7ae7bda55) Computing track assignment... ...done. Success. Distance: 0.50 Requesting MusicBrainz release c13c6243-1511-44c0-b2ba-4ace484a8eb6 Sending event: import_task_created fetchart: downloaded art to: /tmp/tmpyZSYqO.jpg fetchart: using remote image /tmp/tmpyZSYqO.jpg Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: item_copied Sending event: database_change Sending event: database_change Sending event: write Sending event: after_write primary MB release type: album Sending event: albuminfo_received Candidate: Sunn O))) & Boris - Altar (c13c6243-1511-44c0-b2ba-4ace484a8eb6) Computing track assignment... ...done. Success. Distance: 0.48 Requesting MusicBrainz release 66411727-983b-385e-8bbf-62a91c5f04a0 Sending event: item_copied Sending event: import_task_created Sending event: database_change Sending event: database_change Sending event: write Sending event: after_write Sending event: item_copied Sending event: import_task_created Sending event: database_change Sending event: database_change Sending event: write Sending event: after_write primary MB release type: album Sending event: albuminfo_received Candidate: Sunn O))) & Boris - Altar (66411727-983b-385e-8bbf-62a91c5f04a0) Computing track assignment... ...done. Success. Distance: 0.56 Sending event: item_copied Sending event: import_task_created Sending event: database_change Sending event: database_change Sending event: write Sending event: after_write Sending event: item_copied Sending event: database_change Sending event: database_change Sending event: write Sending event: after_write Sending event: import_task_created Sending event: item_copied Sending event: import_task_created Sending event: import_task_created Sending event: database_change Sending event: database_change Sending event: write Sending event: after_write Sending event: item_copied Sending event: import_task_created Sending event: database_change Sending event: database_change Sending event: write Sending event: after_write Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: import_task_files scrub: auto-scrubbing /music/Sunn O)))/Black One/01 - Sin Nanna.mp3 scrub: writing new tags after scrub Sending event: write Sending event: after_write scrub: auto-scrubbing /music/Sunn O)))/Black One/02 - It Took the Night to Believe.mp3 scrub: writing new tags after scrub Sending event: write Sending event: after_write scrub: auto-scrubbing /music/Sunn O)))/Black One/03 - Cursed Realms (of the Winterdemons).mp3 Sending event: import_task_created scrub: writing new tags after scrub Sending event: write Sending event: after_write scrub: auto-scrubbing /music/Sunn O)))/Black One/06 - Cry for the Weeper.mp3 scrub: writing new tags after scrub Sending event: write Sending event: after_write scrub: auto-scrubbing /music/Sunn O)))/Black One/07 - Báthory Erzsebet.mp3 scrub: writing new tags after scrub Sending event: write Sending event: after_write scrub: auto-scrubbing /music/Sunn O)))/Black One/04 - Orthodox Caveman.mp3 scrub: writing new tags after scrub Sending event: write Sending event: after_write scrub: auto-scrubbing /music/Sunn O)))/Black One/05 - CandleGoat.mp3 scrub: writing new tags after scrub Sending event: write Sending event: after_write Sending event: art_set embedart: Embedding album art into Sunn O))) - Black One embedart: embedding /music/Sunn O)))/Black One/albumart.jpg Sending event: write Sending event: after_write embedart: embedding /music/Sunn O)))/Black One/albumart.jpg Sending event: write Sending event: after_write embedart: embedding /music/Sunn O)))/Black One/albumart.jpg Sending event: write Sending event: after_write embedart: embedding /music/Sunn O)))/Black One/albumart.jpg Sending event: write Sending event: after_write embedart: embedding /music/Sunn O)))/Black One/albumart.jpg Sending event: write Sending event: after_write embedart: embedding /music/Sunn O)))/Black One/albumart.jpg Sending event: write Sending event: after_write embedart: embedding /music/Sunn O)))/Black One/albumart.jpg Sending event: write Sending event: after_write Sending event: database_change Sending event: album_imported Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created discogs: Communication error while searching for u'Sunn 0 _ _Boris Altar Bonus ' Traceback (most recent call last): File ""/usr/lib/python2.7/site-packages/beetsplug/discogs.py"", line 211, in get_albums type='release').page(1) File ""/usr/lib/python2.7/site-packages/discogs_client/models.py"", line 347, in page data = self.client._get(self._url_for_page(index)) File ""/usr/lib/python2.7/site-packages/discogs_client/client.py"", line 123, in _get return self._request('GET', url) File ""/usr/lib/python2.7/site-packages/discogs_client/client.py"", line 115, in _request body = json.loads(content.decode('utf8')) File ""/usr/lib/python2.7/json/__init__.py"", line 339, in loads return _default_decoder.decode(s) File ""/usr/lib/python2.7/json/decoder.py"", line 364, in decode obj, end = self.raw_decode(s, idx=_w(s, 0).end()) File ""/usr/lib/python2.7/json/decoder.py"", line 382, in raw_decode raise ValueError(""No JSON object could be decoded"") ValueError: No JSON object could be decoded Evaluating 5 candidates. Sending event: import_task_start ### Setup * OS: Ubuntu 18.04 LTS * Python version: 2.7.15 * beets version: 1.4.7 * Turning off plugins made problem go away (yes/no): problem is related to a plugin...musicbrainz matching works. PS: I'm running beet inside a docker container (linuxserver.io version) My configuration (output of beet config) is: plugins: fromfilename discogs ftintitle duplicates fetchart embedart convert scrub replaygain lastgenre web directory: /music library: /config/musiclibrary.blb art_filename: albumart threaded: yes original_date: no per_disc_numbering: no convert: auto: no ffmpeg: /usr/bin/ffmpeg opts: -ab 320k -ac 2 -ar 48000 max_bitrate: 320 threads: 1 paths: default: $albumartist/$album%aunique{}/$track - $title singleton: Non-Album/$artist - $title comp: Compilations/$album%aunique{}/$track - $title albumtype_soundtrack: Soundtracks/$album/$track $title import: write: yes copy: yes move: no resume: ask incremental: yes quiet_fallback: skip timid: no log: /config/beet.log lastgenre: auto: yes source: album embedart: auto: yes fetchart: auto: yes replaygain: auto: no scrub: auto: yes replace: '^\.': _ '[\x00-\x1f]': _ '[<>:""\?\*\|]': _ '[\xE8-\xEB]': e '[\xEC-\xEF]': i '[\xE2-\xE6]': a '[\xF2-\xF6]': o '[\xF8]': o '\.$': _ '\s+$': '' web: host: 0.0.0.0 port: 8337 #musicbrainz: # host: 127.0.0.1:5051 # ratelimit: 100 Do anyone know how I could fix this ? Musicbrainz alone matches around 30% of my albums only ... :( thanks !","Does this happen for every album, for just specific ones? You’ve truncated the verbose output; what else is in the logs?","Sending event: import_task_start /downloads/036/Sunlounger-The_Downtempo_Edition-2CD-2010-TGX (2 items) Skipping. Looking up: /downloads/036/Sunn_0)))-Black_One-2005-BUTT Tagging Sunn 0))) - Black One No album ID found. Search terms: Sunn 0))) - Black One Album might be VA: False Searching for MusicBrainz releases with: {'release': u'black one', 'tracks': u'7', 'artist': u'sunn 0)))'} Requesting MusicBrainz release 2cd90e18-0b4b-32e9-b07d-ea5d13cd0c2e Sending event: import_task_choice Sending event: import_task_created Sending event: import_task_created primary MB release type: album Sending event: albuminfo_received Candidate: Sunn O))) - Black One (2cd90e18-0b4b-32e9-b07d-ea5d13cd0c2e) Computing track assignment... ...done. Success. Distance: 0.04 Requesting MusicBrainz release fc4ab79b-3622-4221-8883-8da2e9d2b5ce open failed: can't sync to MPEG frame unreadable file: /downloads/036/Super_Flu_and_Andhim--Reeves-(MONABERRY013-6)-WEB-2012-SiBERiA/05-super_flu_and_andhim--scuzzlebutt_(gn)-sibrsion)-siberia.mp3 Sending event: import_task_created Sending event: import_task_created primary MB release type: album Sending event: albuminfo_received Candidate: Sunn O))) - Black One (fc4ab79b-3622-4221-8883-8da2e9d2b5ce) Computing track assignment... ...done. Success. Distance: 0.04 Requesting MusicBrainz release a87bb818-1584-4792-a4ea-ede2a752670f Sending event: import_task_created primary MB release type: album Sending event: albuminfo_received Candidate: Sunn O))) - Black One (a87bb818-1584-4792-a4ea-ede2a752670f) Computing track assignment... ...done. Success. Distance: 0.04 Requesting MusicBrainz release 6cb486f7-ae6d-3205-869a-e99dd5c410f4 Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created primary MB release type: album Sending event: albuminfo_received Candidate: Sunn O))) - Black One (6cb486f7-ae6d-3205-869a-e99dd5c410f4) Computing track assignment... ...done. Success. Distance: 0.18 Requesting MusicBrainz release 2bf3e883-7b9d-4a49-a16b-70a0f043bb31 Sending event: import_task_created Sending event: import_task_created primary MB release type: album Sending event: albuminfo_received Candidate: Sunn O))) - Black One (2bf3e883-7b9d-4a49-a16b-70a0f043bb31) Computing track assignment... ...done. Success. Distance: 0.11 Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created discogs: Connection error in album search Traceback (most recent call last): File ""/usr/lib/python2.7/site-packages/beetsplug/discogs.py"", line 151, in candidates return self.get_albums(query) File ""/usr/lib/python2.7/site-packages/beetsplug/discogs.py"", line 216, in get_albums return [album for album in map(self.get_album_info, releases[:5]) File ""/usr/lib/python2.7/site-packages/beetsplug/discogs.py"", line 246, in get_album_info result.refresh() File ""/usr/lib/python2.7/site-packages/discogs_client/models.py"", line 211, in refresh data = self.client._get(self.data['resource_url']) File ""/usr/lib/python2.7/site-packages/discogs_client/client.py"", line 123, in _get return self._request('GET', url) File ""/usr/lib/python2.7/site-packages/discogs_client/client.py"", line 115, in _request body = json.loads(content.decode('utf8')) File ""/usr/lib/python2.7/json/__init__.py"", line 339, in loads return _default_decoder.decode(s) File ""/usr/lib/python2.7/json/decoder.py"", line 364, in decode obj, end = self.raw_decode(s, idx=_w(s, 0).end()) File ""/usr/lib/python2.7/json/decoder.py"", line 382, in raw_decode raise ValueError(""No JSON object could be decoded"") ValueError: No JSON object could be decoded Evaluating 5 candidates. Sending event: import_task_start /downloads/036/Sunn_0)))-Black_One-2005-BUTT (7 items) Looking up: /downloads/036/Sunn_0)))-Black_One_(Bonus_CD)-CD-2005-COR Correcting tags from: Tagging Sunn 0))) - Black One (Bonus CD) Sunn [31;01m0[39;49;00m))) - Black One To: No album ID found. Sunn [31;01mO[39;49;00m))) - Black One Search terms: Sunn 0))) - Black One (Bonus CD) URL: https://musicbrainz.org/release/2cd90e18-0b4b-32e9-b07d-ea5d13cd0c2e (Similarity: [32;01m96.3%[39;49;00m) [33;01m(artist, tracks)[39;49;00m [37m(2x12"" Vinyl, 2005, US, Southern Lord, picture disc)[39;49;00m Album might be VA: False Searching for MusicBrainz releases with: {'release': u'black one (bonus cd)', 'tracks': u'2', 'artist': u'sunn 0)))'} 12"" Vinyl 1 * Cursed Realms -> Cursed Realms[31;01m (of the Winterdemons)[39;49;00m [33;01m(title)[39;49;00m * Candle[37mg[39;49;00moat -> Candle[37mG[39;49;00moat 12"" Vinyl 2 Sending event: import_task_choice Requesting MusicBrainz release 6cb486f7-ae6d-3205-869a-e99dd5c410f4 Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created primary MB release type: album Sending event: albuminfo_received Candidate: Sunn O))) - Black One (6cb486f7-ae6d-3205-869a-e99dd5c410f4) Computing track assignment... ...done. Success. Distance: 0.63 Requesting MusicBrainz release 2bf3e883-7b9d-4a49-a16b-70a0f043bb31 Sending event: import_task_created Sending event: import_task_created primary MB release type: album Sending event: albuminfo_received Candidate: Sunn O))) - Black One (2bf3e883-7b9d-4a49-a16b-70a0f043bb31) Computing track assignment... ...done. Success. Distance: 0.42 Requesting MusicBrainz release a87bb818-1584-4792-a4ea-ede2a752670f primary MB release type: album Sending event: albuminfo_received Candidate: Sunn O))) - Black One (a87bb818-1584-4792-a4ea-ede2a752670f) Computing track assignment... Sending event: import_task_created ...done. Success. Distance: 0.56 Requesting MusicBrainz release 2cd90e18-0b4b-32e9-b07d-ea5d13cd0c2e Sending event: import_task_apply primary MB release type: album Sending event: albuminfo_received Candidate: Sunn O))) - Black One (2cd90e18-0b4b-32e9-b07d-ea5d13cd0c2e) Computing track assignment... ...done. Success. Distance: 0.56 Requesting MusicBrainz release fc4ab79b-3622-4221-8883-8da2e9d2b5ce Sending event: import_task_created 0 of 7 items replaced Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change primary MB release type: album Sending event: albuminfo_received Candidate: Sunn O))) - Black One (fc4ab79b-3622-4221-8883-8da2e9d2b5ce) Computing track assignment... ...done. Success. Distance: 0.56 Sending event: import_task_created Evaluating 5 candidates. /downloads/036/Sunn_0)))-Black_One_(Bonus_CD)-CD-2005-COR (2 items) Skipping. Sending event: import_task_start Sending event: import_task_choice Looking up: /downloads/036/Sunn_0)))_-_Boris-Altar_(Bonus)-CD-2006-COR Tagging Sunn 0)))_-_Boris - Altar (Bonus) No album ID found. Search terms: Sunn 0)))_-_Boris - Altar (Bonus) Album might be VA: False Searching for MusicBrainz releases with: {'release': u'altar (bonus)', 'tracks': u'1', 'artist': u'sunn 0)))_-_boris'} lastgenre: added last.fm album genre (album): Drone Metal Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Requesting MusicBrainz release eacb8678-70e8-45a0-88c3-06bf42f26a19 Sending event: import_task_created fetchart: trying source filesystem for album Sunn O))) - Black One fetchart: trying source coverart for album Sunn O))) - Black One fetchart: downloading image: https://coverartarchive.org/release/2cd90e18-0b4b-32e9-b07d-ea5d13cd0c2e/front fetchart: not a supported image: image/x-None fetchart: trying source coverart for album Sunn O))) - Black One fetchart: downloading image: https://coverartarchive.org/release-group/5cf7cf40-d44c-3393-923e-ad399170d76d/front Sending event: import_task_created primary MB release type: album Sending event: albuminfo_received Candidate: Sunn O))) & Boris - Altar (eacb8678-70e8-45a0-88c3-06bf42f26a19) Computing track assignment... ...done. Success. Distance: 0.59 Requesting MusicBrainz release 441e27af-1bab-4295-9a07-76b5d534766c Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created primary MB release type: album Sending event: albuminfo_received Candidate: Sunn O))) & Boris - Altar (441e27af-1bab-4295-9a07-76b5d534766c) Computing track assignment... ...done. Success. Distance: 0.59 Requesting MusicBrainz release 0611646d-bd7f-491a-bfd0-d7a7ae7bda55 primary MB release type: album Sending event: albuminfo_received Candidate: Sunn O))) & Boris - Altar (0611646d-bd7f-491a-bfd0-d7a7ae7bda55) Computing track assignment... ...done. Success. Distance: 0.50 Requesting MusicBrainz release c13c6243-1511-44c0-b2ba-4ace484a8eb6 Sending event: import_task_created fetchart: downloaded art to: /tmp/tmpyZSYqO.jpg fetchart: using remote image /tmp/tmpyZSYqO.jpg Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: item_copied Sending event: database_change Sending event: database_change Sending event: write Sending event: after_write primary MB release type: album Sending event: albuminfo_received Candidate: Sunn O))) & Boris - Altar (c13c6243-1511-44c0-b2ba-4ace484a8eb6) Computing track assignment... ...done. Success. Distance: 0.48 Requesting MusicBrainz release 66411727-983b-385e-8bbf-62a91c5f04a0 Sending event: item_copied Sending event: import_task_created Sending event: database_change Sending event: database_change Sending event: write Sending event: after_write Sending event: item_copied Sending event: import_task_created Sending event: database_change Sending event: database_change Sending event: write Sending event: after_write primary MB release type: album Sending event: albuminfo_received Candidate: Sunn O))) & Boris - Altar (66411727-983b-385e-8bbf-62a91c5f04a0) Computing track assignment... ...done. Success. Distance: 0.56 Sending event: item_copied Sending event: import_task_created Sending event: database_change Sending event: database_change Sending event: write Sending event: after_write Sending event: item_copied Sending event: database_change Sending event: database_change Sending event: write Sending event: after_write Sending event: import_task_created Sending event: item_copied Sending event: import_task_created Sending event: import_task_created Sending event: database_change Sending event: database_change Sending event: write Sending event: after_write Sending event: item_copied Sending event: import_task_created Sending event: database_change Sending event: database_change Sending event: write Sending event: after_write Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: database_change Sending event: import_task_files scrub: auto-scrubbing /music/Sunn O)))/Black One/01 - Sin Nanna.mp3 scrub: writing new tags after scrub Sending event: write Sending event: after_write scrub: auto-scrubbing /music/Sunn O)))/Black One/02 - It Took the Night to Believe.mp3 scrub: writing new tags after scrub Sending event: write Sending event: after_write scrub: auto-scrubbing /music/Sunn O)))/Black One/03 - Cursed Realms (of the Winterdemons).mp3 Sending event: import_task_created scrub: writing new tags after scrub Sending event: write Sending event: after_write scrub: auto-scrubbing /music/Sunn O)))/Black One/06 - Cry for the Weeper.mp3 scrub: writing new tags after scrub Sending event: write Sending event: after_write scrub: auto-scrubbing /music/Sunn O)))/Black One/07 - Báthory Erzsebet.mp3 scrub: writing new tags after scrub Sending event: write Sending event: after_write scrub: auto-scrubbing /music/Sunn O)))/Black One/04 - Orthodox Caveman.mp3 scrub: writing new tags after scrub Sending event: write Sending event: after_write scrub: auto-scrubbing /music/Sunn O)))/Black One/05 - CandleGoat.mp3 scrub: writing new tags after scrub Sending event: write Sending event: after_write Sending event: art_set embedart: Embedding album art into Sunn O))) - Black One embedart: embedding /music/Sunn O)))/Black One/albumart.jpg Sending event: write Sending event: after_write embedart: embedding /music/Sunn O)))/Black One/albumart.jpg Sending event: write Sending event: after_write embedart: embedding /music/Sunn O)))/Black One/albumart.jpg Sending event: write Sending event: after_write embedart: embedding /music/Sunn O)))/Black One/albumart.jpg Sending event: write Sending event: after_write embedart: embedding /music/Sunn O)))/Black One/albumart.jpg Sending event: write Sending event: after_write embedart: embedding /music/Sunn O)))/Black One/albumart.jpg Sending event: write Sending event: after_write embedart: embedding /music/Sunn O)))/Black One/albumart.jpg Sending event: write Sending event: after_write Sending event: database_change Sending event: album_imported Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created Sending event: import_task_created discogs: Communication error while searching for u'Sunn 0 _ _Boris Altar Bonus ' Traceback (most recent call last): File ""/usr/lib/python2.7/site-packages/beetsplug/discogs.py"", line 211, in get_albums type='release').page(1) File ""/usr/lib/python2.7/site-packages/discogs_client/models.py"", line 347, in page data = self.client._get(self._url_for_page(index)) File ""/usr/lib/python2.7/site-packages/discogs_client/client.py"", line 123, in _get return self._request('GET', url) File ""/usr/lib/python2.7/site-packages/discogs_client/client.py"", line 115, in _request body = json.loads(content.decode('utf8')) File ""/usr/lib/python2.7/json/__init__.py"", line 339, in loads return _default_decoder.decode(s) File ""/usr/lib/python2.7/json/decoder.py"", line 364, in decode obj, end = self.raw_decode(s, idx=_w(s, 0).end()) File ""/usr/lib/python2.7/json/decoder.py"", line 382, in raw_decode raise ValueError(""No JSON object could be decoded"") ValueError: No JSON object could be decoded Evaluating 5 candidates. Sending event: import_task_start"
curl/curl,https://github.com/curl/curl/issues/3445,curl_curl_issues_3445,"Deleting cookies no longer works

After fixing https://github.com/curl/curl/issues/3351 my unit tests were passing, but now they broke again. ## What it does Basically the unit test lets the server set a cookie, then deletes it, and then lists cookies: 1. create a handle 2. make a request to https://eu.httpbin.org/cookies/set?foo=123&bar=ftw which sets cookies foo and bar 3. make a request to https://eu.httpbin.org/cookies/delete?foo which deletes cookie foo. 4. list cookies with curl_easy_getinfo(handle, CURLINFO_COOKIELIST, &cookies)) ### Previous output Up till version 7.62, CURLINFO_COOKIELIST  would contain both cookies, with the expired cookie in the list with the timestamp at which it expired, and value as NULL like this:  ### Current output with HTTPS (definitely a bug) The expired cookie is not deleted at all:  ### New output output with HTTP (maybe a bug?) The expired cookie is entirely omitted from CURLINFO_COOKIELIST :  ### Full log from R Sorry for the R code :-)","What libcurl version is this? It sounds like you might want a52e46f3900fb02, which is in 7.63.0...",E <NA> bar ftw  domain flag path secure expiration name value 1 eu.httpbin.org FALSE / FALS
curl/curl,https://github.com/curl/curl/issues/4239,curl_curl_issues_4239,"Loosing connection on iOS hangs request

This is continuation to old email thread that was not resolved: https://curl.haxx.se/mail/lib-2015-11/0060.html Steps: 1. Execute HTTPS request - gets response and finishes with CURLE_OK. 2. Turn off WiFi (or loose connection other way) 3. **Execute HTTPS request - completes only after set timeout with CURLE_OPERATION_TIMEDOUT. This is problematic as timeout could be set to few minutes and appears as hanged operation to user.** 4. Execute HTTPS request - completes instantly with CURLE_COULDNT_RESOLVE_HOST as expected. This is true for all subsequent requests. Built C++ libCURL into iOS application, using multi/easy interfaces. Reproduced on iOS 9.x-12.x libcurl/7.65.3 SecureTransport zlib/1.2.11",Which of these do you consider to be an issue and why?,** only This is problematic as timeout could be set to few minutes.** as expected
curl/curl,https://github.com/curl/curl/issues/2701,curl_curl_issues_2701,"transfers not done in the curl_multi_add_handle() order

My application uses libcurl mult interface with an external libuv event loop. It needs to POST requests to a server in a pipelined manner, over a single connection, so that the requests arrive in the same order as they are sent. The write callback (CURLOPT_WRITEFUNCTION) sometimes adds new easy handles to be fetched with curl_multi_add_handle(). I find that often, when the write callback issues a new easy handle, while the internal libcurl queue/pipeline is already full, the newly added handle is executed ahead of the other requests already in the pipeline. This is **not a pipelining bug**. See [update](https://github.com/curl/curl/issues/2701#issuecomment-408717419). ### I expected the following Easy handles should be executed in the same order as they are submitted to libcurl with ### curl/libcurl version curl-7_58_0 (git) ### operating system CentOS 6 I have created a testcase (test1950) which reproduces this problem. It reproduces it frequently (more than 90% of the time), but does not reproduce it at all when running under the debugger (tests/runtests.pl -g) Unfortunately I'm not familiar enough with the curl codebase to suggest a fix at this time, but intend to try.",Do you know if this bug is a regression that used to work better before? Any particular reason you're testing 7.58.0 and not the latest?,This is **not a pipelining bug**. See [update](https://github.com/curl/curl/issues/2701#issuecomment-408717419).
curl/curl,https://github.com/curl/curl/issues/4440,curl_curl_issues_4440,"C-ares Compile Error With Curl.

I try to use c-ares in libcurl. After follow 4 steps and got the error msg.  Operation System:  Prepare Work.  After Step 4 ./configure..., I got below. curl version: 7.61.0 Host setup: mipsel-openwrt-linux-gnu Install prefix: /usr/local Compiler: mipsel-openwrt-linux-gcc **SSL support: enabled (OpenSSL)** SSH support: no (--with-libssh2) **zlib support: enabled** brotli support: no (--with-brotli) GSS-API support: no (--with-gssapi) TLS-SRP support: enabled **resolver: c-ares** IPv6 support: enabled Unix sockets support: enabled IDN support: no (--with-{libidn2,winidn}) Build libcurl: Shared=yes, Static=yes Built-in manual: enabled --libcurl option: enabled (--disable-libcurl-option) Verbose errors: enabled (--disable-verbose) SSPI support: no (--enable-sspi) ca cert bundle: no ca cert path: no ca fallback: no LDAP support: no (--enable-ldap / --with-ldap-lib / --with-lber-lib) LDAPS support: no (--enable-ldaps) RTSP support: enabled RTMP support: no (--with-librtmp) metalink support: no (--with-libmetalink) PSL support: no (libpsl not found) HTTP2 support: disabled (--with-nghttp2) Protocols: DICT FILE FTP FTPS GOPHER HTTP HTTPS IMAP IMAPS POP3 POP3S RTSP SMB SMBS SMTP SMTPS TELNET TFTP",Does it provide the -L flag for the c-ares dir?,"(1.14.0, 1.10.0 are same error)"
curl/curl,https://github.com/curl/curl/issues/4158,curl_curl_issues_4158,"Failing to build with VS2015

<!-- Only file bugs here! Ask questions on the mailing lists https://curl.haxx.se/mail/ SECURITY RELATED? Post it here: https://hackerone.com/curl There are collections of known issues to be aware of: https://curl.haxx.se/docs/knownbugs.html https://curl.haxx.se/docs/todo.html --> ### I did this - Installed VS2015, restarted. - Opened Developer Command Prompt for 2015 - Ran vcvarsall.bat amd64 - cloned curl from GitHub on master - ran buildconf.bat - cd winbuild - nmake /f Makefile.vc mode=static VC=14 MACHINE=x64 ### I expected the following The build process to finish without errors. ### curl/libcurl version ~~curl 7.55.1 (Windows) libcurl/7.55.1 WinSSL~~ ~~Release-Date: [unreleased]~~ ~~Protocols: dict file ftp ftps http https imap imaps pop3 pop3s smtp smtps telnet tftp~~ ~~Features: AsynchDNS IPv6 Largefile SSPI Kerberos SPNEGO NTLM SSL~~ Attempting to build: https://github.com/curl/curl/commit/78ed3abe11de0d8fe465dee6d1de0c1b973f4409 ### operating system x64 Windows 10 Pro build 17134 ### exact log  When I look for the vquic directory that should contain ngtcp2.obj it is not there. I have disabled the PATH character limit on my machine. I have attempted to build with VS2017 and 2019 as well, with the same output.",Can you reproduce this with 7.65.3?,~~~~~~~~~~~~~~~~ Attempting to build: https://github.com/curl/curl/commit/78ed3abe11de0d8fe465dee6d1de0c1b973f4409
mitmproxy/mitmproxy,https://github.com/mitmproxy/mitmproxy/issues/3250,mitmproxy_mitmproxy_issues_3250,"Errors running tests

##### Steps to reproduce the problem: 1. python setup.py pytest  Output:  1. python setup.py pytest Output:  ##### Any other comments? What have you tried so far? 1. I ran pytest --fixtures Output:  ##### System information Latest from git (tag.revision.commit): 4.0.0.r93.7f464b892 (git describe --long --tags | sed 's/^v//;s/\([^-]*-\)g/r\1/;s/-/./g;s/\.rc./rc/g') I don't know why 4.0.0 shows up as the tag. ##### Note I am maintaining [mitmproxy-git](https://aur.archlinux.org/packages/mitmproxy-git) in Arch Linux's AUR repo. The packaging process runs tests by default. Committing to the repo without checking if the changes break the tests will break the package, please be careful when doing this. Thanks.",Does tox work for you?,1. python setup.py pytest Output:  I ran --fixtures
scrapy/scrapy,https://github.com/scrapy/scrapy/issues/4120,scrapy_scrapy_issues_4120,"Allow failing on potential data loss to trigger a retry

### Description By default settings of [DOWNLOAD_FAIL_ON_DATALOSS](https://docs.scrapy.org/en/latest/topics/settings.html#download-fail-on-dataloss) implemented in [#2590](https://github.com/scrapy/scrapy/pull/2590), whenever ResponseFailed([_DataLoss]) error occurs, it should be raised. But very similar error PotentialDataLoss cannot be raised or retried. We pass all such responses through to the callback function after adding the ['partial'] flag to the response object. [This](https://github.com/scrapy/scrapy/blob/52a52e2388e465f81e56fb92a3d731531ea6f007/scrapy/core/downloader/handlers/http11.py#L410) is where we add the flag after checking the PotentialDataLoss.  Now this prevents the retry middleware to step in and retry (depending on settings) the failed request. ### Steps to Reproduce Try to Fetch the following URL for around 150 times and somewhere in between the host server starts sending the partial/bad content-length header resulting in PotentialDataLoss error and addition of 'partial' to the flags. URL: [https://ecorp.azcc.gov/BusinessSearch/BusinessInfo?entityNumber=21816333](https://ecorp.azcc.gov/BusinessSearch/BusinessInfo?entityNumber=21816333) The above URL sometimes return a 404 before providing a response with missing Content-Length which consequently results in DataLoss / PotentialDataLoss. **Expected behavior:** ResponseFailed([_PotentialDataLoss]) exception should be raised. **Actual behavior:** Partial response passes through the engine to the callback function after adding 'partial' flag. Following is one such example log generated by the example [url](//ecorp.azcc.gov/BusinessSearch/BusinessInfo?entityNumber=21816333) above.  **Reproduces how often:** Every time when the server's response is missing 'Content-Length' header. ### Versions Scrapy : 1.7.3 lxml : 4.4.1.0 libxml2 : 2.9.5 cssselect : 1.1.0 parsel : 1.5.2 w3lib : 1.21.0 Twisted : 19.7.0 Python : 3.6.0 (v3.6.0:41df79263a11, Dec 23 2016, 08:06:12) [MSC v.1900 64 bit (AMD64)] pyOpenSSL : 19.0.0 (OpenSSL 1.1.1c 28 May 2019) cryptography : 2.7 Platform : Windows-10-10.0.18362-SP0 ### -- I could not find any URL to regenerate the ResponseFailed([_DataLoss]) error at very first Request, if someone knows any such URL, please let me know so I can update the steps to Reproduce here. @rmax, @nyov",Would you like to turn this issue into a request to add a settings to treat potential data losses as actual data losses?,e raised or retried. We pass all such responses through to the callb thein PotentialPotential
scrapy/scrapy,https://github.com/scrapy/scrapy/issues/4505,scrapy_scrapy_issues_4505,"Using proxy through http fails (https works)

### Description When I scrape without proxy, both https and http urls work. Using proxy through https works just fine. My problem is when I try http urls. In that moment I get the twisted.web.error.SchemeNotSupported: Unsupported scheme: b'' error As I see, most of the people have this issue the other way around. ### Steps to Reproduce 1. Scrape a http link with proxy **Expected behavior:** Get a 200 with the desired data. **Actual behavior:**  **Reproduces how often:** Every time I scrape with proxy ### Versions  ### Additional context I tried to add some breakpoints at the end to see where it cracks. I added the following lines in ""twisted/web/client/py"", before the cracking point:  Apparently in this point there is no schema. If I run the same code with a https url, this code is never reached. It seems that getting up to point there is bad and the proxy is not used (edited to apply formatting)",Could you share a code snippet to reproduce this issue?, python  (edited to apply formatting)
openscad/openscad,https://github.com/openscad/openscad/issues/3150,openscad_openscad_issues_3150,"GUI crash with CGAL assertion

Combining details from Forum & #2351, @tturpin reports a crash in the GUI, this issue is about the crash not poly issues. The the following two lines: (a malformed poly)  gives the following output:   _Originally posted by @tturpin in https://github.com/openscad/openscad/issues/2351#issuecomment-560017797_ @tturpin, I'm not a linux guru, but we have some here. It looks like the GUI is not catching the exception, I get ERROR: CGAL error in in the GUI console without loosing the window. Please post the output from Help/Library-info or openscad --info. <bountysource-plugin> --- Want to back this issue? **[Post a bounty on it!](https://www.bountysource.com/issues/84685047-gui-crash-with-cgal-assertion?utm_campaign=plugin&utm_content=tracker%2F52063&utm_medium=issues&utm_source=github)** We accept bounties via [Bountysource](https://www.bountysource.com/?utm_campaign=plugin&utm_content=tracker%2F52063&utm_medium=issues&utm_source=github). </bountysource-plugin>",Might be #2847?,<bountysource-plugin> --- Want to back this issue? **[Post a bounty on it!](https://www.bountysource.com/issues/84685047-gui-crash-with-cgal-asertion?utm_campaign=plugin&utm_content=tracker%2F52063&utm_medium=issues&utm_source=github)** We accept bounties via [Bountysource](https://www.bountysource.com/?utm_campaign=plugin&utm_content=tracker%2F52063&utm_medium=issues&utm_source=github). </bountysource-plugin>
openscad/openscad,https://github.com/openscad/openscad/issues/2861,openscad_openscad_issues_2861,"Endless function recursion crashes RC2

Endless function recursion crashes 2019.01-RC2 under Windows 8.1 p=[[0,0,2],[0,20,4]];//points echo(vec2(p)); function vec2(A) = A[0][0]==undef?[A[0], A[1]]:[for(a=A) vec2(A)];","Could you please check with RC3? This should be fixed by #2828. I've checked on Linux with latest master, no crash there.",<bountysource-plugin> --- Want to back this issue? **[Post a bounty on it!](https://www.bountysource.com/issues/71253019-endless-function-recursion-crashes-rc2?utm_campaign=plugin&utm_content=tracker%2F52063&utm_medium=issues&utm_source=github)** We accept bounties via [Bountysource](https://www.bountysource.com/?utm_campaign=plugin&utm_content=tracker%2F52063&utm_medium=issues&utm_source=github). </bountysource-plugin>
openscad/openscad,https://github.com/openscad/openscad/issues/2358,openscad_openscad_issues_2358,"Baidu engine detects ""Win32.Trojan.WisdomEyes.16070401."" in OpenSCAD-2015.03-2-x86-64-Installer.exe

Firstly thanks for a great tool! So far I've only installed OpenSCAD on a Hyper-V VM because the Baidu engine on virustotal.com detected the ""Win32.Trojan.WisdomEyes.16070401."" in OpenSCAD-2015.03-2-x86-64-Installer.exe download from http://www.openscad.org/downloads.html. My local scanners - Windows Defender on Windows 10 & Malwarebytes - didn't detect anything. I tried to submit a false positive report to Baidu but the web interface didn't work (Edge & Chrome) (http://antivirus.baidu.com/en/submit-url.php). Link to the Virustotal scan: https://www.virustotal.com/#/file/316b95c1fc029bdf493b752d9917cd895dbfb5ddf9a4561b27efb3728d3d74e4/detection PS: I wonder why the Virustotal SandBox (Behavior tab) finds a System.dll file written to.",Do you use this plugin to do anything during installation?,<bountysource-plugin> --- Want to back this issue? **[Post a bounty on it!](https://www.bountysource.com/issues/57409103-baidu-engine-detects-win32-trojan-wisdomeyes-16070401-in-openscad-2015-03-2-x86-64-installer-exe?utm_campaign=plugin&utm_content=tracker%2F52063&utm_medium=issues&utm_source=github)** We accept bounties via [Bountysource](https://www.bountysource.com/?utm_campaign=plugin&utm_content=tracker%2F52063&utm_medium=issues&utm_source=github). </bountysource-plugin>
openscad/openscad,https://github.com/openscad/openscad/issues/2536,openscad_openscad_issues_2536,"Editor/console window width resets frequently

i just upgraded to OpenSCAD version 2018.10.13.ci145 (git 022f50e9) and have run into an extremely annoying bug. I am using the portable/zip package if that makes a different. It seems like every time the render window updates, the window layout snaps back to default/starting width (which makes the editor about 30 characters wide on my screen). Here are some specific tasks that cause this: * refresh after a change to geometry (ie, it has to redraw the object), or compile error * rotate/zoom/pan/etc in preview window. * resizing application window. It has been a while since i updated(2015.3-2) , but the window layout persisted even through restarts then.",Can you please have a look if that fixes the issue? 64bit: https://circleci.com/gh/openscad/openscad/706#artifacts 32bit: https://circleci.com/gh/openscad/openscad/707#artifacts,<bountysource-plugin> --- Want to back this issue? **[Post a bounty on it!](https://www.bountysource.com/issues/65266987-editor-console-window-width-resets-frequently?utm_campaign=plugin&utm_content=tracker%2F52063&utm_medium=issues&utm_source=github)** We accept bounties via [Bountysource](https://www.bountysource.com/?utm_campaign=plugin&utm_content=tracker%2F52063&utm_medium=issues&utm_source=github). </bountysource-plugin>
openscad/openscad,https://github.com/openscad/openscad/issues/2546,openscad_openscad_issues_2546,"Feature request: add capacity to specify surface from calculated values

Currently the surface function will give you a surface from either a png or a dat file, but you cannot generate a function-defined set of values within openscad and use that as an input to surface. It would be nice to be able to do this, either by leveraging the code in surface and determining whether a data(?) parameter were present and using that in place of file, warning when both exist, or alternatively calculating a set of minimal set of triangles from a set of 3R points and using that to mesh (the latter is obviously much more work). <bountysource-plugin> --- Want to back this issue? **[Post a bounty on it!](https://www.bountysource.com/issues/65445825-feature-request-add-capacity-to-specify-surface-from-calculated-values?utm_campaign=plugin&utm_content=tracker%2F52063&utm_medium=issues&utm_source=github)** We accept bounties via [Bountysource](https://www.bountysource.com/?utm_campaign=plugin&utm_content=tracker%2F52063&utm_medium=issues&utm_source=github). </bountysource-plugin>",What are the example scenarios for this?,<bountysource-plugin> --- Want to back this issue? **[Post a bounty on it!](https://www.bountysource.com/issues/65445825-feature-request-add-capacity-to-specify-surface-from-calculated-values?utm_campaign=plugin&utm_content=tracker%2F52063&utm_medium=issues&utm_source=github)** We accept bounties via [Bountysource](https://www.bountysource.com/?utm_campaign=plugin&utm_content=tracker%2F52063&utm_medium=issues&utm_source=github). </bountysource-plugin>
openscad/openscad,https://github.com/openscad/openscad/issues/2603,openscad_openscad_issues_2603,"New windows are very tiny on high-DPI monitors on Windows

When opening files or just starting the program on a high-dpi monitor, windows are created tiny and icons are unnecessarily small. I'm running at 225% scale in Windows settings. The windows can be manually resized, but this doesn't fix the icon problem and is simply a bit annoying. Here's what the windows look like on my machine. Note that while they look big here on Github because it scales pictures to some kind of webpage DPI, when the screenshots were taken on my display they were very tiny. ![intro window](https://user-images.githubusercontent.com/130929/48970989-7d7a9d80-f014-11e8-8ce5-9975f5a2168c.JPG) ![high dpi small window](https://user-images.githubusercontent.com/130929/48970990-7f446100-f014-11e8-8578-7cfd38da4974.JPG) ![small config window](https://user-images.githubusercontent.com/130929/48970993-823f5180-f014-11e8-98b5-962a571a0050.JPG) <bountysource-plugin> --- Want to back this issue? **[Post a bounty on it!](https://www.bountysource.com/issues/66595687-new-windows-are-very-tiny-on-high-dpi-monitors-on-windows?utm_campaign=plugin&utm_content=tracker%2F52063&utm_medium=issues&utm_source=github)** We accept bounties via [Bountysource](https://www.bountysource.com/?utm_campaign=plugin&utm_content=tracker%2F52063&utm_medium=issues&utm_source=github). </bountysource-plugin>",Can you please try how the [development snapshots](http://www.openscad.org/downloads.html#snapshots) behave? Those should come with a newer version of that framework which is supposed to handle per monitor DPI scaling.,<bountysource-plugin> --- Want to back this issue? **[Post a bounty on it!](https://www.bountysource.com/issues/66595687-new-windows-are-very-tiny-on-high-dpi-monitors-on-windows?utm_campaign=plugin&utm_content=tracker%2F52063&utm_medium=issues&utm_source=github)** We accept bounties via [Bountysource](https://www.bountysource.com/?utm_campaign=plugin&utm_content=tracker%2F52063&utm_medium=issues&utm_source=github). </bountysource-plugin>
openscad/openscad,https://github.com/openscad/openscad/issues/2636,openscad_openscad_issues_2636,"Unnecessarily slow Preview

I had been using the Nightly as the last prod release was (presumably still is as no re-release) flickering badly with a very large model. However, during this year I've found Preview mode very slow, and have been ignoring the Nightly and using a 2018.02 AppImage I happened to have downloaded to try. This version has 2 advantages - fast Preview and double-click to set view focus (rotation, zoom centre). A later AppImage from 2018.04 is much more like the current Nightly. For comparison, for models above a certain complexity, Preview rendering time is 4s for 2018.02 against an unpleasant/unusable 36s for Nightly. If it matters I'm using Mint 18.3 MATE 64bit. I've been able to reproduce the problem with a trivial .scad file, in which the complexity can be changed by altering a single value, and a loop then generates some arbitrary geometry. Hopefully this works for others, the numbers may be different depending on CPU speed etc I guess. See attached. [progress_bar_36s_preview_time.scad.txt](https://github.com/openscad/openscad/files/2682670/progress_bar_36s_preview_time.scad.txt) My conclusion is that with the 2018.02 version, rendering time was up to 1s for the model plus up to 3s for the progress bar. And that for the current nightly, the same models take up to 2s for the model plus up to ~35s for the progress bar. (The model rendering time probably scaling linearly with complexity, the progress bar time linearly with the amount of time or fraction it is displayed). Clearly having the progress bar take 3x (old) up to 15x (latest) the actual activity needs addressing, however my recommendation is that the progress bar is never shown for Preview, it's just too fast without! Presumably the progress bar for Render needs to be checked, in case it's wasting time there too. Don't know why the newer version takes 2s rather than 1s for a big model, but that's secondary to the main issue. If possible, it would be good if the old view is cleared from the screen as late as possible, so the new preview just appears directly after it, without a 'flash' of cleared screen between. That way the effect of tweaking a value and re-Preview can be seen the best. I'd also love to see the return of double-click to set view focus too, if that's still possible (or ctrl-click or anything similar if there's a problem with double-click).",What graphic card and driver are you using? 9999 works fine on Ubuntu 18.10 with an NVidia 710 and Proprietary driver.,<bountysource-plugin> --- Want to back this issue? **[Post a bounty on it!](https://www.bountysource.com/issues/67566276-unnecessarily-slow-preview?utm_campaign=plugin&utm_content=tracker%2F52063&utm_medium=issues&utm_source=github)** We accept bounties via [Bountysource](https://www.bountysource.com/?utm_campaign=plugin&utm_content=tracker%2F52063&utm_medium=issues&utm_source=github). </bountysource-plugin>
openscad/openscad,https://github.com/openscad/openscad/issues/2880,openscad_openscad_issues_2880,"Compilation from source on Mac fails on -lqscintilla2_qt4

Getting error om compilation on Mac OSX High Sierra trying to get the Post Production Carve tooling to work. Anybody had succes with it? $ qmake CONFIG+=""experimental carving -qt=qt5"" OPENSCAD_LIBDIR=/usr/local $ make ApplicationServices are out of sync. Falling back to library file for linking. ld: library not found for -lqscintilla2_qt4 clang: error: linker command failed with exit code 1 (use -v to see invocation)","What is that carving -qt=qt5 part? If you are doing very specialized build, you really need to add a bit more context.",<bountysource-plugin> --- Want to back this issue? **[Post a bounty on it!](https://www.bountysource.com/issues/71607876-compilation-from-source-on-mac-fails-on-lqscintilla2_qt4?utm_campaign=plugin&utm_content=tracker%2F52063&utm_medium=issues&utm_source=github)** We accept bounties via [Bountysource](https://www.bountysource.com/?utm_campaign=plugin&utm_content=tracker%2F52063&utm_medium=issues&utm_source=github). </bountysource-plugin>
openscad/openscad,https://github.com/openscad/openscad/issues/2910,openscad_openscad_issues_2910,"text() do not support Chinese

Change Chinese to unicode can not display too.",Which version? The latest snapshot seems to: ![image](https://user-images.githubusercontent.com/566149/55326443-ae198a80-547f-11e9-847d-9ae688e73fd2.png),<bountysource-plugin> --- Want to back this issue? **[Post a bounty on it!](https://www.bountysource.com/issues/72220490-text-do-not-support-chinese?utm_campaign=plugin&utm_content=tracker%2F52063&utm_medium=issues&utm_source=github)** We accept bounties via [Bountysource](https://www.bountysource.com/?utm_campaign=plugin&utm_content=tracker%2F52063&utm_medium=issues&utm_source=github). </bountysource-plugin>
openscad/openscad,https://github.com/openscad/openscad/issues/3116,openscad_openscad_issues_3116,"Support for lib3mf 2

Version 2 of lib3mf has been [released on 2019-09-26](https://github.com/3MFConsortium/lib3mf/releases/tag/v2.0.0). This is a major release that breaks ABI compatibility, see the changelog: (the defunct COM interface has thus been removed) Since OpenSCAD relies on the mentioned COM interface to allow import and export of 3MF files, it doesn't build with the most recent version of lib3mf. It would be nice to make OpenSCAD compatible with the new version 2.0.0 in order to allow distributions to drop support for the legacy version 1.8.1. <bountysource-plugin> --- Want to back this issue? **[Post a bounty on it!](https://www.bountysource.com/issues/83074047-support-for-lib3mf-2?utm_campaign=plugin&utm_content=tracker%2F52063&utm_medium=issues&utm_source=github)** We accept bounties via [Bountysource](https://www.bountysource.com/?utm_campaign=plugin&utm_content=tracker%2F52063&utm_medium=issues&utm_source=github). </bountysource-plugin>",Which distribution supports lib3mf 2.0? So far it's enough struggle to have 1.8 supported.,<bountysource-plugin> --- Want to back this issue? **[Post a bounty on it!](https://www.bountysource.com/issues/83074047-support-for-lib3mf-2?utm_campaign=plugin&utm_content=tracker%2F52063&utm_medium=issues&utm_source=github)** We accept bounties via [Bountysource](https://www.bountysource.com/?utm_campaign=plugin&utm_content=tracker%2F52063&utm_medium=issues&utm_source=github). </bountysource-plugin>
openscad/openscad,https://github.com/openscad/openscad/issues/3243,openscad_openscad_issues_3243,"Customizer vectors round floats with initial integer

In customizer: v = [1.1, 2.2, 3.7]; // yields float spinboxes v = [1.0, 2.2, 3.7]; // and v = [1, 2.2, 3.7]; // yield int spinboxes, rounding the last two. This is probably a simple fix, but could also be considered together with #3012.",Should I go ahead?,<bountysource-plugin> --- Want to back this issue? **[Post a bounty on it!](https://www.bountysource.com/issues/89843563-customizer-vectors-round-floats-with-initial-integer?utm_campaign=plugin&utm_content=tracker%2F52063&utm_medium=issues&utm_source=github)** We accept bounties via [Bountysource](https://www.bountysource.com/?utm_campaign=plugin&utm_content=tracker%2F52063&utm_medium=issues&utm_source=github). </bountysource-plugin>
openscad/openscad,https://github.com/openscad/openscad/issues/3260,openscad_openscad_issues_3260,"CLI options affecting GUI mode

Based on my observation in https://github.com/openscad/openscad/issues/3256#issuecomment-604175639_ and the ensued discussion it is clear that there are different views on how CLI options should or should not affect OpenSCAD's GUI mode. @nophead: @t-paul: @MichaelAtOz The following could be perceived as if I am cocky and demanding - it is just my personal view. I don't understand the thinking behind having different options for GUI and CLI. I'd expect rather the contrary (and that's what I strive to implement when I work on something): There should be no action available on CLI that cannot also be made in the GUI (generally also the other way round would be great but that's usually not feasible - I cannot think of much in OpenSCAD that couldn't be done from CLI though since most of its interaction is text-based via the editor anyway). Therefore I think an application should do whatever I specify at the CLI no matter how it is displayed eventually. Differentiating between different output modes unnecessarily makes the code more complicated in my experience and also requires additional documentation. The latter seems to be the main issue with OpenSCAD because AFAICT there is no clear indication which CLI arguments have an effect in GUI mode. <bountysource-plugin> --- Want to back this issue? **[Post a bounty on it!](https://www.bountysource.com/issues/90652975-cli-options-affecting-gui-mode?utm_campaign=plugin&utm_content=tracker%2F52063&utm_medium=issues&utm_source=github)** We accept bounties via [Bountysource](https://www.bountysource.com/?utm_campaign=plugin&utm_content=tracker%2F52063&utm_medium=issues&utm_source=github). </bountysource-plugin>",What would they do in GUI mode when passed from the command line? Would they override the registry and would they write back to it?,<bountysource-plugin> --- Want to back this issue? **[Post a bounty on it!](https://www.bountysource.com/issues/90652975-cli-options-affecting-gui-mode?utm_campaign=plugin&utm_content=tracker%2F52063&utm_medium=issues&utm_source=github)** We accept bounties via [Bountysource](https://www.bountysource.com/?utm_campaign=plugin&utm_content=tracker%2F52063&utm_medium=issues&utm_source=github). </bountysource-plugin>
openscad/openscad,https://github.com/openscad/openscad/issues/3298,openscad_openscad_issues_3298,"Using rotate_extruded torus in difference causes assert

This code (simplified) produces assert error when rendering the object difference(){ translate([-2.5,-2.5,0])cube([5,5,5]); rotate_extrude(convexity=10)translate([5,0,0])circle(r=5);} The object is shown normally without an assert when using preview though. <bountysource-plugin> --- Want to back this issue? **[Post a bounty on it!](https://www.bountysource.com/issues/91329027-using-rotate_extruded-torus-in-difference-causes-assert?utm_campaign=plugin&utm_content=tracker%2F52063&utm_medium=issues&utm_source=github)** We accept bounties via [Bountysource](https://www.bountysource.com/?utm_campaign=plugin&utm_content=tracker%2F52063&utm_medium=issues&utm_source=github). </bountysource-plugin>",Does rotate_extruded need to be nurfed by always auto clipping everything x =< 0 ? Nah wouldn't fix single vertex on center line anyways.,<bountysource-plugin> --- Want to back this issue? **[Post a bounty on it!](https://www.bountysource.com/issues/91329027-using-rotate_extruded-torus-in-difference-causes-assert?utm_campaign=plugin&utm_content=tracker%2F52063&utm_medium=issues&utm_source=github)** We accept bounties via [Bountysource](https://www.bountysource.com/?utm_campaign=plugin&utm_content=tracker%2F52063&utm_medium=issues&utm_source=github). </bountysource-plugin>
pallets/flask,https://github.com/pallets/flask/issues/3187,pallets_flask_issues_3187,"Catch-all route does not work if url has another handler even if method is different

I want to set up a handler for all URLs that will handle the OPTIONS preflight requests Following the ""catch-all"" example (@app.route(""/<path:path>"", methods=['OPTIONS'])) I found that if the server gets a route that has a handler with a different method neither of the handlers are called ### Expected Behavior The OPTIONS request should have been routed to the ""wildcard"" OPTIONS handler  ### Actual Behavior The request was not routed to the correct handler OPTIONS handler ### Environment * Python version:3.7 * Flask version:1.0.2 * Werkzeug version:0.15.2",Can you use an @app.before_request func to accomplish this instead?,"handler python from unittest import TestCase from flask import Flask from hamcrest import assert_that, equal_to class TestSitePreferencesService(TestCase): def test(self): app = Flask(__name__) @app.route(""/<path:path>"", methods=['OPTIONS this is expected OPTIONS handler"
rubygems/rubygems,https://github.com/rubygems/rubygems/issues/3523,rubygems_rubygems_issues_3523,"Wrong version of bundler is used if there are 2 versions

For example: Project: [diaspora](https://github.com/diaspora/diaspora) I have system Ruby 2.5.1 (reproducible with rvm either). I have bundlers 2.1.2 and 1.17.3 installed and run bundle install -> lockfile is bundled with 2.1.2 Reproduce: 1. Ensure you have bundler versions pre 2.0 and 2+ 2. Run bundle install -> 2+ version is used (see BUNDLED WITH in lockfile) 3. Run Rails application without bundle exec Expected: Bundler version from bundled with is used. (2+ here) Actual: Older one is used and hence we have an error: You must use Bundler 2 or greater with this lockfile.","Could you post repro steps, please? Thanks.",2+ here) Actual: Older one is used and hence
jsdom/jsdom,https://github.com/jsdom/jsdom/issues/2477,jsdom_jsdom_issues_2477,"Accept custom characters for attributes name

### Basic info: - **Node.js v10.13.0:** <!-- only v8 and above are supported --> - **jsdom 12.2.0:** <!-- only v12 and above are supported --> ### Minimal reproduction case  <!-- Please create a minimal repro. Any reports involving third party libraries will be closed, as we cannot debug third-party library interactions for you. Please do not use syntax that is not supported in Node.js, such as JSX or import statements. If we cannot run the code in Node.js, we will close the issue, as we cannot debug whatever toolchain you are using. --> I know this is not a standard naming of attributes, but can the thrown exception in /node_modules/jsdom/lib/jsdom/living/nodes/Element-impl.js line **240** become an optional For example  The reason for this that i'm using Jsdom for compiling html files and some times some attributes are exchanged with another ones. Also elements could be duplicated with same attributes but with extra/minimal attributes. Mainly i use [custom-attribute] and (custom-event) as attributes for any element so it would be easier to maintain it with same syntax. ### How does similar code behave in browsers? It is not rendered in the browser as the passed html to jsdom is not being displayed directly to the browser. The code is transformed to dom elements so the passed code will be handled only by the dom compiler.",How does this code behave in browsers? You removed that crucial portion of the issue template.,### How does similar code behave in browsers? It is not rendered in the browser as the passed html to jsdom is not being displayed directly to the browser. The code is transformed to dom elements so the passed code will be handled only by the dom compiler.
jsdom/jsdom,https://github.com/jsdom/jsdom/issues/2607,jsdom_jsdom_issues_2607,"JSdom is passing CSS and JS even when they are turned OFF

node v8.10.0 jsdom v11.3.0 when a page load async JS and CSS via HTTP headers LINK field (preload), JSdom is passing this out - and edit you own CSS styles for example. If page you test has: * {background:red;} you rendered page has a red background too... (those setting are used):  No FetchExternalResources, ProcessExternalResources are used - SkipExternalResources is set to true (features: {} - object). Any idea how to resolve that problem? Maybe some code here will help to get the idea:  Code for new version (15) - https://runkit.com/embed/bqdu9la7ezaw",Can you post an example following the issue template?,Maybe some code here will help to get the idea:  Code for new version (15) - https://runkit.com/embed/bqdu9la7ezaw
libgit2/libgit2,https://github.com/libgit2/libgit2/issues/5475,libgit2_libgit2_issues_5475,"git_reference_target_peel returns NULL

### Reproduction steps   ### Expected behavior peeled should contain a pointer to the commit the hard tag refers to. ### Actual behavior peeled is NULL ### Version of libgit2 (release number or SHA1) ca782c913b7052712c7a6163c102a01733202ebf ### Operating system(s) tested ubuntu 18.04 Bug is very similar to https://github.com/libgit2/libgit2/issues/4355","Can you provide some code, or a test that illustrates more details about the failure that you're seeing?",Create a repository with a packed tag  
teeworlds/teeworlds,https://github.com/teeworlds/teeworlds/issues/2003,teeworlds_teeworlds_issues_2003,"Server address should always match the highlighted server

In some cases the (displayed) server address will not match the highlighted server. Thinking that the correct server is already chosen, I press [Connect] to join. But in the end I will hop on a very different server. The server address should always match the highlighted server. **Edit:** Ways to reproduce: * Select a server from _Global_. Switch to _Local_. Select a local server. Switch back to _Global_. The prior selected server from _Global_ is still selected, but the server address points to the local one. * Select a server. Change the server address. The prior selected server is still selected. ------ Teeworlds: 0.7.1 OS: Linux Mint 19.1, Cinnamon Edition","Should it really instantly deselect anything in the browser? I can see the issue, but I don't see an elegant answer",". **Edit:** Ways to reproduce: * Select a server from _Global_. Switch to _Local_. Select a local server. Switch back to _Global_. The prior selected server from _Global_ is still selected, but the server address points to the local one. * Select a server. Change the server address. The prior selected server is still selected"
teeworlds/teeworlds,https://github.com/teeworlds/teeworlds/issues/1974,teeworlds_teeworlds_issues_1974,"inp_grab 0 causes cursor being stuck in top/left corner

TW Version 0.7.2 Environment: Archlinux + i3 SDL2 version: 2.0.9 inp_grab 0 causes the cursor to snap into the upper left corner after the first mouse input is registered. 0.6.X is also affected on my system. edit: inp_grab 1 -> inp_grab 0. Dune is correct, i mixed it up. edit2: added the SDL2 version",Does inp_grab 0 work for you instead?,0 causes the cursor to snap into the upper left corner after the first mouse input is registered. 0.6.X is also affected on my system. edit: inp_grab
bndtools/bnd,https://github.com/bndtools/bnd/issues/3410,bndtools_bnd_issues_3410,"[launcher] Support per-bundle reference directive

The launcher will by default try to use ""reference:"" url schema to install bundles into the running OSGi instance, which is much more efficient because the framework doesn't have to copy the bundles but references them in their existing locations. The -runnoreference directive allows you to override this if necessary. Unless you're on Windows, in which case it will never use the ""reference:"" schema. The documentation mistakenly says that the default on Windows is not to use reference:, but the definition of Launcher.useReferences() will always return ~~true~~false on Windows. (*Edited: originally I wrote true when it should have been false.*) There are good reasons for this. Unfortunately, the frameworks typically hold a lock on the bundles that they install, which means that you can't build them while the framework is running, which means you can't get hot updates. However, when you are testing a framework with lots of bundles there is a high startup cost associated with it. I would like to propose a compromise implementation that allows you to specify ""noreference"" semantics on a per-bundle basis. So this would become an attribute of the runbundles property (similar to the startlevel attribute). This would be useful because in any given test deployment, a large percentage of the bundles are quasi-static - it's really only the bundles in the Workspace repository that might change. An optimum setup on Windows would be to configure all of the Workspace bundles as ""noreference"" and the rest of the bundles as ""reference"". This way you could get 95% of the performance benefit of reference bundles but still allow your hot updates to work.",Can you explain why you think this is the case? If isWindows() returns true then !isWindows() is false and false && anything is false.,(*Edited: originally I wrote true when it should have been false.*)
bndtools/bnd,https://github.com/bndtools/bnd/issues/3903,bndtools_bnd_issues_3903,"bnd-maven-plugin ConcurrentModificationException on latest OpenJDK 15 EA

I've been using the bnd-maven-plugin for a long time with no issues and have not made any recent configuration changes. Beginning tonight, the Travis CI build is failing on the OpenJDK15 (early access) build with this error: The Java version used is:  Link to failing Travis CI job: https://travis-ci.org/github/oshi/oshi/jobs/673288964 It worked as recently as a two days ago on a slightly earlier EA build:  Link to passing Travis CI job from 2 days ago: https://travis-ci.org/github/oshi/oshi/jobs/672568551 Other relevant version strings:  Relevant portions of my POM configuration:  Happy to help troubleshoot but I have no idea where to even start!",Would it be possible for you to re-run the build with -X so that you could give us the full stacktrace? Thanks for reporting!,Other relevant version strings: 
mRemoteNG/mRemoteNG,https://github.com/mRemoteNG/mRemoteNG/issues/921,mRemoteNG_mRemoteNG_issues_921,"Add support for excluding certain active connections from multi ssh commands

*Original Description:* Requesting feature to disable/enable accepting commands for ""Multi ssh"" ------ *Merging request from #1100:* Send command using the MultiSSH toolbar only to the SSH connection inside the currently focused Connection Panel. Preferably optional by checkbox. ## Expected Behavior It would be nice if there would be an option to limit the command forwarding only to the SSH connections, which are in the currently focused Connection Panel. ## Current Behavior Currently the MultiSSH toolbar will send the command to all open SSH connections, independent of the Connection Panel. ## Possible Solution Add checkbox to the MultiSSH toolbar to limit command forwarding only to the SSH connection in the current Connection Panel. ## Context We have SSH connections to different Docker machines. It would save time to have all connections open but to organize them into Connection Panels and to send commands based on the panels.",Can you provide more context around this request? Do you want to disable multi ssh for all connections or just certain ones?,"*Original Description:* ------ *Merging request from #1100:* Send command using the MultiSSH toolbar only to the SSH connection inside the currently focused Connection Panel. Preferably optional by checkbox. ## Expected Behavior It would be nice if there would be an option to limit the command forwarding only to the SSH connections, which are in the currently focused Connection Panel. ## Current Behavior Currently the MultiSSH toolbar will send the command to all open SSH connections, independent of the Connection Panel. ## Possible Solution Add checkbox to the MultiSSH toolbar to limit command forwarding only to the SSH connection in the current Connection Panel. ## Context We have SSH connections to different Docker machines. It would save time to have all connections open but to organize them into Connection Panels and to send commands based on the panels."
nvm-sh/nvm,https://github.com/nvm-sh/nvm/issues/1942,nvm-sh_nvm_issues_1942,"install script does not detect if git is installed or not

- Operating system and version: Ubuntu bionic - nvm debug output: n/a - nvm ls output: n/a - How did you install nvm? (e.g. install script in readme, Homebrew): via the install script, but git **was not** installed - What steps did you perform? as mentioned in the doc, but git **was not** installed - What happened?  - What did you expect to happen?  - Is there anything in any of your profile files (.bashrc, .bash_profile, .zshrc, etc) that modifies the PATH? no",What do echo $METHOD and type git and which git print out?,", but git **was not** installed"
nvm-sh/nvm,https://github.com/nvm-sh/nvm/issues/1994,nvm-sh_nvm_issues_1994,"How does NVM works in non-interactive non-login shell?

Sorry that, the question got created (half way) when I pressed enter by mistake. @ljharb mentioned at [Issue 1959 ](https://github.com/creationix/nvm/issues/1959#issuecomment-447050358) that nvm doesn’t require an interactive shell, but it does require a login shell. But how does it work in a non-interactive shell given that only in .bashrc where nvm script is sourced, is not read for a non-interactive shell. You said that a login shell is required. npm cli global packages are defined only if nvm is defined. I have used the command from these global packages in a non interactive non login script. But it seems to be working. I need to understand that because, I am trying out something.","Do you have a specific question? nvm should work fine in a non-interactive shell, since there's nothing that requires interaction in it.","@ljharb mentioned at [Issue 1959 ](https://github.com/creationix/nvm/issues/1959#issuecomment-447050358) that nvm doesn’t require an interactive shell, but it does require a login shell. But how does it work in a non-interactive shell given that only in .bashrc where nvm script is sourced, is not read for a non-interactive shell. You said that a login shell is required. npm cli global packages are defined only if nvm is defined. I have used the command from these global packages in a non interactive non login script. But it seems to be working. I need to understand that because, I am trying out something."
nvm-sh/nvm,https://github.com/nvm-sh/nvm/issues/2185,nvm-sh_nvm_issues_2185,"warning: Could not find remote branch v0.35.3 to clone.

I have been trying to install nvm using both the curl and install.sh scripts on OS X High Sierra (10.13.6) and I keep getting the message: warning: Could not find remote branch v0.35.3 to clone. fatal: Remote branch v0.35.3 not found in upstream origin Failed to clone nvm repo. Please report this!",What version of git are you using? nvm requires v1.7.10 (see the note just above https://github.com/nvm-sh/nvm#git-install),on OS X High Sierra (10.13.6)
overviewer/Minecraft-Overviewer,https://github.com/overviewer/Minecraft-Overviewer/issues/1577,overviewer_Minecraft-Overviewer_issues_1577,"No such file or directory: 'tmprcobkz'

**EDIT: I'm sorry for the post, it was a file permission issue on my computer!** So this is the first time I'm generating markers out of signs, but the following error appears: ![Screenshot_1](https://user-images.githubusercontent.com/5015666/58302562-32bec000-7dc2-11e9-8487-64b8d4b2b4a7.png) It seems to be processing the region files OK but the generation of the JS files is not working.",What filesystem is the output directory on?,"EDIT: I'm sorry for the post, it was a file permission issue on my computer!"
Atlantic18/DoctrineExtensions,https://github.com/Atlantic18/DoctrineExtensions/issues/2095,Atlantic18_DoctrineExtensions_issues_2095,"Get child nodes by id with all parents not possible (NestedTreeRepository)?

**Goal:** I have a category entity. Every category can have subcategories. Now I want to get specific category nodes by id with *all* its parents **My current solution (custom repository function):** public function getChildrenHierarchyByCategoryIDs($categoryIDsArray) { $qb = $this->getNodesHierarchyQueryBuilder(); $qb->andWhere('node.id in (:categoryIDs)') ->setParameter('categoryIDs', $categoryIDsArray) ->orWhere('node.id in (:rootNodes)') // ""Workaround"", get all available root nodes (1st level only) manually, because I don't know how to get them by children's parentID (node.parent) ""automatically"" (with second level etc.) ->setParameter('rootNodes', $this->getRootNodes()) ; $aComponents = $qb->getQuery()->getResult(\Doctrine\ORM\Query::HYDRATE_ARRAY); return $this->buildTreeArray($aComponents); } **Question:** How can I get the parents (of all nodes found by id) automatically, instead of fetching every possible parent manually by id (See ""Workaround"" in code)? I can't find any solution.. I think it should be something like: ->orWhere('node.id in (node.parent)') which is not working. ([Syntax Error] line 0, col 89: Error: Expected Literal, got 'node') Thanks in advance for taking your time!",Do you maybe need the getPath() method? Example: https://github.com/Atlantic18/DoctrineExtensions/blob/v2.4.x/doc/tree.md#using-repository-functions,"(with second level etc.) ->setParameter('rootNodes', $this->getRootNodes())"
buildbot/buildbot,https://github.com/buildbot/buildbot/issues/4805,buildbot_buildbot_issues_4805,"Buildbot upgrade-master keeps showing up

Hi guys, I am trying to start buildbot in my vagrant machine which is running ubuntu/bionic64 on my windows 10 machine. I followed http://docs.buildbot.net/latest/tutorial/firstrun.html but when running buildbot start master I get ""buildbotNetUsageData is not configured and defaults to basic"" So I add c['buildbotNetUsageData'] = None to the master.cfg and restart the master by running buildbot restart master. Now the first error is gone but I get  So I run  Which returns  Now I restart the buildbot by running buildbot restart master But I get the upgrade-master error again. No matter how often I upgrade master the error keeps showing up  And create the vagrant box by running vagrant up master","Could you give more information about how you acquired buildbot? Which version of buildbot is it, how you configured the master first time and so on?", And create the vagrant box by running vagrant up master
Automattic/node-canvas,https://github.com/Automattic/node-canvas/issues/1443,Automattic_node-canvas_issues_1443,"Hebrew Fonts breaking with specific fonts Friz-Quadrata with measureText

<!--- Having trouble installing node-canvas? Please make sure you have read the installation instructions located here before asking for help: https://github.com/Automattic/node-canvas#installation Still having problems, found a bug or want a feature? Fill out the form below. --> <!--- Provide a general summary of the issue in the Title above --> ## Issue <!--- Provide info about the bug or feature. --> I'm using node-canvas to convert from some legacy fonts to newer google fonts. I ran across this text in hebrew: יוסי האפס :) that will break when I try to render it with a font called friz-quadrata in bold [http://fontsgeek.com/fonts/Friz-Quadrata-Bold](url) ## Steps to Reproduce - Register friz-quadrata bold - set canvas font to the friz-quadrata with this text string יוסי האפס :) - attempt to ctx.measureText ## Your Environment * Version of node-canvas (output of npm list canvas or yarn list canvas): canvas@2.5.0 * Environment (e.g. node 4.2.0 on Mac OS X 10.8): node v10.15.3 Mac OS X 10.14.5 Error that gets thrown  UPDATE: The issue was solved by updating the fonts I was using with fonts that had correct table contents. measureText breaks with international languages with font files that don't have the correct table data.",Does it still happen when you change the text? Does it happen when you change the font it uses?,UPDATE: The issue was solved by updating the fonts I was using with fonts that had correct table contents. measureText breaks with international languages with font files that don't have the correct table data.
Automattic/node-canvas,https://github.com/Automattic/node-canvas/issues/1565,Automattic_node-canvas_issues_1565,"Stretched or bolded font in rendered canvas on node js is off by a lot of pixels.

I'm sending the canvas data to the node js server and doing a fabric.util.enlivenObjects to rerender the canvas. I'm converting the canvas to jpeg/png using the sharp library. But the fonts that are stretched or bolded are always off by a lot of pixels in the rendered image. I can confirm that this works fine when I render the canvas on my client application (AngularJS). Also, this is not a problem with sharp. I tried a couple of other libraries, they all gave me the same output. Reproduction script: Here, canvasObjects is the canvas state.  After the canvas is rendered, I convert it into a jpeg image with sharp.  The actual canvas on my client application looks like this: https://ibb.co/G34X2Lz But the rendered jpeg on the node server looks like this: https://ibb.co/pz5Nrvh PS: Do not worry about the background. Just notice how the text is off. More info: This works fine when the text is not bolded or stretched. Nodejs 8.11.1, Canvas: latest version, Platform: Ubuntu","Can you post a reproduction script, and the info in the issue template please (Node.js version, Canvas version, platform)?","Reproduction script: Here, canvasObjects is the canvas state.  After the canvas is rendered, I convert it into a jpeg image with sharp.  The actual canvas looks like this: https://ibb.co/G34X2Lz But the rendered jpeg looks like this: https://ibb.co/pz5Nrvh PS: Do not worry about the background. Just notice how the text is off. More info: This works fine when the text is not bolded or stretched."
Pylons/pyramid,https://github.com/Pylons/pyramid/issues/3428,Pylons_pyramid_issues_3428,"Feature Request: Upload into PyPi package with all Pyramid tests, e.g. pyramid.tests.

How can I use in my project code form Pyramid tests after #3022 ? I have a project with extended version of pyramid.authorization.ACLAuthorizationPolicy. I am using TestACLAuthorizationPolicy test case to testing my implementation:  It would be cool if you upload into PyPi package with all Pyramid tests, e.g. pyramid.tests.",What did you try? Following HACKING.txt is the supported method of running Pyramid's tests.,in my project code form
MassTransit/MassTransit,https://github.com/MassTransit/MassTransit/issues/1758,MassTransit_MassTransit_issues_1758,"Azure Service Bus UnauthorizedException when using Managed Identity

When using Managed Identity to authenticate towards Azure Service Bus we get an UnauthorizedException when publishing a message with following error message: Microsoft.Azure.ServiceBus.UnauthorizedException: 'claim is empty. TrackingId:daa79edb-af0f-42d0-8583-b5d6006d1815_G5, SystemTracker:NoSystemTracker, Timestamp:2020-03-13T08:22:50' Gist to reproduce: https://gist.github.com/mathiasbl/9c779c8d09a3a2807b0d10679959d81b How to setup managed identity: https://docs.microsoft.com/en-us/azure/service-bus-messaging/service-bus-managed-service-identity My app and myself have the service bus owner role. MassTransit.Azure.ServiceBus.Core: 5.5.6 **Stack**: 2020-03-12 {""Message"":""claim is empty. TrackingId:dc3767cf-a902-4014-bd6d-df8e7f81ac26_G7, SystemTracker:NoSystemTracker, Timestamp:2020-03-12T09:45:23"", ""IsTransient"":false, ""Data"":{}, ""TargetSite"":""Void MoveNext()"", ""StackTrace"":"" at MassTransit.Azure.ServiceBus.Core.Contexts.NamespaceManager.<>c__DisplayClass25_01.<<RunOperation>b__0>d.MoveNext() --- End of stack trace from previous location where exception was thrown --- at MassTransit.Azure.ServiceBus.Core.Contexts.NamespaceManager.RunOperation[T](Func1 operation) at MassTransit.Azure.ServiceBus.Core.Pipeline.ConfigureTopologyFilter1.ConfigureTopology(NamespaceContext context) at MassTransit.Azure.ServiceBus.Core.Pipeline.ConfigureTopologyFilter1.<>c__DisplayClass6_0.<<Send>b__0>d.MoveNext() --- End of stack trace from previous location where exception was thrown --- at GreenPipes.PipeExtensions.OneTimeSetup[T](PipeContext context, Func2 setupMethod, PayloadFactory1 payloadFactory) at MassTransit.Azure.ServiceBus.Core.Pipeline.ConfigureTopologyFilter1.Send(NamespaceContext context, IPipe1 next) at GreenPipes.Agents.AsyncPipeContextPipe1.Send(TContext context) at GreenPipes.Agents.PipeContextSupervisor1.GreenPipes.IPipeContextSource<TContext>.Send(IPipe1 pipe, CancellationToken cancellationToken) at GreenPipes.Agents.PipeContextSupervisor1.GreenPipes.IPipeContextSource<TContext>.Send(IPipe1 pipe, CancellationToken cancellationToken) at GreenPipes.Agents.PipeContextSupervisor1.GreenPipes.IPipeContextSource<TContext>.Send(IPipe1 pipe, CancellationToken cancellationToken) at MassTransit.Azure.ServiceBus.Core.Pipeline.JoinContextFactory3.<>c__DisplayClass8_0.<<CreateJoinContext>g__Join|0>d.MoveNext() --- End of stack trace from previous location where exception was thrown --- at GreenPipes.Agents.PipeContextSupervisor1.GreenPipes.IPipeContextSource<TContext>.Send(IPipe1 pipe, CancellationToken cancellationToken) at GreenPipes.Agents.PipeContextSupervisor1.GreenPipes.IPipeContextSource<TContext>.Send(IPipe1 pipe, CancellationToken cancellationToken) at GreenPipes.Agents.PipeContextSupervisor1.GreenPipes.IPipeContextSource<TContext>.Send(IPipe1 pipe, CancellationToken cancellationToken) at MassTransit.Transports.PublishEndpoint.Publish[T](CancellationToken cancellationToken, T message, PublishEndpointPipeAdapter1 adapter) at MassTransit.Transports.PublishEndpoint.Publish[T](CancellationToken cancellationToken, T message, PublishEndpointPipeAdapter1 adapter)","what's your queue/topic name? I've read in some places that having ""."" in the name might cause some issues with RBAC","2020-03-12 {""Message"":""claim is empty. TrackingId:dc3767cf-a902-4014-bd6d-df8e7f81ac26_G7, SystemTracker:NoSystemTracker, Timestamp:2020-03-12T09:45:23"", ""IsTransient"":false, ""Data"":{}, ""TargetSite"":""Void MoveNext()"", ""StackTrace"":"" Func1 operation, TimeSpan operationTimeoutFunc1 operationTopicDescription topicDescriptionenIPipe1 pipe, CancellationToken cancellationTokenIPipe1 pipe, CancellationToken cancellationTokenen cancellationToken) at MassTransit.Transports.PublishEndpoint.Publish[T](CancellationToken cancellationToken, T message, PublishEndpointPipeTransports.PublishEndpoint.Publish[T](CancellationToken cancellationToken, T message, PublishEndpointPipe"
MassTransit/MassTransit,https://github.com/MassTransit/MassTransit/issues/1306,MassTransit_MassTransit_issues_1306,"Cleanup Logger on shutdown

We observe the following exception in tests where we create and dispose instances of TestServer many times. System.TypeInitializationException  Another exception related to this problem:  I think this is due to insufficient cleanup in the Logger class on shutdown. https://github.com/MassTransit/MassTransit/blob/c83a1b55f008cc12c9b756596ead572662fa7ab6/src/MassTransit/Logging/Logger.cs#L49 As I understand, the following happens: 1. Test runs, a TestServer is created together with MassTransit infrastructure. Simple configuration, in-memory bus. 2. MassTransit logs something. The static field _logger is initialized. 3. TestServer is disposed, Shutdown is called, _logger is disposed. 4. Another test starts, MassTransit logs something. 5. As no logger is set during the configuration stage, the old one (stored in the static field) is used. 6. The above exception happens, because the auto-created logger is disposed. ## Proposed fix In Shutdown set the static field _logger to null. ## Workarounds Several workarounds are possible: - Don't dispose the TestServer. - Always configure logging in test configuration of MassTransit (e.g. cfg.UseExtensionsLogging). - After the disposal of TestServer call MassTransit.Logging.Logger.UseLogger(null).",What is a TestServer?,)  System.AggregateException : One or more errors occurred. (Cannot access a disposed object. Object name: 'LoggerFactory'.) ---- System.ObjectDisposedException : Cannot access a disposed object. Object name: 'LoggerFactory'. ... ----- Inner Stack Trace ----- at MassTransit.Testing.BusTestHarness.Start(
MassTransit/MassTransit,https://github.com/MassTransit/MassTransit/issues/1683,MassTransit_MassTransit_issues_1683,"MassTransit- AzureFunction Sample issues with the latest MassTransit Libraries and .Net core 3.0

<!-- PLEASE READ THE FIRST SECTION :-) --> ### Is this a bug report? Yes <!-- If you answered ""Yes"": Please note that your issue will be fixed much faster if you spend about half an hour preparing it, including the exact reproduction steps and a demo. If you're in a hurry or don't feel confident, it's fine to report bugs with less details, but this makes it less likely they'll get fixed soon. In either case, please fill as many fields below as you can. If you answered ""No"": If this is a question or a discussion, please use [StackOverflow](https://stackoverflow.com/questions/tagged/masstransit) or [MT mailing list for questions](https://groups.google.com/forum/#!forum/masstransit-discuss) --> ### Can you also reproduce the problem with the latest version? Yes ### Environment <!-- Please fill in all the relevant fields. --> 1. Operating system: Windows 2. Visual Studio version: 2019 3. Dotnet version: .NET Core 3.0 ### Steps to Reproduce <!-- How would you describe your issue to someone who doesn’t know you or your project? Try to write a sequence of steps that anybody can repeat to see the issue. --> (Write your steps here:) 1. Trying to run the sample provided here with the latest masstransit libraries and .Net core 3.0. https://github.com/MassTransit/MassTransit/blob/develop/src/Samples/Sample.AzureFunctions.ServiceBus/Functions.cs 2. Use a console app as the client to send the message to the service bus queue. 3. when the messages are received in the azure function, I am getting the following error [1/28/2020 10:10:46 PM] MessageConsumerFunction: consuming message ac500000a8092816aa2008d7a43ee959 [1/28/2020 10:10:46 PM] Executed 'MessageConsumerFunction' (Failed, Id=38d04c6b-2427-430f-9ca7-f28e41b1b174) [1/28/2020 10:10:46 PM] System.Private.CoreLib: Exception while executing function: MessageConsumerFunction. MassTransit.WebJobs.ServiceBusIntegration: An exception occurred during handler creation. MassTransit: Object reference not set to an instance of an object. [1/28/2020 10:10:46 PM] Message processing error (Action=UserCallback, ClientId=MessageReceiver1input-queue, EntityPath=input-queue, Endpoint=abc-orders.servicebus.windows.net) [1/28/2020 10:10:46 PM] System.Private.CoreLib: Exception while executing function: MessageConsumerFunction. MassTransit.WebJobs.ServiceBusIntegration: An exception occurred during handler creation. MassTransit: Object reference not set to an instance of an object. ### Expected Behavior <!-- How did you expect the tool to behave? It’s fine if you’re not sure your understanding is correct. Just write down what you thought would happen. --> the sample should work without any issues ### Actual Behavior <!-- Did something go wrong? Is something broken, or not behaving as you expected? Please attach screenshots if possible! They are extremely helpful for diagnosing issues. --> I am getting errors as I mentioned above ### Reproducible Demo <!-- If you can, please share a project that reproduces the issue. This is the single most effective way to get an issue fixed soon. There are two ways to do it: * Create a new app and try to reproduce the issue in it. This is useful if you roughly know where the problem is, or can’t share the real code. * Or, copy your app and remove things until you’re left with the minimal reproducible demo. This is useful for finding the root cause. You may then optionally create a new project. This is a good guide to creating bug demos: https://stackoverflow.com/help/mcve Once you’re done, push the project to GitHub and paste the link to it below: --> (Paste the link to an example project and exact instructions to reproduce the issue.) <!-- What happens if you skip this step? We will try to help you, but in many cases it is impossible because crucial information is missing. In that case we'll tag an issue as having a low priority, and eventually close it if there is no clear direction. We still appreciate the report though, as eventually somebody else might create a reproducible example for it. Thanks for helping us help you! --> ### Update- Resolution and Sample Code An updated sample with both Sender ( ConsoleApp) and Consumer ( Azure Function) has uploaded here.. https://github.com/shanavt/MassTransitAzureSBDemo",Which MassTransit version? I believe this issue was fixed.,### Update- Resolution and Sample Code An updated sample with both Sender ( ConsoleApp) and Consumer ( Azure Function) has uploaded here.. https://github.com/shanavt/MassTransitAzureSBDemo
stephenmcd/mezzanine,https://github.com/stephenmcd/mezzanine/issues/1902,stephenmcd_mezzanine_issues_1902,"Slow Search in Mezzanine CMS

Slow Search in Mezzanine CMS. I use Mezzanine CMS. I find it too slow when searching for something. PostgreSQL process consumes a lot of CPU. The size of blog_blogpost is about 60,000 records. The example site is [www.chinalaw.vip](http://www.chinalaw.vip). Please help!",Can you try the mailing list? Thanks!,. PostgreSQL process consumes a lot of CPU
EngineHub/WorldEdit,https://github.com/EngineHub/WorldEdit/issues/1332,EngineHub_WorldEdit_issues_1332,"Exception while undo

## Platform - Minecraft 1.15.2 (fabric-loader-0.8.2+build.194-1.15.2/fabric/Fabric) - Fabric-Official(7.1.0;8e55131) ## Description (Reproduce) Worldedit throws class cast exception when attempting to undo a pasted schematic in a single player world using the fabric platform. ## Exception stack trace Full stack trace available here: https://paste.enginehub.org/gidVa9g-V",How does Paper come into this? Can you post your full log on https://paste.enginehub.org/ and put the link here?,Full stack trace available here: https://paste.enginehub.org/gidVa9g-V
edubart/otclient,https://github.com/edubart/otclient/issues/1012,edubart_otclient_issues_1012,"Compatibillity Issues

Compiling Client in Win32 mode it seems some players cannot open the client. I followed all the tutorials and it compiles correctly. The person who cannot open their client just crashes on startup. They are on a Windows 7 computer and other well known OTS open on their computer. Anti-Virus was disabled. I have no problem opening the client. There is no crash error the client just doesn't open. Been trying to figure out for a while cant seem to get it pinned down.",Does the crash has any exception error warning? for me it just looks like missing DLLs... try extracting these in the client folder: http://www.mediafire.com/file/898sy237xnd39cj/DLLs.rar,There is no crash error the client just doesn't open.
drawpile/Drawpile,https://github.com/drawpile/Drawpile/issues/692,drawpile_Drawpile_issues_692,"windows version isnt compatible

My friend uses windows 7 and it says the new 2.1.2 isnt compatible but the previous version worked fine. edit: The previous being 2.0.11 but I found out the problem was 32bit compatability. I'm just finding it strange to release this version of drawpile as the main download so early, basically forcing all older users to upgrade because newer users will have this version. You can't join 2.0.11 servers on 2.1.2 and vice versa",Can you be more specific? By previous version do you mean 2.1.1 or 2.0.11?,". edit: The previous being 2.0.11 but I found out the problem was 32bit compatability. I'm just finding it strange to release this version of drawpile as the main download so early, basically forcing all older users to upgrade because newer users will have this version. You can't join 2.0.11 servers on 2.1.2 and vice versa"
validator/validator,https://github.com/validator/validator/issues/877,validator_validator_issues_877,"Validator skips second 'charset' in meta content attribute

WHATWG HTML spec uses a very simplified algorithm to retrieve encoding name in http-equiv=""Content-Type"" content=""text/html; charset=... case. https://html.spec.whatwg.org/multipage/urls-and-fetching.html#extracting-character-encodings-from-meta-elements It only searches 'charset' literal characters, and if failed, continues searching 'charset'. It seems it is used both in prescan and main parsing. https://html.spec.whatwg.org/multipage/parsing.html#prescan-a-byte-stream-to-determine-its-encoding https://html.spec.whatwg.org/multipage/parsing.html#parsing-main-inhead Validator returns on first failure (perhaps extractCharsetFromContent() in htmlparser/impl/TreeBuilder.java). Example: 1. <meta http-equiv=""Content-Type"" content=""text/html; charset charset=iso-8859-2""> 2. <meta http-equiv=""Content-Type"" content=""text/html; charsetxxxxxcharset=iso-8859-2""> 3. <meta http-equiv=""Content-Type"" content=""text/html; charsetcharset=iso-8859-2""> Validator(https://validator.w3.org/nu/#textarea) reports that The legacy encoding declarationdid not contain charset= after the semicolon.. Cf. When validator succeeds in getting encoding, it usually reports e.g. Internal encoding declaration iso-8859-2 disagrees with the actual encoding of the document (utf-8).. (edited, to make a little easier to understand)",What should the expected behavior be per the current spec?,"Cf. (edited, to make a little easier to understand)"
benedmunds/CodeIgniter-Ion-Auth,https://github.com/benedmunds/CodeIgniter-Ion-Auth/issues/1247,benedmunds_CodeIgniter-Ion-Auth_issues_1247,"Unable to email the Reset Password link

Hello. I know a lot of issue spoke about this subject, i read a lot of topic about this and try a lot of modification but its dont work.. I want to send email with the server method. I create the file ""email.php"" inside the folder applications/config <?php defined('BASEPATH') OR exit('No direct script access allowed'); $config['useragent'] = 'CodeIgniter'; $config['protocol'] = 'sendmail'; $config['mailpath'] = '/usr/sbin/sendmail'; $config['wordwrap'] = TRUE; $config['wrapchars'] = 76; $config['mailtype'] = 'html'; $config['charset'] = 'utf-8'; $config['validate'] = FALSE; $config['priority'] = 3; $config['crlf'] = ""\r\n""; $config['newline'] = ""\r\n""; $config['bcc_batch_mode'] = FALSE; $config['bcc_batch_size'] = 200; I create a function to send mail. I load the library _($this->load->library('email');)_ and the function work great. But, when i try to reset password with ionAuth, i have the error ""Unable to email the Reset Password link"". Inside the ion auth config, i have: $config['use_ci_email'] = TRUE; $config['email_config'] = 'file'; (i try with $config['email_config'] = array('mailtype' => 'html'); and its dont work too. Thanks..","Where is your config/ion_auth.php file ? It must be in application/config/ion_auth.php If you put it in the third party folder, that doesn't work.","This code work nice: <?php defined('BASEPATH') OR exit('No direct script access allowed'); class Mail extends CI_Controller { public function index() { $this->load->library('email'); $this->email->from('your@example.com', 'Your Name'); $this->email->to('thibaut@agence-ecume.fr'); $this->email->cc('another@another-example.com'); $this->email->bcc('them@their-example.com'); $this->email->subject('Email Test'); $this->email->message('Testing the email class.'); $this->email->send(); } }"
TooTallNate/Java-WebSocket,https://github.com/TooTallNate/Java-WebSocket/issues/778,TooTallNate_Java-WebSocket_issues_778,"Websocket channel not close when network disconnect

<!--- Provide a general summary of the issue in the Title above --> ## Expected Behavior <!--- If you're describing a bug, tell us what should happen --> <!--- If you're suggesting a change/improvement, tell us how it should work --> websocket client should be closed when I turn off network on client. ## Current Behavior <!--- If describing a bug, tell us what happens instead of the expected behavior --> <!--- If suggesting a change/improvement, explain the difference from current behavior --> When I turn off network on Android device, the websocket connection is still established. ## Possible Solution <!--- Not obligatory, but suggest a fix/reason for the bug, --> <!--- or ideas how to implement the addition or change --> ## Steps to Reproduce (for bugs) <!--- Provide a link to a live example, or an unambiguous set of steps to --> <!--- reproduce this bug. Include code to reproduce, if relevant --> 1. create websocket client and establish a connection to server. 2. turn off network on client. 3. enter onClose callback on debug mode. 4. check server tcp port status, the connection is still established. ## Debug log (for bugs) <!--- Please provide a debug log for your issue. --> <!--- You can activate the debug log with 'WebSocketImpl.DEBUG = true;' --> ## Context <!--- How has this issue affected you? What are you trying to accomplish? --> <!--- Providing context helps us come up with a solution that is most useful in the real world --> I will do reconnect in onClose callback, so if the previous connection is not be closed, then one more connection will be established. ## Your Environment <!--- Include as many relevant details about the environment you experienced the bug in --> * Version used: 1.3.9 * Java version: 1.8 * Operating System and version: macOS 10.13.6, Android Studio 3+ * Endpoint Name and version: Android 8.0 * Link to your project:","Could you please add more information, such as versions, steps to reproduce and debug log? Issues without enough information may be closed.","websocket client should be closed when I turn off network on client. When I turn off network on Android device, the websocket connection is still established. create websocket client and establish a connection to server. turn off network on client. enter onClose callback on debug mode.. check server tcp port status, the connection is still establishedI will do reconnect in onClose callback, so if the previous connection is not be closed, then one more connection will be established. 1.3.9 1.8 macOS 10.13.6, Android Studio 3+ Android 8.0"
mscdex/node-imap,https://github.com/mscdex/node-imap/issues/788,mscdex_node-imap_issues_788,"fetch.on(""message"") event is firing twice, though imap.search result is only one result

I am trying to fetch all unseen messages, but some times mail event is triggering twice  The logs are showing mail id/uid printed twice but scanned only once and scan result was 1 item This is not always happening, any solution so that fetch.on message only triggered once for once search result",What is in seachResults and is it the same seqno both times inside your 'message' event handler?,", function (msg,seqno) { var uid, flags; msg.on('attributes', function(attrs) { uid = attrs.uid; flags = attrs.flags; ** }); var mp = new MailParser(); mp.once('end', function (mail) { mail.uid = uid; mail.flags = flags; if(uid>last_seq) self.emit('mail', mail);"
celery/kombu,https://github.com/celery/kombu/issues/1019,celery_kombu_issues_1019,"""Error 110 while writing to socket. Connection timed out."" With kombu 4.4.0/4.5.0 and redis 3.2.0/3.2.1

I was previously on kombu 4.3.0 and redis 2.10.6, and after upgrading to kombu 4.4.0/4.5.0 and redis 3.2.0/3.2.1 I noticed a new issue on my Django REST endpoints: Error 110 while writing to socket. Connection timed out. These endpoints never had any issues before and this issue popped up only once since I upgraded, but happened on both endpoints simultaneously Edit: Bug still there in most recent versions of the libraries (celery 4.3.0, kombu 4.5.0, redis 3.2.1) (I also had the bug when just upgrading kombu/redis and keeping celery at 4.2.2) Bug is not there in celery 4.2.2, kombu 4.3.0, redis 2.10.6",How about the Redis logs? Maybe you could also ask upstream?,"/4.5.0(I also had the bug when just upgrading kombu/redis and keeping celery at 4.2.2)2019-05-21T07:51:02.022118+00:00 app[worker.1]: [2019-05-21 07:51:02,021: ERROR/ in 1.00 second."
sonata-project/SonataMediaBundle,https://github.com/sonata-project/SonataMediaBundle/issues/1630,sonata-project_SonataMediaBundle_issues_1630,"The target-entity Application\Sonata\ClassificationBundle\Entity\Category cannot be found in 'App\Application\Sonata\MediaBundle\Entity\Media#category'.

### Environment #### Sonata packages  #### Symfony packages  #### PHP version  ## Subject I have this mistake: ""The target-entity Application\Sonata\MediaBundle\Entity\Media cannot be found in 'App\Appl ication\Sonata\ClassificationBundle\Entity\Category#media'."". ## Steps to reproduce I followed the installation instructions of the official documentation and when launching the server I found that error inside the media option. I try to update the database and the console says to me:  I have the demo uploaded on my github : [https://github.com/HecFranco/Sonata-project-complete-demo](https://github.com/HecFranco/Sonata-project-complete-demo). ## Expected results Update the database and be able to launch the server correctly. ## Actual results The target-entity Application\Sonata\MediaBundle\Entity\Media cannot be found in 'App\Application\Sonata\ClassificationBundle\Entity\Category#media'.",Could you please update the description using the [issue template](https://github.com/sonata-project/SonataMediaBundle/blob/3.x/.github/ISSUE_TEMPLATE.md)?,"' sonata-project/admin-bundle 3.54.0 3.54.0 \Media\Sonata\ClassificationBundle\Entity\Category#media'."". ## Steps to reproduce I followed the installation instructions of the official documentation and when launching the server I found that error inside the media option. I try to update the database and the console says to me: Sonata\MediaBundle\Entity\Media cannot be found in 'App\Appl ication\Sonata\ClassificationBundle\Entity\Category#media'."
csete/gqrx,https://github.com/csete/gqrx/issues/791,csete_gqrx_issues_791,"Frequency correction option leads to impossible decoding in 3rd-party apps

More details are [here](https://ham.stackexchange.com/questions/16488/problem-with-rtl-sdr-v3-dongle-unusable-pll/). Last year I've been using GQRX + Audacity + WXtoImg combination just fine. Then this winter I generated optimal VOLK profile as README.md recommends with the volk_profile command. After that I wasn't able to decode APT and DRM transmissions at all - the whole situation just looked like troubles with PLL locking (I can't upload corresponding images right now because of poor network connection in the forest so please check a link at the very beginning). APT images were heavily skewed and FFT analysis haven't shown prominent 2.4 kHz peak despite original reception was at very good SNR (about 30 dB). DRM transmissions never were fine - CRCs just were wrong so no decoding was possible. But as soon as I've removed the whole .volk directory from my home folder things returned back - APT decoding started to work again and I'm able to receive and decode DRM transmissions right now! My software: - Arch Linux 64 bit - Kernel 5.6.8 - GQRX 2.12.1 - gnuradio 3.8.0.0 - Audacity 2.3.3 - DReaM 2.1.1 My hardware: - RTL SDR v3 dongle from rtl-sdr.com with TXCO (-1.6 ppm calibrated) - Asus VivoMini VC65 - Intel(R) Core(TM) i3-6100T CPU @ 3.20GHz **UPDATE** It seems like original statement that VOLK causes troubles with decoding is false. Actually I've found that frequency correction factor leads to such behavior (see below). So I changed the title to more correct one",Which version of VOLK are you using? Both 2.0.0 and 2.2.0 had serious DSP bugs in the Rotator kernel; those bugs were fixed in the 2.1.0 and 2.2.1 releases respectively.,**UPDATE** It seems like original statement that VOLK causes troubles with decoding is false. Actually I've found that frequency correction factor leads to such behavior (see below). So I changed the title to more correct one
petergoldstein/dalli,https://github.com/petergoldstein/dalli/issues/704,petergoldstein_dalli_issues_704,"Seemingly version 2.7.9 has problems with frozen strings

Appeared after updating from 2.7.8 to 2.7.9 version.  via application.html.erb",Do you have an example of an object being passed into the cache that can trigger this failure?,ERROR [2018-10-24T05:00:51.777778 #14063-22175620] ' xxx/shared/bundle/ruby/2.5.0/gems/actionpack-5.2.1/lib/abstract_controller/caching/fragments.rb:114:in block in read_fragment' xxx/shared/bundle/ruby/2.5.0/gems/actionpack-5.2.1/lib/abstract_controller/caching/fragments.rb:162:in block in instrument_fragment_cache' xxx/shared/bundle/ruby/2.5.0/gems/activesupport-5.2.1/lib/active_support/notifications.rb:168:in block in instrument' xxx/shared/bundle/ruby/2.5.0/gems/activesupport-5.2.1/lib/active_support/notifications/instrumenter.rb:23:in instrument' xxx/shared/bundle/ruby/2.5.0/gems/activesupport-5.2.1/lib/active_support/notifications.rb:168:in instrument' xxx/shared/bundle/ruby/2.5.0/gems/actionpack-5.2.1/lib/abstract_controller/caching/fragments.rb:162:in instrument_fragment_cache' xxx/shared/bundle/ruby/2.5.0/gems/actionpack-5.2.1/lib/abstract_controller/caching/fragments.rb:113:in read_fragment' xxx/shared/bundle/ruby/2.5.0/gems/actionview-5.2.1/lib/action_view/helpers/cache_helper.rb:248:in read_fragment_for' xxx/shared/bundle/ruby/2.5.0/gems/actionview-5.2.1/lib/action_view/helpers/cache_helper.rb:238:in fragment_for' xxx/shared/bundle/ruby/2.5.0/gems/actionview-5.2.1/lib/action_view/helpers/cache_helper.rb:169:in cache' xxx/releases/20181024080643/app/views/layouts/application.html.erb:48:in _app_views_layouts_application_html_erb__3856684245160448392_70278594074300
hierynomus/sshj,https://github.com/hierynomus/sshj/issues/521,hierynomus_sshj_issues_521,"Authentication problem using ECDSA 521 private key

Hey there, I've been using sshj for some time, thanks for such nice tool and continue on your improvements! I encounter a problem when I try to use ecdsa-sha2-nistp521 key. This could be duplicate issue but I couldn't find it from previous issues and in other public sources. Here is the deal, I have a private ECDSA key and a remote server which I can connect successfully over ssh. While using sshj, I receive  Here is how I try to do the magic  Possibly related: On my first trial of SSHClient creation, I receive the following (when calling connect(ip, port, poxy) method) and then I try to create with the fingerpint.  I've tried adding BouncyCastle as Security.addProvider(new BouncyCastleProvider()); however, it didn't change the result. If you can give a hand, it would be appreciated! Specifications: - JDK 1.8_202 ((JCE) Unlimited Strength is enabled by default) - SSHJ 0.27 - BouncyCastle 1.60",Can you send the debug logging of the full connection/login attempt?,_202 ((JCE) Unlimited Strength is enabled by default)
xmppjs/xmpp.js,https://github.com/xmppjs/xmpp.js/issues/620,xmppjs_xmpp.js_issues_620,"Add support for SCRAM-SHA-1-PLUS, SCRAM-SHA-256(-PLUS)

""When using the SASL SCRAM mechanism, the SCRAM-SHA-256-PLUS variant SHOULD be preferred over the SCRAM-SHA-256 variant, and SHA-256 variants [RFC7677] SHOULD be preferred over SHA-1 variants [RFC5802]"". There is only SCRAM-SHA-1, there is not SCRAM-SHA-1-PLUS: - RFC5802: Salted Challenge Response Authentication Mechanism (SCRAM) SASL and GSS-API Mechanisms: https://tools.ietf.org/html/rfc5802 - RFC6120: Extensible Messaging and Presence Protocol (XMPP): Core: https://tools.ietf.org/html/rfc6120 There is not SCRAM-SHA-256(-PLUS): - RFC7677: SCRAM-SHA-256 and SCRAM-SHA-256-PLUS Simple Authentication and Security Layer (SASL) Mechanisms: https://tools.ietf.org/html/rfc7677 - since 2015-11-02 - RFC8600: Using Extensible Messaging and Presence Protocol (XMPP) for Security Information Exchange: https://tools.ietf.org/html/rfc8600 - since 2019-06-21: https://mailarchive.ietf.org/arch/msg/ietf-announce/suJMmeMhuAOmGn_PJYgX5Vm8lNA I add SCRAM-SHA-512(-PLUS): https://xmpp.org/extensions/inbox/hash-recommendations.html -PLUS variants: - RFC5056: On the Use of Channel Bindings to Secure Channels: https://tools.ietf.org/html/rfc5056 - RFC5929: Channel Bindings for TLS: https://tools.ietf.org/html/rfc5929 - Channel-Binding Types: https://www.iana.org/assignments/channel-binding-types/channel-binding-types.xhtml LDAP: - RFC5803: Lightweight Directory Access Protocol (LDAP) Schema for Storing Salted: Challenge Response Authentication Mechanism (SCRAM) Secrets: https://tools.ietf.org/html/rfc5803 HTTP: - RFC7804: Salted Challenge Response HTTP Authentication Mechanism: https://tools.ietf.org/html/rfc7804 IANA: - Simple Authentication and Security Layer (SASL) Mechanisms: https://www.iana.org/assignments/sasl-mechanisms/sasl-mechanisms.xhtml Linked to: - https://github.com/scram-xmpp/info/issues/1",What's your point?,) I add SCRAM-SHA-512(-PLUS) : https://xmpp.org/extensions/inbox/hash-recommendations.html
mixpanel/mixpanel-android,https://github.com/mixpanel/mixpanel-android/issues/678,mixpanel_mixpanel-android_issues_678,"Crash in MixpanelAPI.getInstance

We started seeing this crash with out latest release (that uses Mixpanel SDK v5.8.0). Seems to be related to the deprecation of the InstallReferrer intent. Caused by java.lang.IllegalArgumentException: Service not registered: com.android.installreferrer.api.InstallReferrerClientImpl$InstallReferrerServiceConnection@1276263  Happens on different device models and OS versions.",what's the exception that is thrown? Is it an IllegalArgumentException? Would be great to see the entire stack trace. What installreferrer version are you using?,Caused by java.lang.IllegalArgumentException: Service not registered: com.android.installreferrer.api.InstallReferrerClientImpl$InstallReferrerServiceConnection@1276263 at at at at at at at at at at at at at at at at at at at at at at at at at at at at
cbeust/testng-eclipse,https://github.com/cbeust/testng-eclipse/issues/444,cbeust_testng-eclipse_issues_444,"TestNG Version - 7.0.0 - Bug

### Problem Statement 1. Not able to open existing GIT project. 2. Get null pointer exception every time when the eclipse is closed. ### Any relate message in the ""Error Log"" view I get null pointer exception while closing the eclipse. ### The Dependency Management tool for your project Eclipse 2019-06 version ### Operating System (Windows 10 - 64 bit) ![TestNG-7 0 0 - error while closing the eclipse](https://user-images.githubusercontent.com/32246035/63546160-bc5b6680-c546-11e9-8e47-b8adb3a2c014.PNG) ![TestNG-7 0 0-Error-Eclipse-MavenProject-Windows10](https://user-images.githubusercontent.com/32246035/63546161-bcf3fd00-c546-11e9-99a0-df641b917c11.PNG) ###Please help me to resolve this ASAP. Best Regards, Madhu",what's the testng eclipse plugin version? what's the testng version in your project?,from 6.14.3 to 7.0.0
graphstream/gs-core,https://github.com/graphstream/gs-core/issues/301,graphstream_gs-core_issues_301,"Java10 - DefaultCamera.java has incorrect Tx Matrix

Testing on Windows, OK on Java8, but on Java9, 10 can't select points. After the MouseEvent getX.getY have passed through the DefaultCamera.Tx they don't match any points on the graph, so nothing is selected. Graph graph = new SingleGraph(""Tutorial 1""); graph.addNode(""A"" ); graph.addNode(""B"" ); graph.addNode(""C"" ); graph.addEdge(""AB"", ""A"", ""B""); graph.addEdge(""BC"", ""B"", ""C""); graph.addEdge(""CA"", ""C"", ""A""); graph.display(); Can only select points on Java 8, but for 9,10 nothing is selected when click on node.",What version (or branch) of gs-core are you reffering to?,"Testing on Windows, OK on 8, but on Java9,' Graph graph = new SingleGraph(""Tutorial 1""); graph.addNode(""A"" ); graph.addNode(""B"" ); graph.addNode(""C"" ); graph.addEdge(""AB"", ""A"", ""B""); graph.addEdge(""BC"", ""B"", ""C""); graph.addEdge(""CA"", ""C"", ""A""); graph.display(); Can only select points on Java 8, but for 9,10 nothing is selected when click on node."
icsharpcode/SharpZipLib,https://github.com/icsharpcode/SharpZipLib/issues/251,icsharpcode_SharpZipLib_issues_251,"ZipConstants.DefaultCodePage is set as UTF-8, but the default for FileEntry.IsUnicodeText is still false

Zip chinese filename, but I get filename with garbled text. #NET45 #C Sharp #v1.0.0 RC1 #ZipEntry https://github.com/icsharpcode/SharpZipLib/releases Test Chinese Filename [測試.txt](https://github.com/icsharpcode/SharpZipLib/files/2197070/default.txt) Zip Filename like ""測試.txt"", but I get filename ""皜祈岫.txt"". ![2018-07-16_172858](https://user-images.githubusercontent.com/8554884/42751584-d4ab86e0-891d-11e8-8d9d-419366f331ec.png) Version 0.86.0.518 is OK, but 1.0.0 is wrong.",Could you provide a sample file that does not work? You can upload it here by dragging it into the textbox.,Test File [測試.txt](https://github.com/icsharpcode/SharpZipLib/files/2197070/default.txt) Result ![2018-07-16_172858](https://user-images.githubusercontent.com/8554884/42751584-d4ab86e0-891d-11e8-8d9d-419366f331ec.png)
icsharpcode/SharpZipLib,https://github.com/icsharpcode/SharpZipLib/issues/398,icsharpcode_SharpZipLib_issues_398,"CompressionMethod.Stored defective in V 1.2.0 (compared to 1.1.0)

### Steps to reproduce  3. Then, the resulting ZIP cannot be opened with Windows explorer and in 7Zip, the archive is clearly broken! This is only in Version 1.2.0, not in 1.1.0. I include the complete code below. ### Expected behavior Creating an archive, where all files are stored without compression and the ""packed size"" is exactly the filesize. ### Actual behavior Broken ZIP archive. Cannot be opened with Windows explorer. 7Zip opens it, but does not show all packed files and under properties shows that there is a header error. ### Version of SharpZipLib 1.2.0 only. Version 1.1.0 is correct. ### Obtained from (only keep the relevant lines) - Package installed using NuGet ### Code:",Could this affect FastZip?, zipStream.Password = null; // Removes drive from name and fixes slash direction ${entryName} after Write pos: {zipStream.Position}th; varpos - len; Console.WriteLine
umlaeute/v4l2loopback,https://github.com/umlaeute/v4l2loopback/issues/278,umlaeute_v4l2loopback_issues_278,"Cannot create new /dev/video* device

#### System information: * v4l2loopback version: v4l2loopback driver version 0.12.3 * kernel version: 4.15.0-96-generic #97-Ubuntu SMP Wed Apr 1 03:25:46 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux * Distribution (+version): Ubuntu Description: Ubuntu 18.04.4 LTS Release: 18.04 Codename: bionic #### Steps to reproduce and observed Results: * List existing video devices: ls -ltrh /dev/video* I think this device was created using the command below, as my desktop has not had any video device connected to it, ever. * try creating a new device: sudo modprobe v4l2loopback devices=1 card_label=""OBS cam"" exclusive_caps=1 I have tried without any option, adding the video_number=3 option, with or without sudo, no change here. * List existing video devices again, and find same output as before: ls -ltrh /dev/video* #### Expected Results: I would have expected to get a new video device, however this is not the case. The objective of this is to setup a virtual webcam using obs and obs-virtual-sink (https://github.com/CatxFish/obs-v4l2sink) for use in jitsi, if that is of any relevance. Additionally, tests using vlc vlc v4l2:///dev/video0 and webrtc (https://test.webrtc.org/) both fail (webrtc finds no input device, while vlc returns ""cannot open device '/dev/video0': Operation not permitted"" even with my user part of the video group) so I guess the existing created video device does not work. EDIT : the https://test.webrtc.org/ test fails on both firefox and chrome, both latest release. On firefox, it does display a prompt asking access to ""OBS cam"" and my microphone, but when I agree it outputs : ""Failed to access your computer's camera and microphone (AbortError: Starting video failed)."" Thanks in advance for any help on clearing this issue.","Can you try to unload your module?  To check if the device /dev/video0 disappears? If not, something in your computer is a capture/sink device.","ailed)."" Thanks in advance f"
vlm/asn1c,https://github.com/vlm/asn1c/issues/344,vlm_asn1c_issues_344,"Incorrect string to int conversion in asn_strtoimax_lim

While trying to formally prove the correctness of ASN1C code in Coq proof assistant, one of [Digamma.ai](https://digamma.ai/) engineers, @akinanop reported the following problem with this function: https://github.com/vlm/asn1c/blob/88ed3b5cf012918bc1084b606b0624c45e0d2191/skeletons/INTEGER.c#L1034 ""I found an error in the code in the branch that was problematic to prove. I mean the branch when the input starts with - and we reach a case when value == upper_boundary && d <= last_digit_max. Then we could enter the loop with a negative value and then if there is more digits enter the branch value < upper_boundary. Below are the examples. If we are within the negative limit the output is as expected:""  But if we are in the case when we reach the upper boundary and there is extra stuff, then the result is not as expected. On some inputs it is ERROR_RANGE:  Buf for some, it is OK:  Below is a self-contained test demonstrating the problem:","Where exactly is the bug, and do you have any suggestion for a fix?","Buf for some, it is OK:"
playframework/play1,https://github.com/playframework/play1/issues/1328,playframework_play1_issues_1328,"Support SameSite cookie (Strict, Lax, None)

### Are you looking for help? From Google Chrome 80, they are planning to released from february, 2020, they will force SameSite=None for all Cookies if we want to use for third-party context. If no SameSite is set, then Chrome will understand as SameSite=Lax. It seems to be reversed the result as before (SameSite=None by default). Please see: https://www.chromium.org/updates/same-site Therefore, we need to update our cookies with declarative setting SameSite from Playframework Controller. ### Play Version (1.5.x / etc) Playframework 1.5.x ### Expected Behavior 1. Support SameSite enum inside the Http.Cookie 2. SameSite enum supports three values: Strict, Lax and None",How exactly would you do that in a controller?,Lax for all Cookies if we want to use for third-party context. It seems to be reversed the result as before (SameSite=
WSDOT/wsdot-android-app,https://github.com/WSDOT/wsdot-android-app/issues/171,WSDOT_wsdot-android-app_issues_171,"all option in setting button not working

#### EXPECTED BEHAVIOR is supposed to be when the user hits the settings button the user can set this application to what the user wants. #### ACTUAL BEHAVIOR when the user wants to set according to what the user wants all options in the settings section is not functioning and can not be used #### HOW TO REPRODUCE -download WSDOT fromPLAYSTORE - install the app - click the three dot line at the top right of the app - select settings - then press any option and see if the feature does not have any effect device : android 6.0 browser WSDOT v 5.3.9 #### BUG RECORDING https://www.youtube.com/watch?v=UHwzn9FLRro","Could you be more descriptive? Are you running the latest version from the app store or a local build? Also, which versions of Android have you tried?",#### EXPECTED BEHAVIOR is supposed to be when the user hits the settings button the user can set this application to what the user wants. #### ACTUAL BEHAVIOR when the user wants to set according to what the user wants all options in the settings section is not functioning and can not be used #### HOW TO REPRODUCE -download wtsd from github release - install the app - click the three dot line at the top right of the app - select settings - then press any option and see if the feature does not have any effect device : android 6.0 browser WSDOT v 5.3.9 #### BUG RECORDING https://www.youtube.com/watch?v=UHwzn9FLRro
c-ares/c-ares,https://github.com/c-ares/c-ares/issues/210,c-ares_c-ares_issues_210,"Build error: argument to ‘sizeof’ in ‘strncpy’ call is the same expression as the source;

While building I get this error:  gcc version  Fixed using : -Wno-error=sizeof-pointer-memaccess in the CXX_FLAGS Also I get this other error:  Again fixed with -Wno-error=stringop-overflow= on C_CFLAGS",Does it not see that src and *dest is of the same type? What nit-picking gcc version is this?,Fixed using : -Wno-error=sizeof-pointer-memaccess in the CXX_FLAGS Also I get this other error:  Again fixed with -Wno-error=stringop-overflow= on C_CFLAGS
magro/kryo-serializers,https://github.com/magro/kryo-serializers/issues/104,magro_kryo-serializers_issues_104,"serializer for org.hibernate.proxy.HibernateProxy

a Entity instance that select with hibernate from db, but it's a HibernateProxy  how serialization and deserialization the instance?",Do you want to submit a PR?,oOne; import org.hibernate.annotations.DynamicUpdate; import lombok.Getter; import lombok.Setter; @Getter @Setter @Entity @DynamicUpdate public class DemoEntity { @Id private Long id; private Date createTime; }  how serialization and deserialization the instance?
log4mongo/log4mongo-python,https://github.com/log4mongo/log4mongo-python/issues/48,log4mongo_log4mongo-python_issues_48,"Connection error with Mongo Atlas

Hi, I have a database in Mongo Atlas but cannot connect. My connection code:  The original connection string ""mongodb+srv://testeko:aSd1234@.....mongodb.net/TestDB?retryWrites=true&w=majority"" It gives this error when trying to connect. pymongo.errors.ServerSelectionTimeoutError: connection closed Python : 3.7.4, log4mongo : 1.7.0, pymongo : 3.9.0 What should I do? Thank you.",Can you please post a complete trace?,"Python : 3.7.4, log4mongo : 1.7.0, pymongo : 3.9.0"
