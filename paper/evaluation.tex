\section{Evaluation}

We implemented a prototype of the system with the aim of evaluating its efficacy along
a few different dimensions. First, we use metrics and a held-out data set to evaluate the quality
of the recommendation, i.e., how well the system recommends valid follow-up questions for incomplete bug reports.
Second, we use a survey of software developers to evaluate the follow-up questions across non-functional
dimensions, i.e., their usefulness, novelty and specificity.


\subsection{Recommendation Quality}

Can't use held


With "B" we mark the most useful and most appropriate follow-up question in the list (always 1), with "V" we rank all other questions that are appropriate to the BR.



\subsection{Developer Survey}
