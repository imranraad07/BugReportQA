\section{Evaluation}

We implemented a prototype of the system with the aim of evaluating its efficacy along
a few different dimensions. First, we use metrics and a held-out data set to evaluate the quality
of the recommendation, i.e., how well the system recommends valid follow-up questions for incomplete bug reports.
Second, we use a survey of software developers to evaluate the follow-up questions across non-functional
dimensions, i.e., their usefulness, novelty and specificity.


\subsection{Quality of Follow-Up Question Ranking}

One way of evaluating the ranking system, based on a held-out dataset (of bug reports and candidate follow-up questions), is
by using the posed questions as the ground truth. However, this simple setup has a serious deficiency in that the posed question
may not always be the most optimal among the set of candidate follow-up questions. More importantly, several of
the remaining candidate questions may be valid and (more) relevant to the bug report and therefore should
not be considered as negatively labeled instances for evaluation. Therefore, in order to provide an evaluation
set that identifies all of the valid questions in the candidate set, we perform manual annotation that clearly
identifies all of the valid follow-up questions for a specific bug report.

\subsubsection{Annotation}
We annotated 400 randomly chosen bug reports that were held out from our original corpus of 25K. The annotation
was performed by two of the authors following an agreed-upon predefined procedure. For each bug report, an annotator 1)
read the bug report carefully, spending a few minutes to understand its context, e.g., by looking at the purpose of the overall GitHub
project and the types of technologies it relies on; 2) marked all of the follow-up questions for the candidate set of 10
that were valid. Both of the annotators processed the set of 400 bug reports, marking an average of 3.45/10 of the follow up questions
as valid with an inter-annotator agreement (Cohen's kappa) of X.

\subsubsection{Baselines}
The baselines we identified are meant to convey both straightforward approaches to ranking (e.g., directly using the Lucene output) and
ablation, i.e., using one part of our ranking function but not the other (e.g., ranking based only on the question utility, $U(q_{i})$).
We did not find appropriate prior research work to compare against, since the research direction is novel and models from other domains
with a similar purpose are too different in form. Below is an enumerated list of all of the ranking baselines we used.
\begin{itemize}
\item {\em Random} -- A random permutation of the candidate follow-up question list. We present metrics averaged over 10 runs.
\item {\em Lucene} -- Lucene uses the vector space model (i.e., tf*idf) to rank follow-up questions based on the similarity between the bug reports. This baseline just transfers Lucene's ranking, which we use to generate our candidate set of 10 follow-up questions, as the system's output.
\item {\em Utility only} ($U(q_{i})$) -- The utility function, described in detail in Section~\ref{sec:ranking}, computes the average amount of OB,EB or S2R found in the answers to the specific follow-up question.
\item {\em Compatibility only} ($P(q_{i}+a_{i}|br)$) -- The compatibility function computes the probability a bug report can be combined with a specific follow-up question and answer pair. The implementation uses a deep NN architecture to compute this quantity.
\end{itemize}

\subsubsection{Metrics}
We use a two popular information retrieval metrics: Mean Reciprocal Rank (MRR) and Precision@$n$ ($P@n$). We use values of 1, 3 and 5 for $n$.

The goal of MRR is to evaluate how effective is our technique, or a baseline, in locating the first valid follow-up question. It is
computed as: $$MRR = \frac{1}{|B|} \sum_{i=1}^{|B|} \frac{1}{rank_{i}}$$ ,where $B$ is the set of bug reports in the test set and $rank_{i}$ is the ordered position of the first relevant document for the $i^{th}$ query.

The goal of Precision@$n$ is to measure the number of valid results of our technique, or a baseline, when considering the top $n$ positions in the ranking. It is computed as: $$P@n = $$

\subsubsection{Results}


\subsection{Developer Survey}
