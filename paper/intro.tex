\section{Introduction}

In many popular software projects, bug reports arrive with frequency and in bursts that can overwhelm even well-resourced and well-organized bug triage. At the same time, numerous bug reports lack sufficient actionable information for bug triagers to easily reproduce the bug. Practitioners and reseachers have observed the problem of bug report incompleteness, reporting over 60\% of bug reports lack any steps to reproduce and over 40\% lack any description of the expected behavior~\cite{chaparro17detecting}. While some software projects publish bug reporting guidelines (e.g., specific templates bug reports must follow), there are always some cases where such guidelines are not followed and many cases where bug reports lack crucial context or detail that would allow bug triagers to fully interpret them. Bug triagers posing quick follow up questions in order to elicit additional information from bug reporters is one means to augment the bug reports with necessary information. However follow-up questions are only effective if they are posed quickly, before the bug reporter loses focus on the specific bug. In this paper, we examine how the posing such follow-up questions for incomplete bug reports can be performed automatically, reducing bug triage effort and improving overall bug report quality.

In this paper, we design and describe a system to automatically pose follow-up questions for inadequate bug reports. We base the system on the idea that 1) relevant follow-up question have already been asked for other bug reports (and perhaps in other projects) that are similar to the current one; and 2) the projected answer to the follow-up question must add value to the original bug report. Therefore the task for our system is to retrieve the most relevant and useful follow-up question, given a specific bug report and a corpus of previous bug reports, follow-up questions and their answers.

An example of a bug report and follow-up questions from our corpus is shown in Figure 1.


To curate a corpus of prior bug reports, follow-up questions, and answers we leverage GitHub, where we focus on popular repositories that have a high level of activity. We look for follow-up questions in GitHub that have been posed in comments and gather answers that occur as comments or as edits to the original bug report text. To estimate the utility of an answer we use the patterns to identify Observable Behavior (OB), Expected Behavior (EB) and Steps to Reproduce (S2R), published by Chaparro et al. We evaluate our prototype in two ways, based on the ability to predict a held out set of follow-up questions, and based on a developer survey that aims to gage the perceived value of specific follow up questions. The results indicate that the techniques is viable, with X MRR and Y% of respondents indicating that the follow-up question is “bla bla bla”.
