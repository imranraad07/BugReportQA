\section{Related Work}
To our knowledge, ours is the first effort towards leveraging follow-up questions for improving bug report quality. The related research can be grouped into two categories, techniques for determining and improving bug report quality, and uses of follow-up questions in other domains, external to bug reports and software engineering.

%bug report quality (focus first on measuring, then on improving)
\noindent
{\em Analyzing the quality of bug reports.} The quality of user written bug reports, in the era of open source development, is a topic several researchers have been interested in. Bettenburg et al. proposed techniques for automatically identifying stack traces, code snippets, and other structures in bug reports~\cite{bettenburg08extracting}. Linstead and Baldi applied LDA to a large corpus of bug reports to determine semantic coherence of these documents~\cite{linstead09mining}. Huo et al. investigated how bug reports textually vary based on written by experts or non-experts~\cite{Huo2014AnES}. Davies et al. manually analyzed a corpus of bug reports from four popular open-source projects observing that observable behavior and expected behavior were the most consistently found parts of a bug report~\cite{davies14whats}. Using surveys, researchers observed that developers consider steps to reproduce, test cases and stack traces most helpful in bug reports, but many of those parts were also the hardest for users to supply. Chaparro et al. developed a technique to automatically extract observable behavior, expected behavior, and the steps to reproduce from a bug report~\cite{chaparro17detecting}. Karim et al. identified test cases, code examples, steps to reproduce, expected behavior, and stack traces as initially missing features which are often requested~\cite{Karim2017UnderstandingKF, karim2019identifying}.

\noindent
{\em Improving bug report quality.} Researchers have approached improving the quality of bug reports along a few different dimensions. One line of work, with numerous techniques, is to detect duplicate bug reports~\cite{chaparro19reformulating}. Another research avenue is to classify bug reports into valid vs. invalid or easy vs. difficult~\cite{fan20chaff,hooimeijer07modeling}. More recently, researchers have attempted to automatically improve parts of bug reports. Moran et al. provided auto-completion for the steps to reproduce portion of bug reports by leveraging image processing of screenshots taken from the application's UI~\cite{moran15autocompleting}. Liu et al. inspected on how often steps to reproduce relies on predefined vocabularies~\cite{liu2020automated}. Chaparro et al. explored how bug reports quality can be improved based on unexpected vocabularies in the steps to reproduce~\cite{Chaparro2019AssessingTQ}. Joorabchi et al., meanwhile, researched the area of detecting non-reproducible bugs~\cite{erfani2014works}. Di Sorbo et al. analyzed the characteristics of ``won't fix" issue reports~\cite{Sorbo2019WontWF}. Gromova et al. addressed the issue of improving bug report description by predicting various indicators~\cite{gromova2019raising}.

\noindent
{\em Automatically posing follow-up questions.} There has been prior work on generating questions where answers can be found within the text~\cite{vanderwende2008importance, rus2011question, sutskever2014sequence}. Our task involves generating questions where answer cannot be found in the given description. Mostafazadeh et al. researched on generating natural questions that are not present in a image content~\cite{Mostafazadeh_2016} which is similar to our follow-up question. Rao et al. studied on generating clarification questions on StackExchange forum~\cite{rao-daume-iii-2018-learning}. Hancock et al.~\cite{Hancock2019LearningFD} and Ren et al.~\cite{Ren2020ConversationsWS} investigated on chatbot and search engine respectively over follow-up conversations by asking automatic follow-up questions.


 