\section{Related Work}
To our knowledge, ours is the first effort towards leveraging follow-up questions for improving bug report quality. The prior related research can be grouped into three categories, techniques for determining bug report quality, approaches for improving deficient bug reports, and uses of follow-up questions in other domains, external to bug reports and software engineering.

%bug report quality (focus first on measuring, then on improving)
\noindent
{\em Analyzing the quality of bug reports.} The quality of user written bug reports, in the era of open source development, is a topic several researchers have been interested in. Bettenburg et al. proposed techniques for automatically identifying stack traces, code snippets, and other structures in bug reports~\cite{bettenburg08extracting}. Linstead and Baldi applied LDA to a large corpus of bug reports to determine semantic coherence of these documents~\cite{linstead09mining}. Huo et al. investigated how bug reports textually vary based on whether they are written by experts or non-experts~\cite{Huo2014AnES}. Davies et al. manually analyzed a corpus of bug reports from four popular open-source projects finding that observable behavior and expected behavior were the most consistently encountered parts of a bug report~\cite{davies14whats}. Using surveys, researchers observed that developers consider steps to reproduce, test cases and stack traces most helpful in bug reports, but many of those parts were also the hardest for users to supply~\cite{sasso2016satisficing}. Laukannen et al.
confirmed the importance of the previously identified parts of bug reports and identified other bug report parts related to the application's configuration and operation that were also important. Di Sorbo et al. analyzed the characteristics of ``won't fix" issue reports~\cite{Sorbo2019WontWF}. Erfani Joorabchi et al., meanwhile, performed an empirical analysis of the key properties of non-reproducible bug reports~\cite{erfani2014works}. Chaparro et al. developed a technique to automatically extract observable behavior, expected behavior, and the steps to reproduce from a bug report~\cite{chaparro17detecting}. Liu et al. proposed a technique that eschews using predefined vocabularies and uses word context in identifying the steps to reproduce in a bug report~\cite{liu2020automated}. Also, Yu et al. developed a tool S2RMiner that extracts the steps to reproduce from a bug report with high accuracy~\cite{yu2019s2rminer}.
%Karim et al. identified test cases, code examples, steps to reproduce, expected behavior, and stack traces as initially missing features which are often requested~\cite{Karim2017UnderstandingKF, karim2019identifying}.

\noindent
{\em Improving deficient bug reports.} Researchers have approached improving the quality of bug reports along a few different dimensions. One line of work, with numerous techniques, is to detect duplicate bug reports~\cite{sun2011towards,nguyen2012duplicate,chaparro19reformulating}. Another research avenue is to classify bug reports into valid vs. invalid or easy vs. difficult~\cite{fan20chaff,zhou2016combining,hooimeijer07modeling}. Researchers have also attempted to automatically improve specific parts of bug reports. Moran et al. provided auto-completion for the steps to reproduce portion of bug reports by leveraging image processing of screenshots taken from the application's UI~\cite{moran15autocompleting}.  Chaparro et al. explored how bug report quality can be improved based on unexpected vocabularies in the steps to reproduce~\cite{Chaparro2019AssessingTQ}. Recently, the BEE tool, implemented as a GitHub plugin, extracts observable behavior, expected behavior, and the steps to reproduce from a bug report in order to alert bug reporters when this information is not provided~\cite{song2020bee}.

\noindent
{\em Automatically posing follow-up questions.} Research on automatic question generation varied over different domains and applications. One area where research has been conducted extensively is on generating questions whose answers can be found within the given text~\cite{vanderwende2008importance, rus2011question, zhou2017neural, heilman2010good, duan2017question,  du2017learning}. Asking clarifying question to improve information retrieval is another area of interest~\cite{10.1145/3366423.3380126, 10.1145/3331184.3331265, stoyanchev2014towards}.  Qu et al. studied on understanding and characterizing each human utterances in an information-seeking conversations~\cite{10.1145/3209978.3210124}. Braslavski et al. analyzed clarification question patterns on question-answering websites~\cite{10.1145/3020165.3022149}. Rao et al. studied on generating a set of candidate questions which indicates the missing information in the text~\cite{rao-daume-iii-2018-learning}. Trienes et al. focused on detecting clarification questions quality~\cite{trienes2019identifying}. Asking follow-up questions has been explored in other contexts such as chatbot~\cite{Hancock2019LearningFD}, dialogoue~\cite{de2005implementing, de2003analysis}, search engine~\cite{Ren2020ConversationsWS}, image content~\cite{Mostafazadeh_2016}.
