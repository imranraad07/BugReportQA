\section{Related Work}
To our knowledge, ours is the first effort towards leveraging follow-up questions for improving bug report quality. The prior related research can be grouped into three categories, techniques for determining bug report quality, approaches for improving deficient bug reports, and uses of follow-up questions in other domains, external to bug reports and software engineering.

%bug report quality (focus first on measuring, then on improving)
\noindent
{\em Analyzing the quality of bug reports.} The quality of user written bug reports is a topic that several researchers have been interested in. Linstead and Baldi applied Latent Dirichlet Analysis to a large corpus of bug reports to study the semantic coherence of these documents~\cite{linstead09mining}. Huo et al. investigated how bug reports textually vary based on whether they are written by experts or non-experts~\cite{Huo2014AnES}. Di Sorbo et al. analyzed the characteristics of ``won't fix" issue reports, leading to a predictive model~\cite{Sorbo2019WontWF}. Erfani Joorabchi et al., meanwhile, performed an empirical analysis of the key properties (e.g., active time, number of authors) of non-reproducible bug reports~\cite{erfani2014works}. Kavaler examined language tendencies of bug reports among different developer groups and GitHub projects~\cite{kavaler2017language}.

Research on estimating bug report quality has also progressed reductively, with researchers examining the constituent parts of bug reports. To this end, Bettenburg et al. proposed techniques for automatically identifying stack traces, code snippets, and other structures in bug reports~\cite{bettenburg08extracting}. Davies et al. manually analyzed a corpus of bug reports from four popular open-source projects finding that observable behavior and expected behavior were the most consistently encountered parts of a bug report~\cite{davies14whats}. Using surveys, researchers observed that developers consider steps to reproduce, test cases and stack traces most helpful in bug reports, but many of those parts were also the hardest for users to supply~\cite{sasso2016satisficing}. Laukkanen et al.
confirmed the importance of the previously identified parts of bug reports and identified other parts related to the application's configuration and operation that are also important~\cite{laukkanen2011survey}. Chaparro et al. developed a technique leveraging language patterns to automatically extract observable behavior, expected behavior, and the steps to reproduce from a bug report~\cite{chaparro17detecting}. Liu et al. proposed a technique that eschews using predefined vocabularies and uses word context in identifying the steps to reproduce in a bug report~\cite{liu2020automated}. Recently, Yu et al. developed a tool S2RMiner that extracts the steps to reproduce from a bug report with high accuracy~\cite{yu2019s2rminer}.
%Karim et al. identified test cases, code examples, steps to reproduce, expected behavior, and stack traces as initially missing features which are often requested~\cite{Karim2017UnderstandingKF, karim2019identifying}.

\noindent
{\em Improving inadequate bug reports.} Researchers have approached improving the quality of bug reports along a few different dimensions. One line of work, with numerous techniques, is to detect duplicate bug reports~\cite{sun2011towards,nguyen2012duplicate,chaparro19reformulating}. Another research avenue is to classify bug reports into valid vs. invalid or easy vs. difficult~\cite{fan20chaff,zhou2016combining,hooimeijer07modeling}. Researchers have also attempted to automatically improve specific parts of bug reports. Moran et al. provided auto-completion for the steps to reproduce portion of bug reports by leveraging image processing of screenshots taken from the application's UI~\cite{moran15autocompleting}.  Chaparro et al. explored how bug report quality can be improved based on unexpected vocabularies in the steps to reproduce~\cite{Chaparro2019AssessingTQ}. Recently, the BEE tool, implemented as a GitHub plugin, extracts observable behavior, expected behavior, and the steps to reproduce from a bug report in order to alert bug reporters when this information is not provided~\cite{song2020bee}.

\noindent
{\em Automatically posing follow-up questions.} Research on automatic question generation varied over different domains and applications. One topic of extensive prior research is on generating questions from a document, i.e., questions whose answers can be found within the given text~\cite{vanderwende2008importance,rus2011question,zhou2017neural,heilman2010good,duan2017question,du2017learning}. For instance, such generated questions can be leveraged for educational assessment and automation. Focusing on posing clarifying questions, one application area is improving information retrieval for low-quality queries~\cite{10.1145/3366423.3380126,10.1145/3331184.3331265,stoyanchev2014towards}. Researchers envision a future where user information needs will be satisfied via dialog with a virtual assistant.
To this end, Braslavski et al. analyzed clarification question patterns on question-answering websites in order to understand user behavior, and the types of clarification questions asked~\cite{10.1145/3020165.3022149}.
Qu et al. curate and publish a large dataset of question and answers intended in order to help develop conversational search systems~\cite{10.1145/3209978.3210124}. Asking follow-up questions has been explored in other contexts such as chatbots~\cite{Hancock2019LearningFD}, dialogue~\cite{de2005implementing, de2003analysis}, search engine~\cite{Ren2020ConversationsWS}, image content~\cite{Mostafazadeh_2016}.


Rao et al. used generative adversarial neural networks to automatically generate questions that query for missing information in text, evaluating their system on Amazon product reviews~\cite{rao-daume-iii-2018-learning}. Trienes et al. focused on detecting clarification questions quality~\cite{trienes2019identifying}.
